%&pdflatex
\documentclass[a4paper,11pt,titlepage, article, oneside]{memoir}
%\semiisopage

\usepackage[margin=3cm]{geometry}
\usepackage[T1]{fontenc}
\usepackage[english]{babel}
% usare latin1 con windows
\usepackage[utf8]{inputenc}
% usare utf8 con linux
%\usepackage[uft8]{inputenc}
\usepackage{color, graphicx}%per scrivere con diversi colori
\usepackage{hyperref}
\usepackage{faktor}
\usepackage{hyperref}
\usepackage[makeroom]{cancel}
\usepackage{pgfplots}
\usepgfplotslibrary{patchplots}
\usetikzlibrary{patterns, positioning, arrows}
\pgfplotsset{compat=1.15}
\usepackage{float, subcaption, braket, bbold, amssymb,amsthm,amsmath,mathtools, tikz-cd}
\usepackage{stackengine}
\usepackage{fancyhdr}

\pagestyle{fancy}
\fancyhf{}
\fancyhead[L]{\rightmark}
\fancyhead[R]{\thepage}
\renewcommand{\headrulewidth}{0pt} 

\usepackage{tcolorbox}
\tcbuselibrary{breakable}
\tcbset{breakable, colback=gray!8!white,colframe=gray!15!white}

\newtcolorbox{remarkbox}{breakable,   colback=white,colframe=gray}

\newtcolorbox{eqnbox}{colback=blue!5!white,colframe=blue!20!black, sharp corners}

\newcommand\crule[3][black]{\textcolor{#1}{\rule{#2}{#3}}}

%%% Definizione degli ambienti tipo "Teorema"
%\newtheorem{theorem}{Theorem}[chapter]

%\theoremstyle{theorem}
%\newtheorem{lemma}[theorem]{Lemma}
%\newtheorem{proposition}[theorem]{Proposition}
%\newtheorem{corollary}[theorem]{Corollary}
%\theoremstyle{definition}
%\newtheorem{definition}[theorem]{Definition}
%\theoremstyle{remark}
%\newtheorem{remark}[theorem]{Remark}
%\newtheorem{example}[theorem]{Example}

\numberwithin{equation}{section}
\counterwithout{section}{chapter}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{fact}[theorem]{Fact}

\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}

\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}
\newtheorem{example}[theorem]{Example}



% aumenta la spaziatura
\linespread{1.2}

\DeclareMathOperator{\id}{id}
\DeclareMathOperator{\tr}{tr}
\DeclareMathOperator{\pr}{pr}
\DeclareMathOperator{\Der}{Der}
\DeclareMathOperator{\Diff}{Diff}
\DeclareMathOperator{\sgn}{sgn}
\DeclareMathOperator{\supp}{supp}
\DeclareMathOperator{\im}{im}
% \DeclareMathOperator{i}{i}
\newcommand{\rfield}{\mathbb{R}}
\newcommand{\lie}[1]{\mathfrak{#1}}

%\renewenvironment{proof}[1][\proofname]{\textit{#1.}}{\qed}


\newcommand\oast{\stackMath\mathbin{\stackinset{c}{0ex}{c}{0ex}{\ast}{\bigcirc}}}

\newcommand\ocdot{\stackMath\mathbin{\stackinset{c}{0ex}{c}{0ex}{\cdot}{\bigcirc}}}

%\newcommand{\restrict}[2]{\left.{#1}\right|_{#2}}

\newcommand{\restrict}[2]{{#1}\raisebox{-.5ex}{$|$}_{#2}}

\newcommand{\diondi}[1]{\frac{d}{d{#1}}}

\newcommand{\difondi}[2]{\frac{d{#1}}{d{#2}}}

\newcommand{\deonde}[1]{\frac{\partial}{\partial {#1}}}

\newcommand{\defonde}[2]{\frac{\partial {#1}}{\partial {#2}}}

\newcommand{\tangentgeom}[1]{T_{#1}^{\text{geom}}}

\newcommand{\tangentphys}[1]{T_{#1}^{\text{phys}}}

\newcommand{\tangentalg}[1]{T_{#1}^{\text{alg}}}

\begin{document}
\pagenumbering{gobble}

\include{titlepage}


\newpage
 {These notes are based on the content of the course \textit{Differentiable Manifolds} held at LMU during the Winter Semester 2019/2020. Some additional content from books and from my own studies was also added. These notes are neither complete nor accurate. They might contain typos and mistakes. You can send an email to \href{mailto:flaviorossetti@outlook.com}{\nolinkurl{flaviorossetti@outlook.com}} to help me improve these notes. Any help would be greatly appreciated!\\[13pt]}
 
 {These notes were compiled on \today. For the last version, check \url{https://github.com/fla-io/diff-manifolds}.}

  \vfill
  \vspace{1.5cm}
 %{\Large\bfseries }\\[5pt]
 \begin{center}
 \textit{Many thanks to Raul Morral for his help to improve the script.}
 \end{center}

\newpage

\pagenumbering{Roman}
% indice

\tableofcontents
\clearpage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\pagenumbering{arabic}

%\chapter{Part I}
\section[Introduction]{\crule[red!50!white]{0.3cm}{0.4cm}  Introduction}

From the Lee book \cite{Lee}: "The central idea of calculus is \textit{linear approximation}". A function of one variable can be approximated by its tangent line, a curve by a tangent vector (i.e. velocity vector), a surface in $\rfield^3$ can be approximated by its tangent plane, and a map from $\rfield^n$ to $\rfield^m$ by its total derivative.
Here it comes the importance of tangent spaces.

Main idea: in order to study tangent vectors, we identify them with "directional derivatives". In particular, there is a natural one-to-one correspondence between geometric tangent vectors and linear maps from $C^{\infty}(\rfield^n)$ to $\rfield$ satisfying the product rule. Such maps are called \textit{derivations}.


\begin{remarkbox}\begin{remark} \label{pointorvec}
\textbf{Points or vectors?} We can think of elements of $\rfield^n$ either as points or vectors. As points, their only property is their location, given by the coordinates $(x_1, \ldots, x_n)$ on a chosen basis. As vectors, they are characterized by a direction and a magnitude, but their location is irrelevant (translational invariance). So given $v \in \rfield^n, v=\sum_i v^i e_i = v^i e_i$, it can be seen as an arrow with its initial point anywhere in $\rfield^n$. So, if we think about a vector tangent to the border of the sphere at a point $a$, we imagine the vector as living in a copy of $\rfield^n$ with its origin translated to a.

\end{remark}\end{remarkbox}

\section[Notations and Conventions]{\crule[black!50!white]{0.3cm}{0.4cm}  Notations and Conventions}

In these notes the Einstein summation convention will be used. It means that the sum symbol will be omitted when it is clear with respect to which index we are summing. For instance we will write
$$v^i e_i$$
instead of 
$$\sum\limits_i v^i e_i$$
Given a map $f$ from a set $X$ to a set $Y$, we will denote it by 
$$f \colon X \rightarrow Y$$
We will use the arrow "$\mapsto$" to denote how each element of $X$ is mapped into $Y$ through $f$. For instance, given $x_1, x_2 \in X, y \in Y$:
$$x_1 \mapsto f(x)$$ 
or
$$x_2 \mapsto y$$

\section[Quick review: Basic Algebraic Structures]{\crule[blue!50!white]{0.3cm}{0.4cm}  Quick review: Basic Algebraic Structures}

\begin{definition} [Operation]
Let $G$ be a set. $\cdot$ is called a (binary) operation on $G$ if it is a map
\begin{align*}
\cdot \, \colon G &\times G \longrightarrow G \\
(&a, b) \longmapsto a \cdot b
\end{align*}
Such a map is usually denoted by the symbol $\cdot$ or, analogously, with the symbol $+$.
\end{definition}

\begin{definition} [Group] Given a set $G$ and an operation $\cdot$ on such set, we will call such set with the operation  (i.e. the couple $(G, \cdot)$) a group if the following properties are satisfied (for $a, b, c \in G$):
\begin{itemize}
\item $ a \cdot b \in G$ (closure property, which often follows from the definition of our operation)
\item $a \cdot (b \cdot c) = (a \cdot b) \cdot c$ (associativity)
\item $\exists\, \mathbf{1} \in G$ such that $\mathbf{1} \cdot g = g, \, \forall\, g \in G$ (existence of the identity element)
\item $\forall\, g \in G, \exists\, g^{-1} \in G$ such that $g \cdot g^{-1} = g^{-1} \cdot g = \mathbf{1}$ (existence of the inverse element)
\end{itemize}
For the sake of simplicity, we will often call $G$ a group, without referring to the operation on it.
\end{definition}


\begin{tcolorbox}\begin{example}
$(\mathbb{Z}, +)$ is a group.
\end{example}\end{tcolorbox}

\begin{tcolorbox}\begin{example}
$(\mathbb{R}, +)$ is a group. Also: $(\mathbb{R}, \cdot)$ is a group.
\end{example}\end{tcolorbox}

\begin{tcolorbox}\begin{example}
$(\mathbb{N}, +)$ is not a group!
\end{example}\end{tcolorbox}

\begin{remarkbox}\begin{remark}
The identity element of a group is often denoted as $\mathbf{1}$ if the operation is denoted by the symbol $\cdot$, whereas it is denoted as $\mathbf{0}$ if the operation is denoted by the symbol $+$.
In a similar way, the inverse element is often denoted as $g^{-1}$ if the operation is denoted by the symbol $\cdot$, whereas it is denoted as $-g$ if the operation is denoted by the symbol $+$.

\end{remark}\end{remarkbox}

\begin{definition} [Abelian group]
A group $(G, \cdot)$ is called abelian if its elements commute according to the operation $\cdot$, i.e. $a \cdot b = b \cdot a, \forall \,  a, b \in G$.
\end{definition}

Often we can consider sets with two operations, like $(\mathbb{R}, + , \cdot)$. If they satisfy some properties, they are called rings. If they satisfy even more properties, they are called fields. In particular:

\begin{definition} [Ring]
Given a set $R$ and two operations: $+$ (usually called "additive operation") and $\cdot$ (called "multiplicative operation") on it, we will call the set with the two operations, i.e. $(R, +, \cdot)$, a ring if the following properties are satisfied:
\begin{itemize}
\item $(R, +)$ is an abelian group
\item $\cdot$ is associative, i.e. $a \cdot (b \cdot c) = (a \cdot b) \cdot c, \, \forall\, a, b, c, \in R$
\item the multiplicative identity $\mathbf{1}$ exists, i.e. $\exists\, \mathbf{1} \in R$ such that $\mathbf{1} \cdot r = r, \, \forall\, r \in R$
\item $\cdot$ is distributive with respect to $+$, i.e. $a \cdot (b + c) = a \cdot b + a \cdot c$
\end{itemize}
\end{definition}

\begin{definition} [Commutative ring] $(R, +, \cdot)$ is a commutative ring if the multiplication operation $\cdot$ is commutative
\end{definition}

\begin{definition} [Unitary ring]
$(R, +, \cdot)$ is a unitary ring if it contains the multiplication inverse, i.e. the inverse element according to the operation $\cdot$.
\end{definition}

\begin{definition} [Field]
A unitary, commutative ring is called a field.
\end{definition}

\begin{tcolorbox}\begin{example}
The set of $2 \times 2$ matrices made by real coefficients is a ring with the operation of sum between matrices and with the matrix multiplication. The ring is not commutative because the matrix multiplication is not a commutative operation. Nor it is unitary, since not all the matrices are invertible.
\end{example}\end{tcolorbox}

\begin{tcolorbox}\begin{example}
The set of $2 \times 2$ matrices made by real coefficients is a ring with the operation of sum between matrices and with the matrix multiplication. The ring is not commutative because the matrix multiplication is not a commutative operation. Nor it is unitary, since not all the matrices are invertible.
\end{example}\end{tcolorbox}

\begin{tcolorbox}\begin{example}
$(\mathbb{Z}, +, \cdot)$ is a commutative ring. It is not unitary since the inverse element for $\cdot$ is often in the rational numbers, i.e. $3^{-1}$ is the multiplicative inverse of $3$.
\end{example}\end{tcolorbox}

\begin{tcolorbox}\begin{example}
$(\mathbb{R}, +, \cdot)$ is a field
\end{example}\end{tcolorbox}

\section[Quick review: Morphisms]{\crule[blue!70!white]{0.3cm}{0.4cm}  Quick review: Morphisms}
\begin{definition}[Homomorphism]
A homomorphism $h$ between two sets endowed with operations (think about two groups, for instance) is a map which preserves the operations, i.e. a map
\begin{equation*}
h \colon (G, +) \longrightarrow (H, \circ)
\end{equation*}
such that $h(a + b) = h(a) \circ h(b), \forall\, a, b, \in G$, where $(G, +)$ and $(H, \circ)$ are two groups. The definition for rings and fields is analogous. Sometimes these maps are just called morphisms (there is some difference between morphisms and homomorphisms but it usually matters only if you are dealing with more abstract algebraic structures).
\end{definition}

Now, we can consider some particular types of homomorphisms.

\begin{remarkbox}\begin{remark} [Monomorphism, epimorphisms and isomorphisms]
Homomorphisms can be called in different ways depending whether they are injective, surjective or bijective.
A monomorphism is an injective homomorphism. An epimorphism is a surjective homomorphism. An isomorphism is a bijective homomorphism (thus, it is both a monomorphism and an epimorphism).
\end{remark}\end{remarkbox}

\begin{remarkbox}\begin{remark} \label{endoremark} [Endomorphism, automorphism]
An endomorphism is a homomorphism from one algebraic structure to itself. If such morphism is bijective, it is called automorphism.
\end{remark}\end{remarkbox}

\begin{remarkbox}\begin{remark}
Don't get confused with hom\textbf{e}omorphisms! A homeomorphism is a continuous and bijective map between two topological space, such that its inverse is also continuous. In general it is not a homomorphism (without the \textbf{e}), because a topological space is not necessarily associated with an operation on it. (However if it has an operation, the continuity of the map implies that it is also a homomorphism).
\end{remark}\end{remarkbox}

\section[Quick review: Equivalence Classes, Quotient Spaces]{\crule[blue!90!white]{0.3cm}{0.4cm}  Quick review: Equivalence Classes, Quotient Spaces}
\begin{definition}[Binary relation]
  Given a set $E$, a (binary) relation $\sim$ on $E$ is a set of couples $(a, b) \in E \times E$. In other words, a binary relation is a subset of $E \times E$. Moreover, if $(a, b) \in \, \sim\,  \subset E \times E$, we use the following notation: $a \sim b$.
\end{definition}

The mathematical definition of binary relation is usually not very helpful in the applications. Often it is more convenient not to think about $\sim$, but instead thinking about what is in relation with what. In particular, we want to consider the following:

\begin{definition}[Equivalence relation]
  Given a set $E$, an equivalence relation $\sim$ is a binary relation which satisfies the following properties:
  \begin{itemize}
    \item $a \sim a, \forall\, a \in E$ (reflexive property)
    \item $a \sim b \Rightarrow b \sim a, \forall\, a, b \in E$ (symmetric property)
    \item $a \sim b, b \sim c \Rightarrow a \sim c, \forall \, a, b, c \in E$ (transitive property)
  \end{itemize}
\end{definition}

\begin{tcolorbox}\begin{example}
  The following are examples (or counter-examples) of equivalence relations:
  \begin{itemize}
  \item "\textit{Being equal to}" (i.e. "$\sim$" is "$=$") is an equivalence relation on $\rfield$: $x=x \, \forall\, x \in \rfield; x=y \Rightarrow y=x \, \forall\, x, y \in \rfield$, etc.
  \item "\textit{Has the same birthday as}" on the set of all people in the world is an equivalence relation.
  \item "\textit{Having mutual friends on Facebook}" on the set of Facebook users is not an equivalence relation (there is some problem with transitive property)
  \item "\textit{Being greater or equal to}" on $\rfield$ is not an equivalence relation (it is not symmetric, however it is antisymmetric and so it is called "partial order relation")
\end{itemize}
\end{example}\end{tcolorbox}

\begin{definition}[Equivalence class]
  Given a set $E$ and an equivalence relation $\sim$ on $E$, the equivalence class of $x \in E$ is the set $[x]$, where
  \begin{equation}
    [x] \equiv \left \{ y \in E \,|\, y \sim x \right \}
  \end{equation}
  The equivalence class $[x]$ always contains $x$ itself, by definition of equivalence relation.
\end{definition}

\begin{tcolorbox}\begin{example} \label{modulo4ex}
We can examine the integers which differ in absolute value by a multiple of 4.
  If we consider the equivalence relation "$x \sim y$ if $x-y = 4n$ for some $n \in \mathbb{N} \cup \{0\}$" on the set of $\mathbb{Z}$, the equivalence class of 1 is $[1] = \{\ldots, -7, -3, 1, 5, 9, \ldots\}$. We can also write "$x-y = 4n$ for some $n$" as $x \equiv_4 y$ (congruence modulo $4$)
\end{example}\end{tcolorbox}

\begin{definition}[Quotient space]
  Given a set $E$, an equivalence relation $\sim$ on $E$, the quotient space with respect to $\sim$ is denoted by $\faktor{E}{\sim}$ and it is the set of all equivalence classes:
  \begin{equation}
     \faktor{E}{\sim} \equiv \left \{ \, [x] \,|\, x \in E \right \}
  \end{equation}
\end{definition}

Intuitively, we can say that the quotient space $\faktor{E}{\sim}$ is a copy of $E$ where all the elements equivalent to each other are reduced to one point.

\begin{tcolorbox}\begin{example} \label{quotientexample}
  Let's consider two examples:
  \begin{itemize}
    \item If we consider the example \ref{modulo4ex}, $\faktor{\mathbb{Z}}{\sim} = \{[0], [1], [2], [3]\}$. We notice that all the integers that differ from 0 by a multiple of 4 (in absolute value) are identified with one point (the equivalence class $[0]$). All the integers that differ from 1 by a multiple of 4 (in absolute value) are identified with one point (the equivalence class $[1]$), and so on. In this case, we also write $\faktor{\mathbb{Z}}{\sim} = \faktor{\mathbb{Z}}{4\mathbb{Z}}$
    \item If $V$ is a vector space and $U$ is a vector subspace of $V$, then we can consider the equivalence relation "$x \sim y$ if $x-y \in U$". We notice that $x \sim x$ for each $x \in V$ since $x-x = 0 \in U$, because $U$ is a vector space as well. Then, $\faktor{V}{\sim}$ is isomorphic to $V'$, where $V'$ is $V$ without its subspace $U$ (every element of $U$ is identified with 0 in the quotient space, because every element of $U$ is in the same equivalence class of 0).
    \item If we consider two groups instead of vector spaces, finding the quotient space requires a bit more effort.
  \end{itemize}
\end{example}\end{tcolorbox}

\section[Derivations]{\crule[yellow!50!white]{0.3cm}{0.4cm}  Derivations}
\begin{definition}[Derivation] \label{derivation}
If $a$ is a point of $\rfield^n$, a map $v \colon C^{\infty}(\rfield^n) \rightarrow \rfield$ is called a \textit{derivation at a} if it is linear over $\rfield$ and satisfies the following product rule:
$$\restrict{v(fg)}{a} = \restrict{f}{a} \restrict{v(g)}{a} + \restrict{g}{a} \restrict{v(f)}{a}$$
\end{definition}

\begin{remarkbox}\begin{remark}
Directional derivatives obviously satisfy the above definition, and in these cases such a rule is also called Leibnitz rule.
\end{remark}\end{remarkbox}

%\begin{definition} $T_{a}\rfield^n$ is the set of all derivations of $C^{\infty}(\rfield^n)$ at $a$. (And it is a vector space)
%\end{definition}
\section[Multilinear Forms]{\crule[orange!50!white]{0.3cm}{0.4cm}  Multilinear Forms}

\begin{definition}[1-forms]
Given a  vector space $V$ on a field $K$, a 1-form (or linear form) $\varphi \colon V \rightarrow K$ is a linear function from $V$ to $K$. $V^*$ (also denoted by $\Lambda V^*$) is the set of all linear forms on $V$.
\end{definition}

\begin{remarkbox}\begin{remark}
  The set of linear forms on $V$ has the structure of a vector space. Indeed, if $\mathbb{K}$ is the field of the vector space $V$, $\forall\, a, a_1, a_2 \in \mathbb{K}, \forall\, \varphi, \varphi_1, \varphi_2 \in V^*$:
  \begin{enumerate}
    \item $a(\varphi_1 + \varphi_2) = a \varphi_1 + a\varphi_2$ (because we define the map $a_1\varphi_1 + a_2\varphi_2$ as $a_1\varphi_1 + a_2\varphi_2 (x) \equiv a_1 \varphi_1(x) + a_2 \varphi_2(x), \forall\, x \in V$)
    \item $(a+b)\varphi(x) = (a\varphi + b \varphi)(x)$
    \item $\varphi=0$ if $\varphi(x) = 0 \, \forall\, x \in V$
    \item $-\varphi(x) = \varphi(-x)$
    \item $\mathbb{1}\varphi(x) = \varphi(x)$
    \item $ab\varphi(x) = ba \varphi(x)$
  \end{enumerate}
\end{remark}\end{remarkbox}

\begin{definition}[Dual basis]
If $\{e_i\}_{i=1, \ldots, n}$ is a basis of $V$, then $\{ e^{*i} \}_{i=1}^n \subseteq V^*$ is called the \textit{dual basis} if $e^{*i}(e_j) = \delta^i_j$.
\end{definition}

If the field of $V$ is $\rfield$, we will denote the dual basis of $V^*$ also by $\{dx^i\}_{i \in \mathbb{N}}$ (see also the next examples).

\begin{remarkbox}\begin{remark}
We could prove that the dual basis is indeed a basis of the dual space, so $\dim(V) = \dim(\Lambda V^*)$. Check proposition \ref{basisprop1}.
\end{remark}\end{remarkbox}

\begin{tcolorbox}\begin{example}
  Let's consider three examples.
  \begin{enumerate}
    \item $p \in \rfield^3$, fixed. Let $V_p$ be the vector space $V_p \equiv \{ q-p \,|\, q \in \rfield^3 \}$. We will also denote it by $T_p\rfield^3$. We notice that $V_p = T_p\rfield^3 = \rfield^3$ (It is trivial to verify the inclusions "$\subseteq$" and "$\supseteq$"). Let $\left \{ (e_i)_p \right \}_{i=1,2,3}$ be a basis for $V_p$. If we consider the dual space $V_p^*$, we notice that $\left \{(dx^i)_p \right\}_{i=1,2,3}$ is a dual basis, where $(dx^i)_p (e_j) = \frac{\partial}{\partial x^j} x^i$. The index position is just a matter of notation, for now. Then, we can show that $V_p$ and $V_p^*$ are isomorphic, and the isomorphism is:
    \begin{align} \label{dualisomorph}
      g \colon V_p &\rightarrow V_p^* \\
      x &\mapsto g(x, \cdot) \nonumber
    \end{align}
    where $g$ is the Euclidean metric in $\rfield^3$ (in the sense of General Relativity), i.e. the standard dot product: $g(x, y) = x^i y^i$
    \item $V = \left \{ M \in M_2(\mathbb{C}) \, | \, M^{\dagger} = M, \tr(M) = 0 \right \}$ is a vector field on $\rfield$, where
    $$M= \left ( \begin{matrix}
    a &b \\
    c &d
  \end{matrix}\right ), M^{\dagger}= \left (\begin{matrix}
    \bar a &\bar b \\
    \bar c &\bar d
    \end{matrix} \right ), a, \ldots, d \in \mathbb{C}$$

  A basis for $V$ is $\{\sigma_i\}_{i=1,2,3}$, where
  $$\sigma_1 = \left ( \begin{matrix}
  0\, &1 \\
  1\, &0
\end{matrix}
  \right), \sigma_2 = \left ( \begin{matrix}
  0 &i \\
  -i &0
\end{matrix}
  \right), \sigma_3 = \left ( \begin{matrix}
  1 &0 \\
  0 &-1
\end{matrix}
  \right)$$
  and a dual basis is given by $e^{*i}(e_j) = \frac{1}{2} \tr(e_i, e_j)$ (using the matrix product).
  \item Quantum Mechanics: $$V = \{ f \colon \rfield^3 \rightarrow \rfield \, | \, ||f|| = \int_{\rfield^3} |f|^2 d^3x < \infty \} = L^2(\rfield^3)$$ is a vector space. If we consider the Laplacian operator $\Delta = \frac{\partial^2}{(\partial x^1)^2} + \frac{\partial^2}{(\partial x^2)^2} + \frac{\partial^2}{(\partial x^3)^2}$, then a basis is given by $\{e_n\} = \{\frac{f_n}{||f_n||}\}$, where $f_n$ eigenfunctions of $\Delta$: $\Delta f_n = \lambda_n fn$.
  A dual basis is given by $e^{*n} = \int_{\rfield^3} e_n$
  Notice that both $V$ and $V^*$ are infinite-dimensional spaces.
  Some notations:
  $$e_n = |f_n \rangle,\quad e^{*n} = \langle f_n |,\quad e^{*n}(e_m) = \braket{f_n | f_m}$$
  \end{enumerate}
\end{example}\end{tcolorbox}


\begin{remarkbox}\begin{remark}\label{vectorasder}
Every vector in $\rfield^n$ about a point $p \in \rfield^n$ (i.e. such that its origin is the point $p$) "can be seen" as a derivation (cf. def. \ref{derivation}), i.e. as a directional derivative of a function evaluated at the point $p$. For the sake of simplicity, we think $p=0$ (but the following results are true $\forall \, p \in \rfield^n$).
The sentence "can be seen" means that there is an isomorphism $\psi$ associating such vectors to such linear forms. Let's construct this isomorphism in the following steps:

\begin{enumerate}
\item Because of linearity, we just need to define the isomorphism for the basis vectors $\{ e_i \}_{i=1,\ldots,n}$ of $\rfield^n$.
\item Given the vector $e_j$ of the canonical basis, we associate it with the derivation $\partial_{x_j}$ in 0:
\begin{align}
\restrict{\partial_{x_j}}{p=0} \equiv \restrict{\frac{\partial}{\partial x_j}}{p=0} \colon C^{\infty}(\rfield^n) &\rightarrow \rfield \\
f &\mapsto \restrict{\frac{\partial}{\partial x_j}}{0} (f) \equiv \frac{\partial f}{\partial x_j} (0) \nonumber
\end{align}

In particular, if $\Der(\rfield^n) = \{ \text{derivations on }\rfield^n\}$ $ = \{ v\colon C^{\infty}\rightarrow \rfield, \\ \text{satisfying Leibnitz rule}\}$, then the map:
\begin{equation}
   \rfield^n \overset{\psi}{\longleftrightarrow}\Der(\rfield^n)
\end{equation}
such that $\psi(e_j) = \restrict{\partial_{x_j}}{p=0}, \forall \, e_j$ basis vector, and with $\restrict{\partial_{x_j}}{p=0}$ partial derivative with respect to the $j$-th component, defines a linear map. Indeed, it is linear because of linearity of derivations, and since we defined its behaviour on the basis vectors, it is also defined for every vector of $\rfield^n$. In general we have $\psi(v)=\restrict{\partial_{v}}{p=0}$, where $\restrict{\partial_v}{p=0}$ is the directional derivative with respect to $v$. Moreover, $\Der(\rfield^n)$ is a vector space and we used the double arrow above because $\psi$ is an isomorphism, i.e. a bijective map which preserves operations from one space to the other. Here every derivative is evaluated at $p=0$. We notice that the point $p$ itself is not important for the directional derivative (the \textit{direction} in which we differentiate is the same for every point of the space), but $p$ is meaningful when we \textit{evaluate} the derivative of the function at that point. Indeed, $\restrict{\partial_x (x^2)}{x=0} \not = \restrict{\partial_x (x^2)}{x=1}$, even if we are differentiating along the $x$-axis in both cases.
\end{enumerate}


What is more: given $V_0$, the set of all the vectors about $0$, we can consider its dual space $V_0^*$. %interpretation?
What is a possible dual basis?  We want to find linear forms $$e^{*i} \colon V_0 \rightarrow \rfield$$ such that $e^{*i}(e_j) = \delta_j^i$.
We have just seen that we can consider vectors as directional derivatives. So, given $e_j$ vector of the canonical basis, we will call it $\restrict{\frac{\partial}{\partial x^j}}{0}$ (because of the isomorphism, they are quite the same mathematical object).
Now, we want that
\begin{equation} \label{dualbasisdelta}
e^{*i}\left( \restrict{\frac{\partial}{\partial x^j}}{0} \right) = \delta_j^i
\end{equation}
First, let's consider the \textit{coordinate function}:
\begin{align} \label{coofunc}
x_j \colon \rfield^n \rightarrow & \rfield \\
v=(v_1, \ldots, v_n) \mapsto &v_j \nonumber
\end{align}
where $v_1, \ldots, v_j$ are the coordinates of the vector $v$ in the canonical basis. The linear form $x_j$ returns the $j$-th coordinate of a vector. So, given a vector $v \in \rfield^n$, every coordinate $v_j$ can be seen as $v_j = x_j(v)$.
Now, let's just define
\begin{equation} \label{dualdef}
e^{*i}\left( \restrict{\frac{\partial}{\partial x^j}}{0} \right) \equiv \restrict{\frac{\partial}{\partial x^j}}{0} x_i =\restrict{\frac{\partial}{\partial x^j} x_i}{0} = \delta_j^i
\end{equation}
Where $x_i$ is the coordinate function defined above (remember: $\partial_{x_j}$ is a derivation, so it must be applied to functions!). Now, it might seem that $e^{*i}$ does not take a vector as argument, but rather a function. Actually, this problem is solved by the isomorphism between vectors and directional derivatives proved above. If $\psi$ is the name of such isomorphism, we could slightly change the definition \eqref{dualdef} in order to solve this ambiguity:
\begin{equation}
e^{*i}(e_j) \equiv \psi(e_j) (x_i)
\end{equation}
where $x_i$ is the $i$-th coordinate function and
\begin{equation}
\psi(e_j) = \restrict{\frac{\partial}{\partial x^j}}{0} = \restrict{\partial_{x_j}}{0}
\end{equation}
The vectors of the dual basis will also be called
\begin{equation}
  dx^i \equiv e^{*i}
\end{equation}
This will be important later: we will define exterior forms of degree $k$ and we'll use both notations. The set of all these forms is $\Lambda^k V^*$, and its basis is given by products (in particular, exterior products) of $e^{*i_1}, \ldots, e^{*i_k}$ (i.e. $dx^{i_1}, \ldots, dx^{i_k}$).
\end{remark}\end{remarkbox}

\begin{definition} [Set of vector fields] \label{setsfields}
We denote by $\mathfrak{X}(\rfield^n)$ the set of all possible vector fields in $\rfield^n$, i.e.
\begin{align}
\mathfrak{X}(\rfield^n) = \text{Der}\mathbb{F}(\rfield^n) \equiv\{ v \colon \mathbb{F}(\rfield^n) \rightarrow \mathbb{F}(\rfield^n) \text{ such that } \nonumber \\
 v \text{ is } \rfield\text{-linear and } v(fg) = v(f)g + fv(g)  \}
\end{align}
where $\mathbb{F} \equiv \{$ functions $f\colon \rfield^n \rightarrow \rfield \}$.
\end{definition}

\begin{remarkbox}\begin{remark} [Fields vs. Derivations] \label{fieldsvsder}
  If $v$ is a vector field in $\rfield^n$, then $v$ assigns a vector to another vector of $\rfield^n$. So $v \colon \rfield^n \rightarrow \rfield^n$. So, the set of all vector fields should be (we will use a different symbol to denote it):
  $$X(\rfield^n) = \left \{ v \colon \rfield^n \rightarrow \rfield^n \right \}$$
  However, the definition \ref{setsfields} is a bit different. Why?
  The fact is, we can consider a vector of $\rfield^n$ as a directional derivative, cf. remark \ref{vectorasder} (we are not considering any fixed point here, but the results do not change). Now a derivation, as defined in def. \ref{derivation}, is a map $v \colon C^{\infty}(\rfield^n) \rightarrow \rfield$, i.e. we can say that a derivation is a very smooth element of $\mathbb{F}(\rfield^n) = \{ \text{functions } f \colon \rfield^n \rightarrow \rfield\}$. So, $\mathfrak{X}(\rfield^n) \cong X(\rfield^n)$ because we can associate a derivation of $\mathfrak{X}(\rfield^n)$ to each vector of $X(\rfield^n)$, and vice versa. Then we also explained why in the definition of $\mathfrak{X}( \rfield^n)$ every element must be $\rfield$-linear and satisfy the Leibnitz rule: it follows from the definition of derivations.

  Now, a question arises: given $v$ vector field, should we write $v(p)$ (i.e. it takes vectors as argument) or should we write $v(f)$ (i.e. it takes smooth functions as arguments)? The answer is: it depends on the case, since they are two different "$v$"s. Which is, we will use vectors when we think of $v$ as a function who takes elements of $\rfield^n$, and we will use functions in the other case. And we can choose which case to use, since we can identify every vector field with a derivation, and every derivation with a vector field (for more info, see pag. 181 of \cite{Lee}). Now, let us analyze how $v(f)$ is made in the latter case.
  Given $v \in \mathfrak{X}(\rfield^n), f: \rfield^n \rightarrow \rfield$, we define the function $v(f)$ as
  \begin{align} \label{vfpdef}
    v(f) \colon \rfield^n &\rightarrow \rfield \nonumber \\
    p & \mapsto v(f)(p) \equiv v_p f
  \end{align}
  Now, in coordinates:
  \begin{equation} \label{vfieldincoord}
    v(f)(p) = v_p f = v^i(p)(e_i)_p f = v^i(p) \restrict{\frac{\partial}{\partial x^i}}{p} f = v^i(p) \frac{\partial f}{\partial x^i} (p)
  \end{equation}
  where we used summation convention, and the fact that every vector basis $e_i$ can be seen as $\restrict{\partial_{x_i}}{p}$. We defined it in the right way because, as expected, we found that $v_p(f)$ is the directional derivative of $f$ in the direction of $v$, evaluated at $p$. So, in brief:
  \begin{itemize}
    \item $v(f)(p)$ is a number
    \item $v(f)(\cdot)$ is a function from $\rfield^n$ to $\rfield$
    \item $v(\cdot)$ is a function from $\mathbb{F}(\rfield^n)$ to $\mathbb{F}(\rfield^n)$
  \end{itemize}
We also notice that the mathematical object $e_i$ is not much different from $\restrict{(e_i)}{p}$ in this case: there is no difference if we think about them as directions, but it makes a difference if we think about them as directional derivatives, because the latter notation gives info about the point in which the derivative is evaluated. So, we add the pedix "$p$" in order to make the isomorphism between vectors and directional derivatives more explicit. Check also the remark \ref{pointorvec}.
\end{remark}\end{remarkbox}

\begin{figure}[h]
     \centering
     \includegraphics[width=.6\linewidth]{images/vfield_e1.pdf}
     \caption{Vector field v=$e_1=\partial_x$} \label{Fig:vfield_e1}
\end{figure}

%\begin{remarkbox}\begin{remark}
%So, if $v \in \mathfrak{X}(\rfield^n)$, then $v(f)(p) = \restrict{v(f)}{p} = v^i(p)\restrict{(e_i)}{p} = v^i(p) \restrict{\frac{\partial}{\partial x^i}}{p} f = v^i(p) \frac{\partial}{\partial x^i} f(p)$, where
%\end{remark}\end{remarkbox}
\section[Exterior Product and Generalisation]{\crule[red!50!white]{0.3cm}{0.4cm}  Exterior Product and Generalisation}

\begin{definition} [Exterior form of degree $k$]
Given a vector space $V$ on a field $\mathbb{K}$, with $\dim(V) = n$, and with $k \le n$, an \textit{exterior form of degree $k$} (or $k$-linear form, or $k$-form) is a map $\omega$:
\begin{equation*}
\omega \colon \underbrace{V \times \ldots \times V}_{\text{k times}} \rightarrow \mathbb{K}
\end{equation*}
such that
\begin{equation}
\omega(v_1, \ldots, v_k) = \sgn(\pi)\,\omega(v_{\pi(1)}, \ldots, v_{\pi(k)})
\end{equation}
and such that $\omega$ is multilinear. Where $\pi$ is a permutation of $k$ elements, i.e. $\pi \in S_k$, and $\sgn(\pi)$ is the sign of the permutation.
We will also write $\omega \in \Lambda ^k V^*$.
\end{definition}

\begin{definition}[Exterior product between two 1-forms] \label{extprod}
Given a vector space $V$ on a field $\mathbb{K}, \dim(V) \ge 2$, and given $\varphi^1, \varphi^2 \in \Lambda V^* $, then we define the exterior product (or wedge product) $\wedge$ as:
\begin{align*}
\wedge \colon \Lambda V^* \times \Lambda V^* &\rightarrow \Lambda^2 V^*  \\
(\varphi^1, \varphi^2) &\mapsto \varphi^1 \wedge \varphi^2
\end{align*}
where:
\begin{equation*}
\varphi^1 \wedge \varphi^2 (x_1, x_2) = \varphi^1(x_1) \varphi^2(x_2) - \varphi^2(x_1) \varphi^1(x_2) = \det(\varphi^i (x_j))
\end{equation*}
for $i, j=1,2$.
\end{definition}

\begin{remarkbox}\begin{remark}[Exterior product between $k$ 1-forms]
The exterior product $\wedge$ that we defined for $k=2$ in def. \ref{extprod} gives an exterior form of degree 2. We want to generalize it for $k$ vector spaces. In order to extend the definition, we want it to give an exterior form of degree $k$, so:
\begin{align*}
  \wedge \colon \underbrace{\Lambda V^* \times \ldots \times \Lambda V^*}_{k \text{ times}} &\rightarrow \Lambda^k V^*  \\
  (\varphi^1, \ldots, \varphi^k) &\mapsto \varphi^1 \wedge \ldots \wedge \varphi^k
\end{align*}
where, given $(x_1, \ldots, x_k) \in \underbrace{V \times \ldots \times V}_{k \text{ times}}$:
\begin{equation} \label{detformula}
  \varphi^1 \wedge \ldots \wedge \varphi^k (x_1, \ldots, x_k) =  \det\left (\varphi^i(x_j) \right)
\end{equation}
This is a particular case of an exterior $k$-form (because the sign of determinant changes if we swap two rows or two columns).
Now let's consider and explicit computation using the determinant, in $\rfield^3$, with coordinates $x, y, z$, and with $\varphi^1, \varphi^2, \varphi^3$ corresponding to the three elements of the dual basis $dx \equiv e^{*1} = dx^1, dy \equiv e^{*2} = dx^2$ and $ dz \equiv e^{*3} = dx^3$:
\begin{equation*}
(dx \wedge dy \wedge dz)(x, y, z) = \det\left (dx^i(e_j) \right) = \det \left( 
\begin{matrix}
dx(x) & dx(y) & dx(z) \\
dy(x) & dy(y) & dy(z) \\
dz(x) & dz(y) & dz(z)
\end{matrix}
\right )
\end{equation*}
and we can use that $dx^i(e_j) = \delta^i_j$. For instance, we can verify that the 3-form $dx \wedge dy \wedge dz$ gives 0 if two coordinates of the input are repeated:
\begin{equation*}
(dx \wedge dy \wedge dz)(x, x, z) = \det \left( 
\begin{matrix}
1 & 1 & 0 \\
0 & 0 & 0 \\
0 & 0 & 1
\end{matrix}
\right ) = 0
\end{equation*}
\end{remark}\end{remarkbox}



\begin{proposition}\label{basisprop1}
  If $\{e_i\}_{i=1,\ldots,n}$ is a basis in $V$, then $\{e^{*i_1} \wedge \ldots \wedge e^{*i_k}\}_{i_1 < \ldots < i_k, k \le n}$ forms a basis of $\Lambda ^k V^*$
\end{proposition}
\begin{proof}[Proof]
In the statement we implicitly assumed that the elements of $\{e^{*i_1} \wedge \ldots \wedge e^{*i_k}\}$ are defined as usual, for instance $e^{*i_1}(e_{j_1}) = \delta^{i_1}_{j_1}$. In order to prove that it is a basis, we need to prove that the element of the basis are linearly independent and that they span the entire space.
\begin{itemize}
\item Linear independence: If 
$$0 = \sum\limits_{i_1 < \ldots < i_k} a_{i_1 \cdots i_k} (e^{*i_1} \wedge \ldots \wedge e^{*i_k})(e_{j_1}, \ldots, e_{j_k}), \quad \forall \, (e_{j_1}, \ldots, e_{j_k})$$
then $a_{i_1 \cdots i_k} = 0, \forall\, i_1, \ldots, i_k$. Indeed, if it is true $\forall \, (e_{j_1}, \ldots, e_{j_k})$, then we can choose $e_{j_1}, \ldots, e_{j_k}$ such that $a_{i_1 \cdots i_k} = 0, \forall\, i_1, \ldots, i_k$. e.g.:
\begin{align*}
0 &= a_{i_1 \cdots i_k} \underbrace{(e^{*i_1} \wedge \ldots \wedge e^{*i_k}) (e_{i_1}, \ldots, e_{i_k})}_{= 1} + \\
& + \sum\limits_{\substack{j_1 < \ldots < j_k \\
j_1 \not = i_1, j_2 \not = i_2, \ldots}} a_{j_1 \cdots j_k} \underbrace{(e^{*j_1} \wedge \ldots \wedge e^{*j_k}) (e_{i_1}, \ldots, e_{i_k})}_{= 0} = a_{i_1 \cdots i_k}
\end{align*}
The second term is zero because $(e^{*j_1} \wedge \ldots \wedge e^{*j_k}) (e_{i_1}, \ldots, e_{i_k}) = \det \left (e^{*j_l} (e_{i_l}) \right)$ and the matrix $e^{*j_l} (e_{i_l})$ has null coefficients on the main diagonal (because $j_1 \not = i_1, j_2 \not = i_2, \ldots$) and null coefficients on the lower triangle (because we have the ordering $j_1 < \ldots < j_k$). Thus, we are computing the determinant of an upper triangular matrix with null elements on the diagonal, i.e. the determinant is 0.
Therefore, $a_{i_1 \cdots i_k} = 0$.
So, we proved the linear independence for the case in which we consider the basis vectors $e_{j_1}, \ldots, e_{j_k}$. Because of multilinearity, the result holds for all vectors
 $(x_1, \ldots, x_k)$ as well.
\item Completeness: given $\psi \in \Lambda^k V^*$, we want to prove that there exists a coefficient $a_{i_1 \cdots i_k}$ such that $\psi = \sum\limits_{i_1 < \ldots < i_k} a_{i_1 \cdots i_k} (e^{*i_1} \wedge \ldots \wedge e^{*i_k})$, i.e. we want to prove that we can write any element of $\Lambda^k V^*$ as a linear combination of elements of $\{e^{*i_1} \wedge \ldots \wedge e^{*i_k}\}$. Let's consider $(v_1, \ldots v_k) \in V \times \ldots \times V$. Then:
\begin{align*}
\psi(v_1, \ldots, v_k) &\overset{(1)}{=} \sum\limits_{i_1 = 1}^k \sum\limits_{i_2 = 1}^k \ldots \sum\limits_{i_k = 1}^k x_{i_1} \cdot \ldots \cdot x_{i_k} \psi(e_{i_1}, \ldots, e_{i_k}) \overset{(2)}{=} \\
&= \sum\limits_{i_1 < \ldots < i_k} x_{i_1} \cdot \ldots \cdot x_{i_k} \psi(e_{i_1}, \ldots, e_{i_k}) \overset{(3)}{=} \\
&= \sum\limits_{i_1 < \ldots < i_k} x_{i_1} \cdot \ldots \cdot x_{i_k} \psi(e_{i_1}, \ldots, e_{i_k}) (e^{*i_1} \wedge \ldots \wedge e^{*i_k}) (e_{i_1}, \ldots, e_{i_k}) \overset{(4)}{=} \\
&= \sum\limits_{i_1 < \ldots < i_k} \psi(e_{i_1}, \ldots, e_{i_k}) (e^{*i_1} \wedge \ldots \wedge e^{*i_k}) (x_{i_1}e_{i_1}, \ldots, x_{i_k}e_{i_k}) \overset{(5)}{=} \\
&= \sum\limits_{i_1 < \ldots < i_k} a_{i_1 \cdots i_k}(e^{*i_1} \wedge \ldots \wedge e^{*i_k}) (v_1, \ldots, v_k) 
\end{align*}
Where we used: (1) $v_1 = \sum\limits_{i_1 = 1}^k x_{i_1}e_{i_1}$ in the $\{e_i\}_{i=1}^n$ basis, (2) we can simplify the sum since $\psi=0$ if two entries are repeated, (3) $1 = (e^{*i_1} \wedge \ldots \wedge e^{*i_k}) (e_{i_1}, \ldots, e_{i_k})$, (4) multilinearity, (5) $0 = e^{*i}(e_j)$ if $i \not = j$ and $a_{i_1 \cdots i_k} \equiv \psi(e_{i_1}, \ldots e_{i_k})$. \qedhere
\end{itemize}
\end{proof}

\begin{remarkbox}\begin{remark}
  The above proposition proves that $\dim(\Lambda^k V^*)=\binom{n}{k}$, because through the expression "$i_1 < \ldots < i_k, \, k \le n$" we are selecting subsets of $k$ elements (subsets, not $k$-tuples, because the order of the elements is externally fixed) from a bigger set of $n$ elements. This is exactly the definition of the binomial coefficient $\binom{n}{k}$. Moreover, it means that any $\alpha \in \Lambda^k V^*$ can be written as:
  \begin{equation*}
    \alpha = \sum\limits_{i_1 < \ldots < i_k} a_{i_1 \cdots i_k} e^{*i_1} \wedge \cdots \wedge e^{*i_k}
  \end{equation*}
  where $a_{i_1 \cdots i_k} \in \mathbb{K}$, $\mathbb{K}$ field of the vector space.
\end{remark}\end{remarkbox}

Now, we want to define the exterior product between a $k$-form and a $p$-form (and it will return a ($p+k$)-form).

\begin{definition}[Exterior product between a $k$-form and a $p$-form]
  Given $\alpha \in \Lambda ^k V^*, \beta \in \Lambda^p V^*$, the exterior product between them is defined as:
  \begin{align}
    \wedge \colon \Lambda ^k V^* \times \Lambda ^p V^* &\rightarrow \Lambda^{k+p} V^* \nonumber \\
    (\alpha, \beta) &\mapsto \alpha \wedge \beta \nonumber
  \end{align}
  with:
  $$\alpha \wedge \beta = \sum\limits_{\substack{i_1 < \ldots < i_k \\ j_1 < \ldots < j_p}} \alpha_{i_1 \cdots i_k}\beta_{j_1 \cdots j_k} e^{*i_1} \wedge \cdots \wedge e^{*i_k} \wedge e^{*j_1} \wedge \cdots \wedge e^{*j_k} $$
  where $\alpha_{i_1 \cdots i_k},\beta_{j_1 \cdots j_k} \in \mathbb{K}$
\end{definition}


\begin{tcolorbox}\begin{example}[Oriented area] \label{orarea}
  Let's consider the following examples:
  \begin{enumerate}
    \item $V = \rfield^3 \times \rfield^3$ with cartesian coordinates.
    $$\varphi \equiv dx^1 \wedge dx^2 + dx^2 \wedge dx^4$$
    $\varphi$ is a sum of two 2-forms. They are 2-forms (and not 1-forms, nor 3-forms, nor 4-forms, ...) because they are written as wedge product ("$\wedge$") of \textbf{two} elements of the dual basis. Remember that we denote the elements of the dual basis either by $e^{*i}$ or $dx^i$ for some $i$.
    Let's compute $\varphi(e_i, e_j)$. We can use the determinant formula \eqref{detformula}: 
\begin{align*}
    \varphi(e_i, e_j) = &dx^1(e_i) dx^2(e_j) + dx^2(e_i)dx^4(e_j) - dx^2(e_i)dx^1(e_j) + \\ - &dx^4(e_i)dx^2(e_j)
    \end{align*}
    \item $V = \rfield^2$ with cartesian coordinates $x^1, x^2$.
    $$\varphi \equiv dx^1 \wedge dx^2$$
    Let's compute $\varphi(ae_1, be_2)$, where $ a,b \in \rfield$:
    \begin{align*}
    \varphi(ae_1, be_2) &= dx^1 \wedge dx^2(ae_1, be_2) = ab\,  dx^1 \wedge dx^2(e_1, e_2) = \\
    &= ab \, (dx^1(e_1)dx^2(e_2) - \cancel{dx^2(e_1)} \cancel{dx^1(e^2)}) = ab = \\
    &= \text{oriented area of the rectangle of sides } a \text{ and } b
  \end{align*}
  \end{enumerate}

\end{example}\end{tcolorbox}

There are some interesting properties about $k$-forms:
\begin{proposition}
  $\alpha \in \Lambda^k V^*, \beta \in \Lambda^p V^*, \gamma \in \Lambda ^q V^*$, then:
  \begin{enumerate}
    \item $(\alpha \wedge \beta) \wedge \gamma = \alpha \wedge (\beta \wedge \gamma)$
    \item $\alpha \wedge (\beta + \gamma) = \alpha \wedge \beta + \alpha \wedge \gamma$
    \item $\alpha \wedge \beta = (-1)^{kp} \beta \wedge \alpha$
  \end{enumerate}
\end{proposition}
\begin{proof}[Sketch of the proof]
	\begin{enumerate}
	\item Writing the forms in terms of the basis, we only need to prove that the property (1) holds for the elements of the dual basis. Moreover, by multilinearity, we just need to prove it for the elements of the dual basis applied to a basis $\{e_i\}$ of $V$. Thus, the proof for (1) is over because this property is true for the determinant: when we compute it, we can start from any row or column.
	\item By writing the forms in terms of the basis as before.
	\item  Again, by writing the forms in terms of the basis we have a sequence of wedge products like
	$$e^{*i_1} \wedge \ldots \wedge e^{*i_k} \wedge e^{*j_1} \wedge \ldots \wedge e^{*j_p}$$
	If we swap two elements of the product we get a factor "$-1$". In order to have the right hand side we need to "move $e^{*j_1}$ on the left $k$ times", so we get a factor $(-1)^k$. Same for $e^{*j_2}$: we need to move it on the left $k$ times and so we get an extra factor $(-1)^k$. Again, we repeat the procedure for all the elements up to $e^{*j_p}$. At the end, we get a factor $(-1)^{kp}$.
	\end{enumerate}
\end{proof}

\section[Differential Forms]{\crule[purple!50!white]{0.3cm}{0.4cm}  Differential Forms}
\begin{definition} [Field of exterior forms, geometric definition] \label{geomkforms}
  (A field of) exterior forms of degree $k$, $k \le n$ is a map $\omega$ that associates to each point $p \in V$ an element $\omega(p) \in \Lambda^{k}V_p^*$.
  Choosing a basis, we have:
  \begin{equation} \label{kformdef}
    \omega(p) = \sum\limits_{i_1 < \ldots < i_k} \underbrace{a_{i_1 \cdots i_k}(p)}_{\text{now it is a function!}} e^{*i_1} \wedge \cdots \wedge e^{*i_k}
    \end{equation}
    $\omega$ is a differential form if $a_{i_1 \cdots i_k}$ are differentiable. The set of differential $k$-forms is denoted by $\Omega^k(\rfield^n)$.
\end{definition}

Another (equivalent) definition:

\begin{definition} [Algebraic definition of differential $k$-form] \label{algkforms}
  A differential $k$-form is a map:
  \begin{align}
    \omega \colon \underbrace{\mathfrak{X}(\rfield^n) \times \ldots \times \mathfrak{X}(\rfield^n)}_{k \text{ times}} \rightarrow \mathbb{F}(\rfield^n)
  \end{align}
  $C^\infty(\rfield^n)$-linear and alternating.
\end{definition}

\begin{remarkbox}\begin{remark}
  To show the equivalence of the two definition of differential $k$-forms we just need to show that:
  \begin{equation}
    \omega(p)(v_1, \ldots, v_k) = \omega(v_1, \ldots, v_k)(p)
  \end{equation}
  See also the exercise 1, problem sheet 3.
\end{remark}\end{remarkbox}

We want to generalize the concept of differential of a function.
\begin{definition}[Differential] \label{differential}
  Let $f$ be a function $f \colon U \subseteq \rfield^n \rightarrow \rfield$, $f$ differentiable. Let $v \in \mathfrak{X}(\rfield^n) = \Der \mathbb{F}(\rfield^n)$. The exterior derivative of $f$ is its differential $d$, defined as a 1-form such that:
  \begin{equation}
    df(v) = v(f)
  \end{equation}
\end{definition}

\begin{remarkbox}\begin{remark}[differential expression in coordinates]
  We want to verify that the above definition of differential is equivalent to our usual definition for $C^1(\rfield^n)$ function, which is:
  \begin{equation}
    df = \sum\limits_{i=1}^n \frac{\partial f}{\partial x_i} dx^i = \frac{\partial f}{\partial x_i} dx^i
  \end{equation}
  In order to prove that, we first consider a pointwise definition. Given $p \in \rfield^n$:
  \begin{equation} \label{diffdefp}
    df_p(v) = v(f), \forall\,\, v \in T_p \rfield^n \cong \rfield^n
  \end{equation}
  ($T_p \rfield^n$ is the tangent space to $\rfield^n$ at $p$). Now, we can write $v(f)$ in coordinates (the gray part is the one we don't care about):
  \begin{equation} \label{diffcoordinates}
    df_p = \textcolor{lightgray}{v(f) = \text{ }} v_i(p) (\lambda^i)_p
  \end{equation}
  where $(\lambda^i)_p$ is a dual basis at $p$ (later, we will prove that $(\lambda^i)_p = (dx^i)_p$). Now, applying $df$ to a particular vector (i.e. directional derivative) at $p$:
  \begin{equation} \label{diffpart1}
    df_p \left (\restrict{\frac{\partial}{\partial x^i}} {p} \right) = v_i(p)
  \end{equation}
  where we used the property of the dual basis
  \begin{equation}
    (\lambda^i)_p \restrict{\frac{\partial}{\partial x^j}} {p} = \delta^i_j
  \end{equation}
   and then:
   \begin{equation}
   \textcolor{lightgray}{df_p \left (\restrict{\frac{\partial}{\partial x^i}} {p} \right) = \text{ }} v_i(p) (\lambda^i)_p \restrict{\frac{\partial}{\partial x^i}} {p} = v_i(p)
 \end{equation}
   On the other hand, by definition \eqref{diffdefp} we know that:
   \begin{equation} \label{diffpart2}
     df_p \left (\restrict{\frac{\partial}{\partial x^i}} {p} \right)  = \restrict{\frac{\partial}{\partial x^i}} {p} f = \frac{\partial f}{\partial x^i} (p)
   \end{equation}
   Hence, using \eqref{diffpart1} and \eqref{diffpart2} we get:
   \begin{equation}
     v_i (p) = \frac{\partial f}{\partial x^i} (p)
   \end{equation}
   Then, by the expression of differential in coordinates \eqref{diffcoordinates}:
   \begin{equation}
     df_p = \frac{\partial f}{\partial x^i} (p) (\lambda^i)_p
   \end{equation}
   Applying the definition to $f = x^j$ (coordinate function, as defined in \eqref{coofunc}), we get:
   \begin{equation}
     df_p = \frac{\partial f}{\partial x^i} (p) (\lambda^i)_p = \frac{\partial f}{\partial x^i} (p) (dx^i)_p
   \end{equation}
   And then:
   \begin{equation}
     df = \frac{\partial f}{\partial x^i} dx^i
   \end{equation}
   Indeed, if $f = x^j$ then, as before:
   \begin{equation}
     (dx^j)_p = \frac{\partial x^j}{\partial x^i} (p) \restrict{(\lambda^i)}{p} = \delta^i_j \restrict{(\lambda^i)}{p} = \restrict{(\lambda^j)}{p}
   \end{equation}
Pay attention: what we did here is a bit different from what we did for the definition $\ref{vfpdef}$ of a vector field applied to a function. In this case, $p$ is the point where we fixed our vector, whereas in the other case $p$ was the point where we wanted to evaluate the directional derivative of $f$.
\end{remark}\end{remarkbox}

In the above definition, $f$ was a 0-form (i.e. a function). What is the generalization of the differential to $k$-forms?

\begin{definition}[Exterior derivative] \label{extder}
  If $k > 0$, then the exterior derivative (acting on $k$-forms) is a map
  \begin{align*}
    d \colon \Omega^k(\rfield^n) & \rightarrow \Omega^{k+1}(\rfield^n) \\
    \omega &\mapsto d (\omega) \equiv d \omega
  \end{align*}
  where
  \begin{equation*}
    d \omega = \sum\limits_{j_1 < \ldots < j_k} \left (d a_{j_1, \ldots, j_k} \right) \wedge dx^{j_1} \wedge \ldots \wedge dx^{j_k}
  \end{equation*}
With $d a_{j_1, \ldots, j_k}$ differential of the function $a_{j_1, \ldots, j_k}$.
\end{definition}

\begin{tcolorbox}\begin{example}[Computation of $d\omega$]
  Let's consider the following examples:
  \begin{enumerate}
    \item Let $\omega$ be a 2-form on $\rfield^3$ (coordinates $x^1, x^2, x^3$):
    $$\omega = dx^1 \wedge dx^2 + x^2 dx^1 \wedge dx^3$$
    Then
    $$d \omega = dx^2 \wedge dx^1 \wedge dx^3$$
    where we used that $d (dx^1 \wedge dx^2) = 0$ because there is no 3-form on a 2-dimensional space (otherwise, we can use that $d^2=0$, but we still have to prove it!)
    \item In $\rfield^n$, let's consider:
    $$\omega = x^2 dx^1, \quad d \omega = dx^2 \wedge dx^1$$
    where we computed  $d \omega$ by using $d g(v) = v(g)$ for a function $g$ and a vector field $v$. In fact, if $u,v \in \mathfrak{\rfield^n}$, then by definition of exterior product we have:
    $$d \omega (u, v) = dx^2(u)dx^1(v) - dx^2(v)dx^1(u)$$
    On the other hand, using $dx^2(u)=u(x^2), v(x^2) = dx^2(v)$ (where $x^2$ is a function, the coordinate function defined in \eqref{coofunc}) we also have:
    $$d \omega (u, v) = u(x^2)dx^1(v) - v(x^2)dx^2(u)$$
  \end{enumerate}
\end{example}\end{tcolorbox}

Now, some properties:
\begin{proposition}[Properties of exterior derivatives]
  $\omega_1 \in \Omega^k(\rfield^n), \omega_2 \in \Omega^p(\rfield^n)$. Then:
  \begin{itemize}
    \item $d(\omega_1 +\omega_2) = d\omega_1 + d\omega_2$
    \item $d(\omega_1 \wedge \omega_2) = d\omega_1 \wedge \omega_2 + (-1)^k \omega_1 \wedge d\omega_2$
    \item $d(d\omega_1)  = 0 = d(d\omega_2)$
  \end{itemize}
\end{proposition}
\begin{proof}[Sketch of the proof]
$ $
	\begin{itemize}
	\item We have 
	$$d \omega_1 = \sum\limits_{j_1 < \ldots < j_k} \left (d a^{(1)}_{j_1 \cdots j_k} \right) \wedge dx^{j_1} \wedge \ldots \wedge dx^{j_k}$$
	and similarly for $\omega_2$. Then we use the linearity of the differential: $d(a^{(1)} + a^{(2)}) = da^{(1)} + da^{(2)}$.
	\item We have
	$$d(\omega_1 \wedge \omega_2) = \sum\limits_{\substack{j_1 < \ldots < j_k \\ i_1 < \ldots < i_k}} d \left (a^{(1)}_{j_1 \cdots j_k} a^{(2)}_{i_1 \cdots i_k} \right) \wedge dx^{j_1} \wedge \ldots \wedge dx^{j_k} \wedge dx^{i_1} \wedge \ldots \wedge dx^{i_k}$$
	Using the product rule we have a term $\left ( da^{(1)} \right ) a^{(2)} + a^{(1)} \left(da^{(2)}\right)$ inside the sum. Now, $\left( da^{(1)} \right) a^{(2)}$ gives the term $d \omega_1 \wedge \omega_2$. In order to get the term $(-1)^k \omega_1 \wedge d \omega_2$ we consider $a^{(1)}\left(da^{(2)} \right) \wedge dx^{j_1} \wedge \ldots \wedge dx^{j_k} \wedge dx^{i_1} \wedge \ldots \wedge dx^{i_k}$ inside the sum and we move the factor $da^{(2)}$ to the right for $k$ times. So we get the factor $(-1)^k$) and the wedge product $\omega_1 \wedge d \omega_2$.
	\item Let's consider
	$$d(d \omega) = \sum\limits_{j_1 < \ldots < j_k} d \left (da_{j_1 \cdots j_k} \right) \wedge dx^{j_1} \wedge \ldots \wedge dx^{j_k}$$
	Now, 
	$$da_{j_1 \cdots j_k} = \left(\frac{\partial}{\partial x^l} a_{j_1 \cdots j_k} \right)dx^l$$
	and:
	$$\left[d \left( \frac{\partial}{\partial x^l} a_{j_1 \cdots j_k}\right) \right] dx^l = \underbrace{\frac{\partial^2}{\partial x^m \partial x^l} a_{j_1 \cdots j_k}}_{\text{symmetric in } l \leftrightarrow m} \, \underbrace{dx^m \wedge dx^l}_{\text{antisymmetric in } l \leftrightarrow m}$$
	Then we get a minus sign if we exchange $l$ and $m$. Moreover we can compute  $\left[d \left( \frac{\partial}{\partial x^m} a_{j_1 \cdots j_k}\right) \right] dx^m$ in the same way exchanging the roles of $l$ and $m$, and we wouldn't get any minus sign. Then we have that $d(d \omega) = -d (d \omega) \Rightarrow d(d \omega) = 0$.
	\end{itemize}
\end{proof}

\begin{remarkbox}\begin{remark} \label{abusenot1}
  In the above proposition, we claimed that $d(d\omega)  = 0$ if $\omega \in \Omega^k(\rfield^n)$. The notation here is not the most precise, since the inner "d" is acting on a $k$-form, whereas the outer "d" is acting on a ($k+1$)-form (so, even if they share the same name, they are different maps). However the behaviour of both "d"s is clear, so we will continue with this abuse of notation.
\end{remark}\end{remarkbox}

\begin{remarkbox}\begin{remark}
  The exterior derivative increases the degree of a $k$-form by 1 (the $k$-form becomes a ($k+1$)-form). Can we get backwards, which is, can we decrease the degree of a $k$-form? Yes, using the interior derivative.
\end{remark}\end{remarkbox}

\begin{definition}[Interior derivative] \label{intder}
  Let $z$ be a vector field on $\rfield^n$, i.e. $z \in \mathfrak{X}(\rfield^n)$, then we define the \textit{interior derivative} $i_z$ (acting on differential $k$-forms) as the map:
  \begin{align}
    i_z \colon \Omega^k(\rfield^n) &\rightarrow \Omega^{k-1}(\rfield^n) \\
    \omega &\mapsto i_z(\omega) \equiv i_z \omega \nonumber
  \end{align}
  such that
  $$(i_z \omega) (v_1, \ldots, v_{k-1}) = \omega(z, v_1, \ldots, v_{k-1}), \forall \, v_i \in \mathfrak{X}(\rfield^n)$$
  $i_z \omega$ is also called  \textit{contraction} or \textit{interior multiplication}. Another notation for $i_z \omega$ is $z \, \lrcorner \, \omega$.
\end{definition}

\begin{tcolorbox}\begin{example}[Some computations]
  In $\rfield^2$, $e_x, e_y$ basis vectors (that can be seen as vector fields):
  $$i_{e_x}(dx \wedge dy) = dy$$
  $$i_{e_y}(dx \wedge dy) = - dx$$
\end{example}\end{tcolorbox}

\begin{remarkbox}\begin{remark}
  In the definition \ref{intder} above, we used the algebraic definition of differential $k$-forms, i.e. definition \ref{algkforms}
\end{remark}\end{remarkbox}

Now some properties for interior derivatives.

\begin{proposition}
  $\omega \in \Omega^k(\rfield^n), \eta \in \Omega^p(\rfield^n), z \in \mathfrak{X}(\rfield^n)$, then:
  \begin{itemize}
    \item $i_z (\omega \wedge \eta) = (i_z \omega) \wedge \eta + (-1)^k \omega \wedge (i_z \eta)$
    \item $i_z^2 w = i_z(i_z \omega) = 0$
  \end{itemize}
\end{proposition}
\begin{proof}[Sketch of the proof]
$ $
\begin{itemize}
\item First, we assume $k+p \le n$, otherwise $\omega \wedge \eta$ would be 0. Then, let's consider a basis $\mathcal{B} = \{e_1, \ldots, e_n\}$ in $\rfield^n$ which is positively oriented (i.e. if $A$ is the matrix that we use to change coordinates from the canonical basis to $\mathcal{B}$, we have $\det A > 0$). Let's choose $\mathcal{B}$ such that the vector field $e_1$ is the tangent vector to the vector field $z$ in one point (we will make a pointwise reasoning). Moreover, $\{dx^l\}_{l=1}^n$ is the dual basis with respect to the canonical basis, and $\{e^{*l}\}_{l=1}^n$ is the dual basis with respect to $\mathcal{B}$. We have 
$$\omega = \sum\limits_{i_1 < \ldots i_k} a_{i_1 \cdots i_k} dx^{i_1} \wedge \ldots \wedge dx^{i_k}$$
and
$$\omega \wedge \eta = \sum\limits_{\substack{i_1 < \ldots i_k \\ j_1 < \ldots j_p }} a_{i_1 \cdots i_k} b_{j_1 \cdots j_p} dx^{i_1} \wedge \ldots \wedge dx^{i_k} \wedge dx^{j_1} \wedge \ldots \wedge dx^{j_p}$$
Then either:
\begin{enumerate}
\item $dx^{i_r} = e^{*1}$ for some $i_r, r= 1, \ldots, k$, or
\item $dx^{j_r} = e^{*1}$ for some $j_r, r= 1, \ldots, p$, or
\item there are no $i_r, j_r$ such that "$dx^{i_r} = e^{*1}$ or $dx^{j_r} = e^{*1}$"
\end{enumerate}
So, $i_z (\omega \wedge \eta) = (\omega \wedge \eta)(z, \ldots)$ is either equal to:
\begin{enumerate}
\item $\sum\limits_{\substack{i_1 < \ldots i_k \\ j_1 < \ldots j_p }} a_{i_1 \cdots i_k} b_{j_1 \cdots j_p} i_z \left( dx^{i_1} \wedge \ldots \wedge dx^{i_k} \right ) \wedge dx^{j_1} \wedge \ldots \wedge dx^{j_p}$ ,because the only non-zero term comes from $i_z(e^{*1}) = e^{*1}(z) = e^{*1}(e_1) = 1$ (locally), and we know that the non-zero term must be hidden in the first $k$ wedge products,
\item $\sum\limits_{\substack{i_1 < \ldots i_k \\ j_1 < \ldots j_p }} (-1)^k a_{i_1 \cdots i_k} b_{j_1 \cdots j_p}  dx^{i_1} \wedge \ldots \wedge dx^{i_k}  \wedge i_z \left( dx^{j_1} \wedge \ldots \wedge dx^{j_p} \right)$ , for the same reason above, and we have the factor $(-1)^k$ because we moved the interior derivative $k$ times (it is like moving an entry of a $(k+p)$-form $k$ times),
\item 0
\end{enumerate}
Then, $i_z(\omega \wedge \eta)$ is the sum of these non-zero terms.
\item Using the fact that a differential form gives 0 if two or more entries are repeated:
$$i_z \left( i_z(\omega) \right) (v_1, \ldots, v_{k-2}) = i_z \omega (z, v_1, \ldots, v_{k-2}) = \omega (z, z, v_1, \ldots, v_{k-2}) = 0$$ \qedhere
\end{itemize}
\end{proof}

\begin{remarkbox}\begin{remark}
  In the above proposition there is a little abuse of notation when we claimed $i_z(i_z \omega) = 0$, see also the remark \ref{abusenot1}.
\end{remark}\end{remarkbox}

Now, let's talk about \textit{pullbacks} and \textit{pushforwards} for functions and $k$-forms.

\begin{definition}[Pullback]
  Let $f \colon U \rightarrow V$ (with $U, V \subseteq \rfield^n$) be a differentiable map. Let's suppose that $\dim(U) = \dim(V) = n$ (just for the sake of simplicity, since it is not necessary). Then the \textit{pullback} of a $k$-form (from $V$) to $U$ is the map:
  \begin{align*} \label{pullbackdef}
    f^* \colon \Omega^k(V) & \rightarrow \Omega^k(U) \\
    \omega & \mapsto f^*w
  \end{align*}
  such that
  \begin{equation*}
    (f^* \omega)(p) (u_1, \ldots, u_k) = \omega (f(p)) (df(u_1), \ldots, df(u_k)), \forall\, \, p \in \rfield^n, \forall \,\, u_i \in \mathfrak{X}(U)
  \end{equation*}
\end{definition}

Now, we want to give another name to the differential of a function.
\begin{definition}[Pushforward]
  Given $f \colon U \rightarrow V$ as before, we will also call the differential of $f$ at $p \in \rfield^n$, i.e. $df_p = df(p)$, as the \textit{pushforward} of $f$ at $p$, and it will be denoted by the symbol $(f_*)_p$.
\end{definition}

In our mind, we'll think of $df_p = (f_*)_p$, at least until this concept is generalized. Using the pullback definition above, we can write the pushforward map as:
\begin{align*}
  df_p \equiv (f_*)_p \colon U \subset \rfield^n &\rightarrow V \subset \rfield^m \\
    v & \mapsto (f_*)_p (v)
\end{align*}
By definition of differential, $df_p(v) = v(f)$, where $v$ is a vector tangent to $\rfield^n$ at $p$. Since vector are like directional derivatives, $v(f)$ is the directional derivative of $f$ with respect to $v$ (not evaluated at any point, for now!). In particular, if we apply the definition to a point $h(q)$, where $h \in C^{\infty}(\rfield^n, \rfield^m)$ (\textbf{CHECK}) and $q \in \rfield^n$, we have:
$$(f_*)_p (v)(h)(q) = (f_*)_p (v)(h(q)) = v(h(f(q))) = v(h \circ f) (q) = v (f^* h) (q)$$
In the last passage, we used the pullback for a differentiable function, which is completely legal since we defined it for differentiable $k$-forms, and a differentiable function is just a 0-form.

\begin{remarkbox}\begin{remark}
Using the pushforward, we can define the pullback of a differential form using a different notation (i.e. using $f_*$ instead of $df$):
  \begin{equation}
    (f^* \omega)(p) (u_1, \ldots, u_k) = \omega (f(p)) (f_*(u_1), \ldots, f_*(u_k)), \forall\, \, p \in \rfield^n, \forall \,\, u_i \in \mathfrak{X}(U)
  \end{equation}
\end{remark}\end{remarkbox}

Now, some properties of the pullback.

\begin{proposition} \label{pullbackprop}
  $g, f \in C^1(\rfield^n, \rfield)$, $\omega, \varphi \in \Omega^k(\rfield^n)$, $h\colon \rfield^n \rightarrow \rfield$. Then:
  \begin{enumerate}
    \item $f^*(\omega + \varphi) = f^*(\omega) + f^*(\varphi)$
    \item $f^*(h \omega) = f^*(h)f^*(\omega)$
    \item $(f \circ g)^* = g^*(f^*(\omega))$
    \item If $\varphi^1, \ldots, \varphi^k \in \Omega^1(\rfield^n)$, then $f^*(\varphi^1 \wedge \ldots \wedge \varphi^k) = f^*(\varphi^1) \wedge \ldots \wedge f^*(\varphi^k)$
    \item $df^*(\omega) = f^*(d\omega)$
  \end{enumerate}
  From property (4) also follows that $f^*(\omega \wedge \phi) = (f^*\omega) \wedge (f^* \phi)$
\end{proposition}
\begin{proof}[Sketch of the proof]
$ $
\begin{enumerate}
\item \begin{align*}
f^*(\omega + \varphi) &= (\omega + \varphi)(f(p)) (f_*(u_1), \ldots, f_*(u_k)) =\\
& = \omega(f(p)) (f_*(u_1), \ldots, f_*(u_k)) + \varphi(f(p)) (f_*(u_1), \ldots, f_*(u_k)) = \\
&=  f^*(\omega) + f^*(\varphi)
\end{align*} 
\item $f^*(h \omega) (z_1, \ldots, z_k)(p) = h(f(p))\, \omega (f_* z_1, \ldots, f_* z_k) (f(p)) = f^*(h)f^*(\omega)$
\item We will use: $(f \circ g)_* v(h) = v \left(f \circ g)^* h \right) = v(g^* \circ f^*(h)) = g_*v(f^*h) = f_* \tilde v (h) = f_* \circ g_* v$, where $\tilde v \equiv g_* v$. Then:
\begin{align*}
(f \circ g)^* \omega(z_1, \ldots, z_k) &= \omega ((f \circ g)_* z_1, \ldots, (f \circ g)_* z_k)) = \omega(f_* \circ g_* z_1, \ldots, f_* \circ g_* z_k) = \\ &= g^*(f^* \omega)(z_1, \ldots, z_k)
\end{align*}
\item \begin{align*}
f^*(\varphi^1 \wedge \varphi^k)(z_1, \ldots, z_k) &= (\varphi^1 \wedge \ldots \wedge \varphi^k)(f_* z_1, \ldots, f_*z_k) = \det\left( \varphi^i (f_* z_j)\right) = \\
&= \det \left( f^* \varphi^i(z_j) \right) = (f^* \varphi^1)\wedge \ldots \wedge (f^* \varphi^k) \, (z_1, \ldots, z_k)
\end{align*}
\item If $z$ is a vector field and $dx^i$ is an element of the dual basis (in particular, $dx^i(e_j) = \frac{\partial}{\partial x^j} x^i$), we have
$$f^*dx^i(z) = dx^i(f_* z) = z(f^*x^i) = df^i(z)$$
where in the last inequality we used the def. \ref{differential} of differential.
Now, 
\begin{align*}
df^*(\omega) &= df^* \left(\sum\limits_{i_1 < \ldots i_k} a_{i_1 \cdots i_k} dx^{i_1} \wedge \ldots \wedge dx^{i_k} \right) = \\
&= d \, \sum\limits_{i_1 < \ldots i_k} f^* (a_{i_1 \cdots i_k}) f^*(dx^{i_1}) \wedge \ldots \wedge f^*(dx^{i_k}) \overset{(1)}{=} \\
&=\sum\limits_{i_1 < \ldots i_k} df^*(a_{i_1 \cdots i_k}) df^{i_1} \wedge \ldots \wedge df^{i_k} \overset{(2)}{=} \\
&= f^* d \omega
\end{align*}
Where we used: (1) $f^*dx^i = df^i$ (i-th coordinate), the product rule and the fact that $d^2 = 0$, (2) given $h \equiv a_{i_1 \cdots i_k} \in C^1(\rfield^n)$, then $df^* h(z) = z(f^*h) = dh(f_* z) = f_*z(h) = f^*dh(z)$.
\end{enumerate}
\end{proof}

\begin{remarkbox}\begin{remark}
We can express the pullback of a differential form in the following way:
\begin{align*}
(f^* \omega)(p) &= \sum\limits_{1 \le i_1 < i_2 < \cdots < i_k \le n} (f^*a_{i_1, \ldots i_k} (p)) f^*dy^{i_1} \wedge f^* dy^{i_2} \wedge \cdots \wedge f^* dy^{i_k} = \\
& = \sum\limits_{1 \le i_1 < i_2 < \cdots < i_k \le n} a_{i_1, \ldots, i_k} (f(p)) df^{i_1} \wedge df^{i_2} \wedge \cdots \wedge df^{i_k}
\end{align*}
where $f^i = y^i(f)$. We used properties (2) and (4) of proposition \ref{pullbackprop}
\end{remark}\end{remarkbox}

\begin{remarkbox}\begin{remark}
  From our definition of pullback, it is not necessary that $f_*$ is invertible.
\end{remark}\end{remarkbox}

\begin{tcolorbox}
\begin{example}[Example of a pullback]
Let's consider 
$$U = \{ r > 0, \, 0 < \theta \le 2 \pi \}$$ 
$$V = \rfield^2 \setminus \{(0, 0)\}$$
Let: 
\begin{align*}
f \colon U &\rightarrow V \\
(r, \theta) &\mapsto (r\cos \theta, r \sin \theta) \equiv (x, y)
\end{align*}
Let's consider the 1-form $\Omega^1(V) \ni \omega = - \frac{y}{x^2 + y^2}dx + \frac{x}{x^2 + y^2} dy$ on $\rfield^2 \setminus\{(0, 0\}$. Then:
$$f^* \omega = - \frac{r \sin \theta}{r^2}\cdot (\cancel{dr\, \cos \theta} - r \sin \theta \, d\theta) + \frac{r \cos\theta}{r^2} (\cancel{dr\, \sin \theta} + r \cos\theta \, d\theta) = d \theta$$
where we used $f^*dx^i = df^i$.
\end{example}
\end{tcolorbox}

\section[Integration of differential forms]{\crule[red!30!white]{0.3cm}{0.4cm}  Integration of differential forms}

Let $\omega$ be a differential form of degree $n$ in $\rfield^n$. Then $\omega$ is necessarily of the form
\begin{equation}
  \omega = \underbrace{a(p)}_{\text{it's a function}} dx^1 \wedge \ldots \wedge dx^n
\end{equation}
Such a form can be integrated:
\begin{equation}
  \int_{f(D)} \omega = \int_D f^* \omega
\end{equation}

\begin{tcolorbox}
\begin{example}[Example of $\int$ of a $k$-form]
Let's consider 
$$D = [0,1] \times [0,1]$$
$$f(D) = \{(x, y) \in \rfield^2 \, | \, x \in [0, 1], 0 \le y \le x\}$$
i.e. $f(D)$ is the lower-right triangle of the square $D$. In particular, we can write $f$ as $f(x^1, x^2) =(x^1, x^1 x^2)$. Let $\omega = dy^1 \wedge dy^2$ on $\rfield^2$, thus:
\begin{align*}
\int_{f(D)} dy^1 \wedge dy^2 &= \int_D f^*(dy^1 \wedge dy^2) = \int_D df^1 \wedge df^2 = 
\int_D dx^1 \wedge(x^2 dx^1 + x^1 dx^2) = \\
&= \int_D x^1 dx^1 \wedge dx^2 = \int_D x^1 dx^1 dx^2 = \frac{1}{2}
\end{align*}

\end{example}
\end{tcolorbox}

\section[More on Vector Fields]{\crule[gray!50!white]{0.3cm}{0.4cm}  More on Vector Fields}
\begin{definition}[Tangent space]
  $U \subset \rfield^n, U$ is an open set. $p \in U$, then the set of all derivations of $C^\infty(U)$ (cf. def \ref{derivation}) is called tangent space to $U$ at $p$ and is denoted by $T_p U$. An element of $T_p U$ is called a tangent vector at $p$, and it is often denoted by $v_p$.
\end{definition}


\begin{definition}[Tangent bundle]
The tangent bundle over an open subset $U \subset \mathbb{R}^n$ is defined as
\begin{equation}
	TU \equiv \underset{p \in U}{\sqcup} T_p U
\end{equation}
where $T_p U$ is the tangent space of $U$ at $p$. Every element of the disjoint union is represented by an ordered pair $(p, v)$ where $p \in U, v \in T_p U$.
So, elements of the tangent bundle are couples that consist of a tangent vector at a point of $U$, and the point itself.
The tangent bundle comes equipped with the projection map
\begin{align}
  \pr \colon TU &\rightarrow U \\
  (p, v) &\mapsto p \nonumber
\end{align}
which sends each vector in $T_pU$ to the point $p$ at which it is tangent. 
\end{definition}

\begin{remarkbox}\begin{remark}
  In the previous definition, the "$\sqcup$" symbol denotes a disjoint union. "Disjoint" here means that, if we consider the disjoint union of two elements $x$ and $y$ such that $x=y$, the union is the set $\{x, y\}$ and not $\{x\} = \{y\}$ as in normal unions. The mathematical operator doesn't know if two elements are equal. Since we are not mathematical operators, we can enumerate the elements like: $\{(1, x), (2, y)\} = \{(1, x), (2, x)\} = \{(1, y), (2, y)\}$ in order to distinguish them.
\end{remark}\end{remarkbox}

\begin{definition}[Alternative definition of vector field]
  A smooth vector field $v$ on $U \subset \rfield^n, U$ open, is a smooth map
  \begin{equation}
    v \colon U \rightarrow TU
  \end{equation}
  such that $\pr(v_p) = p, \forall \, p \in U$ %add command for pr
\end{definition}

\begin{remarkbox}\begin{remark} [Space of sections]
  The set of all vector fields $\mathfrak{X}(U) \equiv \{ C^\infty(U, TU) \, | \\ \, \pr(v_p) = p \}$ is also called the space of sections in $TU$.
\end{remark}\end{remarkbox}

\begin{definition} [Cotangent bundle]
  We define
  \begin{equation}
    T^*U \equiv \underset{p \in U}{\sqcup} T_p^* U
  \end{equation}
  as the cotangent bundle. Where $T^*_p$, the dual space of the tangent space, is called cotangent space. We also associate a projection $\pr \colon T^*U \rightarrow U$ with it.
\end{definition}

Now, let's talk about about Lie algebras.

\begin{definition}[Abstract Lie algebra]
  A Lie algebra $(V, [\cdot, \, \cdot])$ is a vector space $V$ over $\rfield$ endowed with a map
  $$[\cdot , \, \cdot] \colon V \times V \rightarrow V$$
  with the following properties:
  \begin{itemize}
    \item $[\cdot , \, \cdot]$ is bilinear
    \item $[\cdot , \, \cdot]$ is antisymmetric ($[u, v] = -[v, u], \forall \, u, v \in V$)
    \item $[\cdot , \, \cdot]$ satisfies the \textit{Jacobi identity}:
    $$[[u, v], z] + [[z, u], v] + [[v, z], u] = 0$$
  \end{itemize}
\end{definition}

\begin{remarkbox}\begin{remark}[Jacobi]
  How to remember Jacobi identity: remember $[[u, v], z]$ and then  permute cyclically.
\end{remark}\end{remarkbox}

\begin{proposition}
  $\mathfrak{X}(\rfield^n)$ is an (infinite dimensional) Lie algebra with $[u, v](f) = u(v(f)) - v(u(f))$, for $u, v \in \mathfrak{X}(\rfield^n), f \in C^\infty(\rfield^n)$. (Note that $u$ and $v$ are vector fields and $[u, v]$ is still a vector field).
\end{proposition}

\begin{definition}[Integral curve]
  An integral curve for a vector field $v$ is a smooth curve $\phi \colon (a, b) \rightarrow \rfield^n$ satisfying $\dot\phi(t) = v_{\phi(t)}$ ($v_{\phi(t)}$ is the vector tangent at $\phi(t)$ for $t$ fixed, remember the previous notation!). Let us suppose $0 \in (a, b)$. Then, $\phi(0)$ is called the starting point of $\phi$.
\end{definition}

We can also visualize the family of integral curves in the following way.

\begin{definition}[Flow]
  The map
  \begin{align}
    \theta \colon \rfield \times \rfield^n &\rightarrow \rfield^n \\
    (t, p) &\mapsto \theta_t(p) \nonumber
  \end{align}
  such that $\dot\theta_t(p) = v_{\theta_t(p)}$ is the flow of the vector field $v$ where, if we fix $p$, $\theta_t(p)$ is the integral curve which passes through $p$ at $t=0 \in (a, b)$.
  So the flow satisfies two conditions:
  \begin{align}
    \dot\theta_t(p) &= v_{\theta_t(p)} , \, &\forall \, p \in \rfield^n \\
    \theta_0(p) &= p, \, &\forall \, p \in \rfield^n
  \end{align}
Under the right hypothesis (e.g. Lipschitz hypothesis and smoothness of $v$) we can prove existence and uniqueness of the solution of such ODEs ($\forall\, p \in \rfield^n$).
By fixing either the time or the starting point of the flow, we can consider two maps:
\begin{itemize}
\item $p \mapsto \theta_t(p)$, for each fixed $t$ (we are observing several integral curves at the same time $t$)
\item $t \mapsto \theta_t(p)$, for each fixed $p$ (we are observing the integral curve starting from $p$, for all times)
\end{itemize}
\begin{figure}[h]
     \centering
     \includegraphics[width=.6\linewidth]{images/integralcurve_1.pdf}
     \caption{the map $t \mapsto \theta_t(p)$ selects just one integral curve} \label{Fig:integralcurve_1}
\end{figure}
\end{definition}

\begin{definition}[Lie derivative of a $k$-form]
  Let $z \in \mathfrak{X}(\rfield^n)$ be a differentiable vector field, $\phi_t$ its flow and $\omega \in \Omega^k(\rfield^n)$. Then the Lie derivative of $\omega$ is defined as
  \begin{equation}
    L_z \omega = \restrict{\frac{d}{dt}(\phi_t^* \omega)}{t=0}
  \end{equation}
\end{definition}

\begin{remarkbox}\begin{remark}
  We denoted the flow by the symbol $\phi_t$ and not $\phi$. What we are doing here is not caring about $p$: $\phi^*_t \omega (\cdot) = \omega(\phi_t(\cdot))$.
  %In components we have: ...to do
  Useful formula (Cartan's formula): $L_z \omega = (d i_z + i_z d) \omega$
\end{remark}\end{remarkbox}
\section[Lie derivative of a vector field]{\crule[blue!50!white]{0.3cm}{0.4cm}  Lie derivative of a vector field}
\begin{definition}[pullback of a vector field]
  %(move all definitions of pullback together)
  Let $\varphi$ be a diffeomorphism of $\rfield^n$ (i.e. a differentiable and invertible map from $\rfield^n$ to $\rfield^n$, such that its inverse is differentiable as well). Let $v \in \mathfrak{X}(\rfield^n)$. Then:
  \begin{equation}
    \varphi ^* v \equiv \varphi_* ^{-1}v
  \end{equation}
  is the pullback of $v$ with $\varphi$.
  In particular, given a flow $\phi$ and $t$ fixed, $\phi(t, \cdot) = \phi_t(\cdot)$ is a diffeomorphism on $\rfield^n$ with $\phi^{-1} = \phi(-t, \cdot) = \phi_{-t}(\cdot)$.
\end{definition}

\begin{definition}[Lie derivative of a vector field]
Let $u, v \in \mathfrak{X}(\rfield^n)$. The Lie derivative of $v$ in direction $u$ is
\begin{equation}
  L_u v \equiv \restrict{\frac{d}{dt}( \phi_t^* v)}{t=0}
\end{equation}
(Remember: $\phi\colon (t, p) \mapsto \phi(t, p), \phi_t^*v \colon (t, p) \mapsto v(\phi (t, p))$).
\end{definition}

\begin{lemma} \label{lemmadiff1}
  Let $u, v$ be smooth vector fields on $\rfield^n$ and $\varphi \in \Diff(\rfield^n)$. Let $\phi_t$ be the flow of $u$ and let $\psi_s$ be the flow of $v$. Then
  \begin{itemize}
    \item $\varphi^*v = \restrict{\frac{d}{ds}}{s=0} \varphi^{-1} \circ \psi_s \circ \varphi$
    \item $\varphi^*v = v \Leftrightarrow \varphi \circ \psi_s = \psi_s \circ \varphi$ for all $s$.
    \item $L_u v = 0 \Leftrightarrow \phi_t \circ \psi_s = \psi_s \circ \phi_t$ for all $s, t$
  \end{itemize}
\end{lemma}
\begin{proof}[Sketch of the proof]
$ $
\begin{itemize}
\item We have
$$\restrict{\frac{d}{ds}}{s=0} \,  \varphi^{-1} \circ \psi_s \circ \varphi (p) \overset{(1)}{=} \varphi^{-1}_* v_{\varphi(p)} = (\varphi_*^{-1} v)_p \overset{(2)}{=} (\varphi^* v)_p$$
where we used: (1) the definition of flow for the flow $\varphi^{-1}$ of the vector field $u$ and the fact that $\psi_s$ is the flow of $v$. And we applied the definitions to the point $\varphi(p)$ instead of $p$ as usual. (2) $\varphi_*^{-1} = \varphi^*$ by definition.
\item $$\frac{d}{ds} (\varphi \circ \psi_s)(p) = \varphi_* v_{\psi_s(p)} = (\varphi_* v)_(\varphi \circ \psi_s(p))$$
and so, in a similar way:
$$\frac{d}{ds}(\psi_s \circ \varphi) = v_{\psi_s \circ \varphi(p)} = \frac{d}{ds} (\varphi \circ \psi_s) + O(s)$$
where in the last step we Taylor-expanded with respect to $s$, using the fact that for $s=0$ we have $\psi_0 = \mathbb{1}$, so the thesis is verified. Then we can proceed in the same way for each order of the expansion.
\item $(\Leftarrow)$: follows from the previous point, and using the definition of Lie derivative for a vector field. $(\Rightarrow)$: 
$$\frac{d}{dt} \phi_t^* v = \restrict{\frac{d}{d\varepsilon}}{\varepsilon=0} \phi^*_{t + \varepsilon} v \overset{(1)}{=} \phi_t^* \restrict{\frac{d}{d \varepsilon}}{\varepsilon=0} \phi^*_{\varepsilon} v = \phi_t^* L_u v = 0, \forall \, t$$
where in (1) we used that $\phi^*_{t + \varepsilon} = \phi_t^* \circ \phi^*_{\varepsilon}$.
Thus $\phi_t^* v = v \forall \, t$, and by the previous point we have the thesis.
\end{itemize}
\end{proof}

\begin{lemma}
  Let $u, v$ be smooth vector fields on $\rfield^n$ and and let $\phi_t$ (respectively $\psi_s$) be the flow of $u$ (respectively $v$). Then:
  \begin{itemize}
    \item $L_u v = \restrict{\frac{\partial^2}{\partial s \partial t} \phi_{-t} \circ \psi_s \circ \phi_t}{t=0,s=0}$
    \item $(L_u v) (f) = [u, v](f) = u(v(f)) - v(u(f))$ for all smooth functions $f$ on $\rfield^n$
  \end{itemize}
\end{lemma}
\begin{proof}[Sketch of the proof]
$ $
\begin{itemize}
\item $\frac{d}{dt}\phi_t^* v = \frac{\partial}{\partial t} \frac{\partial}{\partial s} \phi_{-t} \circ \psi_s \circ \phi_t$ by the first point of the lemma \ref{lemmadiff1}, where we put $\varphi^* = \phi_t$.
\item See problem sheet 4, exercise 4.
\end{itemize}
\end{proof}

\begin{lemma} \label{commlemma}
  Let $u, v$ be smooth vector fields on $\rfield^n$ and $\varphi \in \Diff(\rfield^n)$. Then:
  \begin{enumerate}
    \item $[u, v]$ is $\rfield-$bilinear (i.e. bilinear for a parameter $\lambda \in \rfield$)
    \item $[u, v] = - [v, u]$
    \item The Jacobi identity holds
    \item $[u, fv] = f[u, v] + u(f)v$
    \item $\varphi_*[u, v] = [\varphi_* v, \varphi_* u]$
  \end{enumerate}
\end{lemma}
\begin{proof}
See problem sheet 4, exercise 4.
\end{proof}

  \section[Stokes' Theorem on $\rfield^n$]{\crule[black!50!white]{0.3cm}{0.4cm}  Stokes' Theorem on $\rfield^n$}

\begin{itemize}
  \item For a function $f$ (i.e. a 0-form) on $[a, b] \subset \rfield$ we have $\int_a^b df = \int_a^b \partial_x f dx = f(b) - f(a)$ (fundamental theorem of calculus).
  \item for $\omega = a_i dx^i$, a 1-form on $U = [0, 1] \times [0, 1] \subset \rfield^2$ we have:
  $$\int_S d\omega = \int_S (\partial_{x^1} a_2) dx^1 \wedge dx^2 + (\partial_{x^2} a_1) dx^2 \wedge dx^1 = \int_{\partial S} \omega$$
  where we used the fundamental theorem of calculus.
  \item More generally, if $S$ is a compact subset of $\rfield^2$ with piecewise regular boundary $\partial S$ (piecewise homeomorphic to intervals in $\rfield$) then we obtain by decomposing $S$ in terms of little squares and interpreting $\int_U d\omega$ as a Riemann sum over the square the result
  $$\int_U d\omega = \int_{\partial U} \omega$$
  This result generalizes immediately to compact co-dimension zero subsets of $\rfield^n$ with piecewise regulary boundary
  \item If $M$ is a compact subset of dimension $m \le n$ in $\rfield^n$ (with piecewise regular boundary $\partial M$), diffeomorphic to a compact subset of $U \subset \rfield^m$ (i.e. $M = f(U)$) and $\omega \in \Omega^{m-1}(M)$, then:
  $$\int_M d\omega = \int_U f^* d\omega = \int_U df^* \omega = \int_{\partial U} f^* \omega = \int_{\partial M} \omega$$
  \item More generally, the parametrization of $\partial M$ may be different from that induced by $M$. Then we have:
  $$\int_M d\omega = \int_{\partial M} i^* \omega$$
  where $i \colon \partial M \rightarrow M$ is the inclusion map of $\partial M$ into $M$.
\end{itemize}

So, the most general result that we achieved is the following:

\begin{theorem}[Stokes]
  $\omega \in \Omega^{m-1}(\rfield^n)$. Let $M$ be a compact subset of $\rfield^n$, $dim(M) = m \le n$, such that $M$ is homeomorphic to a closed subset $U \subset \rfield^m$. $\partial M$ is the boundary of $M$ and $i \colon \partial M \rightarrow M$ is the inclusion map of $\partial M$ into $M$. Then:
  \begin{equation}
    \int_{\partial M} i^* \omega = \int_M d\omega
  \end{equation}
\end{theorem}
\begin{proof}[Sketch of the proof]
First, we notice that $U$ is compact as well because it is homeomorphic to a compact set.
Then, the proof uses the naturality of the pullback "$f^*d = df^*$" and the results for open subsets of $\rfield^n$. [\textbf{TO DO}]
\end{proof}

\begin{corollary}[Fundamental theorem of line integrals]
  Let $f$ be a smooth function defined near an oriented curve $C$ in $\rfield^n$, with endpoints $A$ and $B$.
  Then:
  \begin{equation}
    \int \nabla f \cdot dx = f(B)-f(A) % TODO: add parametrization
  \end{equation}
\end{corollary}
\begin{proof}
	$\int df = \int \nabla f \cdot dx$ because $\nabla f = \frac{df}{dt} dt = \frac{\partial f}{\partial x^i}$ and $dx^i = \frac{dx^i}{dt}dt$. Moreover, thanks to the Stokes' theorem, we have $\int df  = f(B) - f(A)$.
\end{proof}

\begin{corollary}[Curl theorem or Classical Stokes theorem]
  Let $v$ be a differentiable vector field defined near a surface $S \subset \rfield^3$ with boundary $\partial S$.
  \begin{equation}
    \int_S n \cdot (\nabla \times v) \, dS = \int_{\partial S} v \cdot dx
  \end{equation}
  where $n$ is the normal vector on the surface at each point.
\end{corollary}
\begin{proof}
Let $\alpha$ be a 1-form in $\rfield^3$. Since the euclidean metric gives an isomorphism between $\rfield^3$ and its dual space (see also eq. \eqref{dualisomorph}), we can always write $\alpha$ as $\alpha = g(v, \cdot)$, for a certain $v \in \rfield^3$, where $g$ is the Euclidean metric, i.e. $g(x, y)$ is the standard dot product between $x$ and $y$. So, $\alpha = g(v, \cdot) = \delta_{ij} v^j dx^i = v_i dx^i$. Then: $d \alpha = (\partial_{x^j} v_i) dx^j \wedge dx^i \overset{(1)}{=} \partial_{x^j} v_i \varepsilon^{ji}_k n^k dS$. Where in (1) we used that $dx^j \wedge dx^i$ is the oriented surface generated by $dx^j$ and $dx^i$ (see also the second point of the example \ref{orarea}). Then we used that the oriented surface area can be expressed using the Levi-Civita tensor $\varepsilon$, the normal vector $n$ and the unoriented area element $dS$. Hence, using that $\nabla \times v = \partial_{x^j} v_i \varepsilon^{ji}_k$:
$$\int_S n \cdot (\nabla \times v) \, dS = \int_S d\alpha \overset{(2)}{=} \int_{\partial S} \alpha = \int_{\partial S} v_i dx^i = \int_{\partial S} v \cdot dx$$
Using the Stokes' theorem in (2).
\end{proof}

\begin{corollary}[Divergence theorem]
  For a smooth vector field $v$ defined on a solid $T \subset \rfield^3$ with boundary $\partial T$:
  \begin{equation}
    \int_T \nabla \cdot v \, dV = \int_{\partial T} v \cdot n \, dS
  \end{equation}
  where $dV$ is the unoriented volume element.
\end{corollary}
\begin{proof}
See problem sheet 5, exercise 3.
\end{proof}

\section[Poincar Theorem of 1-forms]{\crule[brown!50!white]{0.3cm}{0.4cm}  Poincar Theorem of 1-forms}
\begin{definition} [Closed and exact forms]
  If $\omega \in \Omega^{k}(U)$ such that $d\omega = 0$, then $\omega$ is closed. If there exists $\alpha \in \Omega ^{k-1}(V), V \subset U$ such that $\omega = d\alpha$ in $V$ then $\omega$ is exact.
\end{definition}

\begin{proposition}
  The following are equivalent (\textbf{note that we are considering just 1-forms!}):
  \begin{enumerate}
    \item $\omega \in \Omega^1(U)$ is exact in a connected open subset $V \subset U$
    \item For any curve $\gamma \colon (a, b) \rightarrow U, \int_{\gamma} \omega$ depends only on the endpoints $\gamma(a)$ and $\gamma(b)$.
    \item $\int_{\gamma} \omega = 0$ for any closed curve $\gamma$ in $V$
  \end{enumerate}
\end{proposition}
\begin{proof}[Sketch of the proof]
First, $(1) \Rightarrow (2)$ because of Stokes' theorem. Moreover, $(2) \Rightarrow (3)$ using Stokes' theorem again. Now, $(3) \Rightarrow (2):$ the curve made of the union of $\gamma_1$ and $\gamma_2$ is a closed curve. Then, if $\gamma$ is the name of such a closed curve:
$$0 = \int_{\gamma} \omega = \int_{\gamma_1} \omega - \int_{\gamma_2} \omega$$
where the minus sign comes from the orientation of the curves. 
Then: $\int_{\gamma_1} \omega = \int_{\gamma_2} \omega$ where $\gamma_1$ and $\gamma_2$ are any two curves with the same endpoints. It means that we can consider the integral of $\omega$ on any curve and the result  wouldn't change, \textbf{if} the new curve has the same endpoints of the initial curve.
\begin{figure}[h] \label{Fig:closedcurve}
     \centering
     \includegraphics[width=0.3\textwidth]{Images/closedcurve.pdf} 
     \caption{Union of $\gamma_1$ and $\gamma_2$}      
\end{figure}
$(2) \Rightarrow (1)$: Let's consider the 1-form $\omega = a_i dx^i$ (when $\omega$ has this form, we call it a \textit{standard form}). Let's fix a point $p \in V$ and define a function 
\begin{align*}
f \colon V &\rightarrow \rfield \\
x &\mapsto \int_{\gamma} \omega
\end{align*}
where $x = x(p')$ is the coordinate given by a point $p'$ (i.e. $x$ gives a parametrization) and $\gamma$ is a curve joining $p$ to $p'$. By (2), $f$ is well defined (i.e. the definition does not depend on the $\gamma$ chosen, so we don't need to specify it). Furthermore, $df = \frac{\partial f}{\partial x^i} dx^i$. Since $\omega = a_i dx^i$, we only need  to prove that $\frac{\partial f}{\partial x^i} = a_i$. Let's consider the curve $\gamma$. We extend it from $p'$ with a straight segment $\Delta \gamma$. In particular, $\Delta \gamma = x + t e_i, t \in (-\varepsilon, \varepsilon)$ with $e_i$ any vector of the canonical basis of $\rfield^n$. We choose $\varepsilon$ small such that $\gamma + \Delta \gamma \subset V$. Then:
\begin{align*}
\restrict{\frac{\partial f}{\partial x^i}}{x} &= \lim\limits_{t \to 0} \frac{1}{t} \left[ f(x+t e_i) - f(x) \right] = \lim\limits_{t \to 0} \frac{1}{t} \left[  \int_{\gamma + \Delta \gamma} \omega - \int_{\gamma} \omega \right] = \\
&= \lim\limits_{t \to 0} \frac{1}{t} \int_{\Delta \gamma} \omega = \lim\limits_{t \to 0} \frac{1}{t} \int_0^t a_i(x(t)) dt = a_i(x)
\end{align*}
where we used $\int \omega = \int a_i \frac{dx^i}{dt} dt$ and the fact that every point belonging to $\Delta \gamma$ can be written as $x + t e_i$, so we have $\frac{dx^i}{dt} = 1$ on $\Delta \gamma$. Moreover, in the last equality we Taylor-expanded $a_i(x(t))$ with respect to $t$. Only the first-order term $a_i(x)$ matters because we are considering the limit as $t \to 0$. Then we have the thesis for standard forms. Then, the result holds for any form (in general we could have $a_i = 0$ for some $i$).
\begin{figure}[h] \label{Fig:pcurve}
     \centering
     \includegraphics[width=0.24\textwidth]{Images/pp'.pdf} 
     \caption{A possible curve $\gamma$ from $p$ to $p'$}      
\end{figure}
\end{proof}

\begin{remarkbox}\begin{remark}[A closed form is not always exact]
  If $\omega$ is exact, then it is closed (because $d^2=0$). But not every closed form in $\Omega^1(U), U$ open subset of $\rfield^n$ is exact. Cf. $\omega = - \frac{y}{x^2 + y^2} dx + \frac{x}{x^2+y^2}dy$ in $\rfield^2$ minus the non-negative $x$-axis. If $\gamma$ is a closed curve around the origin of $\rfield^2$, we have:
  $$\int_{\gamma} \omega = \int_{\gamma} d\theta = 2\pi$$
  and therefore $\omega$ cannot be exact by the previous proposition. However, we notice that we have problems only with the origin of $\rfield^2$. If we consider a subset of $\rfield^2$ which is far enough from the origin, the form would be an exact form in such subset. Indeed, we say that $\omega$ is locally exact, and the general result follows from the next theorem.
\end{remark}\end{remarkbox}

\begin{theorem}[Poincar theorem for 1-forms on $\rfield^n$]
  Let $\omega \in \Omega^1(U), U \subset \rfield^n, U$ open. Then $d\omega = 0$ if and only if for each $p \in U$ there is a neighbourhood $V \subset U$ of $p$ and a differentiable function $f \colon V \rightarrow \rfield$ such that $\omega = df$.
\end{theorem}

\begin{remarkbox}\begin{remark}
  Using the Poincar theorem for 1-forms, we can extend the definition of the integral of a closed 1-form along a \textbf{continuous} path (until now, we have always assumed the our paths were piecewise differentiable). In fact, assume that $ \omega \in \Omega^1(U), d \omega = 0$, and $\gamma$ such that:
  $$\gamma \colon [0, 1] \rightarrow U$$ is a differentiable map.
  Now, we choose a partition of $[0, 1]$, i.e. a collection of points $0 = t_0 < t_1 < \ldots < t_k < t_{k+1} = 1$ such that the restriction of $\gamma$ to the interval $(t_i, t_{i+1})$ is contained in a ball $B_i$ where $\omega$ is exact. In particular:
  $$\omega = df_i, \text{ for } f_i \colon B_i \rightarrow \rfield$$
  Then:
  $$\int_{\gamma} \omega = \sum_i \left[ f_i(t_{i+1}) - f_i(t_i) \right ]$$
  If $\gamma$ is only continuous, we could still consider such a partition, and still define
  $$\int_{\gamma} \omega = \sum_i \left[ f_i(t_{i+1}) - f_i(t_i) \right ]$$
  The integral of $\gamma$ is well defined because the definition is independent from the choice of our partition: if $P$ is one partition and $P'$ is a refinement of $P$ (i.e. it is the same partition plus an extra point $t' \in (t_i, t_{i+1})$ for some $i$), then:
  $$[f_i(t_{i+1}) - \cancel{f_i(t')}] + [\cancel{f_i(t')} - f_i(t_i)] = [f_i(t_{i+1}) - f_i(t_i)]$$
  Then the integral does not change if we consider a refinement. If we consider a general partition $P'$, we can add every point of the partition $P$ to $P'$, so that we get a refinement of $P'$ that we will be called $P''$. The integral on the partition $P'$ has the same value of the integral on the partition $P''$ by the above argument. Now, we can add every point of the partition $P'$ to $P$, so to get the partition $P''$ again, but now we can see $P''$ as a refinement of $P$. Then the integral on $P$ and on $P''$ are the same. Then also the integrals on $P$ and $P'$ are the same.
\end{remark}\end{remarkbox}

Now, we want to extend the above theorem to $k$-forms.

\begin{definition} [Contractible set]
  An open subset $U \subset \rfield^n$ is contractible to some point $p_0 \in U$ if there exists a differentiable map
  \begin{align}
    H \colon U \times [0, 1] &\rightarrow U \\
    (p,t) &\mapsto H(p, t) \nonumber
  \end{align}
  such that $H(p, 1) = p, H(p, 0) = p_0, \forall\, p \in U$
\end{definition}

\begin{remarkbox}\begin{remark}
  If $U$ is contractible, we can associate a $k$-form $\bar\omega \in \Omega^k(U \times \rfield)$ to every $\omega \in \Omega^k(U)$. $\bar \omega$ is defined as
  \begin{equation}
    \bar\omega = H^* \omega
  \end{equation}
  On the other hand, any $\bar \omega \in \Omega^k(U \times \rfield)$ has a unique decomposition of the form
  \begin{equation}
    \bar \omega = \omega_1 + dt \wedge \eta
  \end{equation}
  with $i_{\partial_t} \omega_1 = 0$ and $i_{\partial_t} \eta = 0$.
  Conversely, we can associate a $k$-form $\omega \in \Omega^k(U)$ to each $\bar \omega \in \Omega^k(U \times \rfield)$ with the help of the inclusion map
  \begin{align}
    i_t \colon U &\rightarrow U \times \rfield \\
    p &\mapsto i_t(p) = (p,t)\nonumber
  \end{align}
  Then, $i_t^* \bar \omega \in \Omega^k(U)$ if $\bar \omega \in \Omega^k(U \times \rfield)$

  Furthermore, let's define the map
  \begin{align}
    I \colon \Omega^k(U \times \rfield) &\rightarrow \Omega^{k-1}(U) \\
    \eta &\mapsto I\eta \nonumber
  \end{align}
  such that $$(I\eta)(z_1, \ldots, z_{k-1}) = \int_0^1 \eta(p, t)(\partial_t, i_{t^*}z_1, \ldots, i_{t^*}z_{k-1})dt$$
\end{remark}\end{remarkbox}

\begin{proof}
  Let's choose coordinates $\{x^1, \ldots, x^n, t\}$ for $U \times \rfield$. Then we write $\bar \omega$ on the basis:
  $$\bar \omega = \sum\limits_{i_1 < \ldots < i_k} a_{i_1 \cdots i_k} dx^{i_1} \wedge \ldots \wedge dx^{i_k} + \sum_{i_1 < \ldots < i_{k-1}} b_{i_1 \ldots i_{k-1}} dt \wedge dx^{i_1} \wedge \ldots \wedge dx^{i_{k-1}}$$
  (We can always do this, the coefficients could also be trivial).
  Now, we want to integrate this form. Let:
  \begin{align*}
    i_t \colon U &\rightarrow U \times \rfield \\
    p &\mapsto (p, t)
  \end{align*}
  $i_t$ is the inclusion map (it "includes" $U$ into $U \times \rfield$ at $t$)...TO DO
\end{proof}

\begin{lemma}
  \begin{equation}
    i_1^* \bar \omega - i_0^* \bar \omega = d(I \bar \omega) + I(d\bar \omega)
  \end{equation}
  Indeed, since $H \circ i_1 = Id$ and $H \circ i_0 = p_0$, $\forall \, p \in U$ we have
  $$\omega = (H \circ i_1)^* \omega = i_1^* \bar \omega$$
  and
  $$0 = (H \circ i_0)^* \omega = i_0^* \bar \omega$$
\end{lemma}

Then we can extend Poincar lemma to $k$-forms:
\begin{theorem} [Poincar Lemma] \label{poinclemma}
  Let $k \ge 1$. Let $U$ be a contractible, open subset of $\rfield^n$ and $\omega \in \Omega^k(U)$ with $d\omega = 0$. Then there exists a $(k-1)$-form $\alpha \in \Omega^{k-1} (U)$ such that $\omega = d\alpha$.
\end{theorem}

Question: For $\omega \in \Omega^1(U)$, when is $\int_{\gamma}\omega$ independent of the choice of $\gamma$?

\begin{definition}[Homotopy between curves]
  Two continuous curves $\gamma_1$ and $\gamma_2$, $\gamma_i \colon [a, b] \rightarrow U, i=1,2, U \subset \rfield^n$ are freely homotopic if there exists a continuous map $H$
  \begin{align*}
    H \colon [a, b] &\times [0, 1] \rightarrow U &\text{ such that:}\\
    H(s, 0) &= \gamma_1(s), &\forall\, s \in [a, b] \\
    H(s, 1) &= \gamma_2(s), &\forall\, s \in [a, b] \\
  \end{align*}
\end{definition}

\begin{definition}[Homotopy between closed curves with same endpoints]
  Two continuous curves $\gamma_1$ and $\gamma_2$, $\gamma_i \colon [a, b] \rightarrow U, i=1,2, U \subset \rfield^n$, with $\gamma_1(a) = \gamma_2(a)$ and $\gamma_1(b) = \gamma_2(b)$ are homotopic relatively to $\{\gamma_1(a), \gamma_2(b)\}$ if there exists a continuous map $H$
  \begin{align*}
    H \colon [a, b] &\times [0, 1] \rightarrow U &\text{ such that:}\\
    H(s, 0) &= \gamma_1(s), &\forall\, s \in [a, b] \\
    H(s, 1) &= \gamma_2(s), &\forall\, s \in [a, b] \\
    H(a, t) &= \gamma_1(a) = \gamma_2(a), &\forall\, t \in [0, 1] \\
    H(b, t) &= \gamma_1(b) = \gamma_2(b), &\forall\, t \in [0, 1]
  \end{align*}
\end{definition}

\begin{theorem}
  Let $\omega \in \Omega^1(U)$, with $d \omega = 0$ (closed), and $\gamma_1, \gamma_2$ be two homotopic curves (as in the previous definition). then:
  \begin{equation}
    \int_{\gamma_1} \omega = \int_{\gamma_2} \omega
  \end{equation}
\end{theorem}

What if $\gamma_1(a) \not = \gamma_2(a), \gamma_1(b) \not = \gamma_2(b)$
\begin{definition}[Homotopy between closed curves]
$\gamma_1, \gamma_2 \colon [a, b] \rightarrow U$, $\gamma_i$ closed curves, are freely homotopic if there exists a continuous map
\begin{align*}
  H \colon [a, b] \times [0, 1] &\rightarrow U &\text{ such that:} \\
  H(s, 0) &= \gamma_1(s) &\forall\, s \in [a, b] \\
  H(s, 1) &= \gamma_2(s) &\forall\, s \in [a, b] \\
  H(a, t) &= H(b, t) &\forall\, t \in [0, 1]
\end{align*}
\end{definition}

\begin{proposition}
  If $\omega$ is a closed 1-form on $U$, $\gamma_1$ and $\gamma_2$ two closed curves, freely homotopic in $U$, then:
  \begin{equation}
    \int_{\gamma_1} \omega = \int_{\gamma_2} \omega
  \end{equation}
  In particular, if $\gamma_1$ is freely homotopic to a point, then $\int_{\gamma_1} \omega = 0$
\end{proposition}

\begin{definition}[Simply connected set]
  %\marginpar{If $U$ is arc-wise connected we also say that it is simply connected if its fundamental group is trivial}
  A connected open set $U \subset \rfield^n$ is simply connected if every continuous closed curve in $U$ is freely homotopic to a point in $U$.
\end{definition}

\begin{tcolorbox}\begin{example}
  $\rfield^n$, the unitary ball in $\rfield^n$ and its homeomorphic images are simply connected
\end{example}\end{tcolorbox}

\begin{remarkbox}\begin{remark}[Contractible vs. simply connected]
"Contractible $\Longrightarrow$ simply connected" (why?), but "Simply connected $\cancel{\Longrightarrow}$ contractible" (cf. $S^2$).
\end{remark}\end{remarkbox}

\begin{remarkbox}\begin{remark}
  Every closed form on a simply connected subset $U$ of $\rfield^n$ is exact.
\end{remark}\end{remarkbox}

\begin{lemma}
  A connected open subset $U$ of $\rfield^n$ is simply connected if every closed curve in $U$ is homotopic to a point in $U$.
\end{lemma}

We can limit ourselves to consider continuous curves, thanks to the two following results:
\begin{theorem}[Whitney approximation on $\rfield^n$]
  If $\gamma$ is a continuous map between $U, V \subsetneq \rfield^n$, then $\gamma$ is homotopic to a smooth map $\tilde \gamma$. If $\gamma$ is smooth on a closed subset $A$ of $U$, then the homotopy can be taken relatively to $A$.
\end{theorem}

\begin{theorem}
  If $\gamma_1$ and $\gamma_2$ are homotopic maps between $U$ and $V$ then they are smoothly homotopic. %TO CHECK
\end{theorem}
\section[de Rham Cohomology]{\crule[green!50!white]{0.3cm}{0.4cm}  de Rham Cohomology}
Let $U$ be an open subset of $\rfield^n$. We can think of $\Omega^k(U)$ as a vector space over $\rfield$. Indeed, if $\alpha, \beta \in \Omega^k(U)$, then $a \alpha + b \beta \in \Omega^k(U), \forall\, a, b, \in \rfield$. And also the remaining properties of a vector space are satisfied.

\begin{remarkbox}\begin{remark}
  We say that $\Omega^k(U, \mathbb{Z}) = \{k\text{-forms on } U \text{ with coefficients in } \mathbb{Z}\}$  forms a group (and not a vector space) since $\mathbb{Z}$ is not a field. In contrast, $\Omega^k(U, \rfield)$ is a vector space.
\end{remark}\end{remarkbox}

\begin{definition}
  Let $U \subset \rfield^n$, $U$ open, $\dim(U) = m \le n$. Then:
  \begin{itemize}
    \item The set of closed $k$-forms is the $k$-th \textit{cocycle group} $Z^k(U, \rfield)$ (it is a group with respect to addition).
    \item The set of exact $k$-forms is the $k$-th \textit{coboundary group} $B^k(u, \rfield)$
    \item The $k$-th \textit{de Rham cohomology group} $H^k(U, \rfield)$ is defined as: $$H^k(U, \rfield) = \faktor{Z^k(U, \rfield)}{B^k(U, \rfield)}$$
  \end{itemize}
\end{definition}

\begin{remarkbox}\begin{remark}
  $H^k(U, \rfield)$ contains the closed $k$-forms defined on $U$ which are not exact. See also the example \ref{quotientexample}.
\end{remark}\end{remarkbox}

\begin{tcolorbox}\begin{example}
  Let's consider the following examples:
  \begin{itemize}
    \item If $U$ is contractible, then $H^k(U, \rfield) = \{0\}$ by Poincar lemma.
    \item $U=\rfield^2 \setminus \{0\}$, $H^0(\rfield^2 \setminus \{0\}, \rfield) = \rfield$ (constant functions)
    \item $H^1(\rfield^2 \setminus \{0\}, \rfield) = \rfield$
    \item $H^2(\rfield^2 \setminus \{0\}, \rfield) = \{0\}$
    \item Torus $T$: $H^0(T) = \rfield$,  $H^1(T) = \rfield \oplus \rfield$, $H^2(T) = \rfield$
  \end{itemize}
\end{example}\end{tcolorbox}

\begin{definition}
  $M \subset \rfield^n, \Omega^*(M, \rfield) = \oplus_{k=0}^n \Omega^k(M, \rfield)$
\end{definition}

\begin{remarkbox}\begin{remark}
  $\wedge \colon \Omega^* \times \Omega^* \rightarrow \Omega^*$ endows $\Omega^*$ with the structure of a ring.
\end{remark}\end{remarkbox}

\begin{definition}[de Rham Complex]
  The de Rham Complex is defined by $\Omega^*(M, \rfield)$ together with the sequence
  $$\overset{d}{\longrightarrow} \Omega^{k-1} \overset{d}{\longrightarrow} \Omega^{k} \overset{d}{\longrightarrow} \Omega^{k+1} \overset{d}{\longrightarrow} \cdots \overset{d}{\longrightarrow} \Omega^{n} \overset{d}{\longrightarrow} 0$$
  with $Im(d_k) \subset Ker(d_{k+1})$ since $d^2 = d \circ d = 0$
\end{definition}

\begin{remarkbox}\begin{remark} \label{drremark}
  $H^k(M) = \faktor{Ker(d_{k+1})}{Im(d_k)}$
\end{remark}\end{remarkbox}

\begin{definition} [exact sequence] \label{exsequence}
  If $H^k(M, \rfield) = 0, k=1, \ldots, n$, then
    $$\overset{d}{\longrightarrow} \Omega^{k-1} \overset{d}{\longrightarrow} \Omega^{k} \overset{d}{\longrightarrow} \Omega^{k+1} \overset{d}{\longrightarrow} \cdots $$
    is an exact sequence if and only if $Ker(d_{k+1}) = Im(d_k)$
\end{definition}

\begin{remarkbox}\begin{remark}
  For $k=0$, $\Omega^0 = \{\text{functions}\}$, then $H^0 = 0$
\end{remark}\end{remarkbox}

\begin{definition}[Cohomology ring]
  The cohomology ring is defined as
  $$H^*(M, \rfield) = \oplus_{k=0}^n H^k(M, \rfield)$$
\end{definition}

\begin{tcolorbox}\begin{example}
  $H^*(T^2, \rfield) = \rfield \oplus \rfield$, where $T^2$ is the torus.
\end{example}\end{tcolorbox}

\begin{remarkbox}\begin{remark}
  If $\phi \colon U \rightarrow V$, $U, V$ subsets of $\rfield^n$, $\phi$ diffeomorphism, then $\phi^* \colon \Omega^k(V) \rightarrow \Omega^k(U)$ is the pullback of $\phi$ on $k$ forms. By the construction above, we can also define such pullback on $H^k$, as $\phi^* \colon H^k(V) \rightarrow H^k(U)$, which maps closed-but-not-exact forms to closed-but-not-exact forms.
  If $\phi$ is a diffeomorphism, then $\phi^*$ is an isomorphism. Thus, $H^k(V) \cong H^k(U)$.
  Question: what if $U$ and $V$ are not diffeomorphic, but only homotopic equivalent? We want to show that the final result is the same.
 \end{remark}\end{remarkbox}

\begin{definition}[Homotopy between maps]
  Two maps $\phi, \psi \colon U \rightarrow V$ are homotopic if there exists a continuous map $H$
  \begin{align*}
    H \colon [0,1] &\times U \rightarrow V &\text{ such that:}\\
    H(0, \cdot) &\colon U \rightarrow V &\text{ with } H(0, \cdot) = \phi(\cdot) \\
    H(1, \cdot) &\colon U \rightarrow V &\text{ with } H(1, \cdot) = \psi(\cdot)
  \end{align*}
\end{definition}

 \begin{definition}[Homotopy equivalence between sets]
   Two subsets $U$ and $V$ of $\rfield^n$ are homotopic equivalent if there exist continuous maps $f \colon U \rightarrow V$ and $g \colon V \rightarrow U$ such that the compositions $g \circ f$ and $f \circ g$ are homotopic to the identity in $U$ and $V$, respectively. $f$ and $g$ are called homotopy equivalences.
 \end{definition}

 \begin{tcolorbox}\begin{example}
   Let's consider the following examples:
   \begin{itemize}
     \item Any homeomorphism $\phi \colon U \rightarrow V$ with homotopy inverse (i.e. inverse up to a homotopy) $\phi^{-1}$ is a homotopy equivalence, but the converse is not always true (a disk is homotopy equivalent to a point, but it's not homeomorphic to a point)
     \item A circle is homotopy equivalent to $\rfield^2 \setminus \{0\}$
     \item $S^{n-1}$ is homotopy equivalent to $\rfield^n \setminus \{0\}$
     \item A solid torus is homotopy equivalent to a tea cup
   \end{itemize}
 \end{example}\end{tcolorbox}

 \begin{lemma}
   Let $\phi, id \colon V \rightarrow V$ be two smoothly homotopic maps. Then $\restrict{\phi^*}{H^*(V, \rfield)} = \restrict{id^*}{H^*(V, \rfield)}$
 \end{lemma}

 \begin{theorem}
   Let $\phi \colon U \rightarrow V$ be a homotopy equivalence between $U$ and $V$ with homotopy inverse $\psi$. Then $\phi^*$ is an isomorphism between $H^k(U)$ and $H^k(V)$
 \end{theorem}

\begin{theorem}
  Let $\phi \colon U \rightarrow V$ be a homotopy equivalence between $U$ and $V$ with homotopy inverse $\psi \colon V \rightarrow U$. then $\phi^*$ induces an isomorphism $\hat \phi^*$ such that:
  \begin{equation}
    H^n(V, \rfield) \cong H^n(U, \rfield), \forall\, n
  \end{equation}
\end{theorem}

\begin{definition}[(co)chain map] %add def of (co)chain complex
  Let $A^*$ and $B^*$ be two (co)chain complexes (a (co)chain complex is a generalization of a de Rham complex, e.g. think about $A^*=\Omega^*$).
  A (co)chain map $\hat \phi^* \colon A^* \rightarrow B^*$ is a collection of maps $\phi^* \colon A^n \rightarrow B^n$ s.t. $d \circ \phi^* = \phi^* \circ d \colon A^n \rightarrow B^{n+1}$. We often denote $\hat \phi^*$ by $\phi^*$.
\end{definition}

\begin{tcolorbox}\begin{example}
  Let's consider:
  \begin{align*}
    A^* &= \Omega^*(U, \rfield) = \Omega^0(U, \rfield) \oplus \Omega^1(U, \rfield) \oplus \ldots \\
    B^* &= \Omega^*(V, \rfield) = \Omega^0(V, \rfield) \oplus \Omega^1(V, \rfield) \oplus \ldots
  \end{align*}
  And we consider $\hat \phi^*\colon A^* \rightarrow B^*$, $\phi^* \colon \Omega^0(U, \rfield) \rightarrow \Omega^0(V, \rfield)$, $\phi^* \colon \Omega^1(U, \rfield) \rightarrow \Omega^1(V, \rfield)$, etc.
\end{example}\end{tcolorbox}

\begin{definition}
  A short exact sequence (\textit{SES}) is a collection of (co)chain complexes $A^*, B^*, C^*$ and (co)chain maps $\phi^* \colon A^n \rightarrow B^n, \psi^* \colon B^n \rightarrow C^n$ such that for each $n$:
  \begin{equation}
    0 \longrightarrow A^n \overset{\phi^*}{\longrightarrow} B^n \overset{\psi^*}{\longrightarrow} C^n \longrightarrow 0
  \end{equation}
  is exact
\end{definition}

\begin{remarkbox}\begin{remark}
  Remember the definition of exact sequence: it gives a condition on the kernel and the range of the maps (cf. definition \ref{exsequence}).
  By this condition, we have that $\phi^*$ must be an injective map, and $\psi^*$ must be a surjective map in the above definition.
\end{remark}\end{remarkbox}

\begin{tcolorbox}\begin{example}\label{seqexample}
  Let's consider the following example:\\
  \begin{tikzcd}
\Omega^0(U) \arrow[r, "d"] \arrow[d, "\phi^*"'] & \Omega^1(U) \arrow[r, "d"] \arrow[d, "\phi^*"'] & \Omega^2(U) \arrow[r] \arrow[d, "\phi^*"'] & \ldots & \} \textit{ sequence} \\
\Omega^0(V) \arrow[r, "d"] \arrow[d, "\psi^*"'] & \Omega^1(V) \arrow[r, "d"] \arrow[d, "\psi^*"'] & \Omega^2(V) \arrow[r] \arrow[d, "\psi^*"'] & \ldots & \} \textit{ sequence} \\
\Omega^0(W) \arrow[r, "d"]                      & \Omega^1(W) \arrow[r, "d"]                      & \Omega^2(W) \arrow[r]                      & \ldots & \} \textit{ sequence} \\
\underbrace{}_{\textit{sequence}}               & \underbrace{}_{\textit{sequence}}               & \underbrace{}_{\textit{sequence}}          &        &
\end{tikzcd}
\end{example}\end{tcolorbox}

\begin{lemma}[zig-zag lemma]
  $\phi^*, \psi^*$ as in the previous definitions. Then there exists a linear map $\delta$ such that:
  \begin{align*}
    \ldots \longrightarrow &H^{n-1}(C^*) \overset{\delta}{\longrightarrow} H^n(A^*) \overset{\phi^*}{\longrightarrow} H^n(B^*) \overset{\psi^*}{\longrightarrow} \\
    \overset{\psi^*}{\longrightarrow} &H^n(C^*) \overset{\delta}{\longrightarrow} H^{n+1}(A^*) \overset{\phi^*}{\longrightarrow} \ldots
  \end{align*}
  is an exact sequence.
\end{lemma}

\begin{tcolorbox}\begin{example}
  See also example \ref{seqexample}.\\
  \begin{tikzcd}
H^0(U) \arrow[r, "d"] \arrow[d, "\phi^*"']                                               & H^1(U) \arrow[r, "d"] \arrow[d, "\phi^*"']                                               & H^2(U) \arrow[r] \arrow[d, "\phi^*"']                                               & \ldots \\
H^0(V) \arrow[r, "d"] \arrow[d, "\psi^*"']                                               & H^1(V) \arrow[r, "d"] \arrow[d, "\psi^*"']                                               & H^2(V) \arrow[r] \arrow[d, "\psi^*"']                                               & \ldots \\
H^0(W) \arrow[r, "d"] \arrow[ruu, "\color{gray}{\delta}" description, dotted, bend left] & H^1(W) \arrow[r, "d"] \arrow[ruu, "\color{gray}{\delta}" description, dotted, bend left] & H^2(W) \arrow[r] \arrow[ruu, "\color{gray}{\delta}" description, dotted, bend left] & \ldots
\end{tikzcd}
\end{example}\end{tcolorbox}

\begin{theorem}[Mayer-Vietoris]
  Let $M \subset \rfield^n$ such that $M=f(U) \cup g(V), U, V$ open subsets in $\rfield^m, m \le n$. $f, g$ homeomorphisms.
  Let:
  $$i \colon U \cap V \rightarrow U \text{ (inclusion)}$$
  $$j \colon U \cap V \rightarrow V \text{ (inclusion)}$$
  Let:
  \begin{align*}
    (f^* \oplus g^*) \colon \Omega^k(M) &\rightarrow \Omega^k(U) \oplus \Omega^k(V) \\
    \omega &\mapsto (f^*(\omega), g^*(\omega))
  \end{align*}
  \begin{align*}
    (i^* - j^*) \colon \Omega^k(U) \oplus \Omega^k(V) &\rightarrow \Omega(U \cap V)\\
    (\omega, \eta) &\mapsto i^* \omega - j^* \eta
  \end{align*}
  Then, for each $k$ there exists a linear map $\delta$ such that:
  \begin{align*}
    \ldots \longrightarrow &H^k(M) \overset{f^* \oplus g^*}{\longrightarrow} H^k(U) \oplus H^k(V) \overset{i^* - j^*}{\longrightarrow} H^k(U \cap V) \overset{\delta}{\longrightarrow} H^{k+1}(M) \longrightarrow \ldots
  \end{align*}
  is exact.
\end{theorem}
\section[Submanifolds of $\rfield^n$]{\crule[blue!30!white]{0.3cm}{0.4cm}  Submanifolds of $\rfield^n$}
\textbf{Premise:} We want to go outside $\rfield^n$, and analyze more general topologies. For now, we can think about them as generalizations of curves and surface of $\rfield^n$, a proper definition will come later. Note that what we studied until now can often be extended to manifolds: with sufficient conditions, manifolds can be "embedded" in $\rfield^n$.
We will give four equivalent definitions of a submanifold $M$ of dimensions $m$ in $\rfield^n$ ($m \le n$).
\begin{definition}[(a) Submanifold - Local parametrization]
  $\forall \, p \in M \subset \rfield^n, \exists$ a neighbourhood $V$, $p \in V$, $V \subset \rfield^n$ and $U \subset \rfield^m, U, V$ open, $m \le n$, and a smooth map $\phi \colon U \rightarrow \rfield^n$ such that:
  \begin{itemize}
    \item $\phi \colon U \rightarrow M \cap V$ is a homeomorphism,
    \item $\phi_* (x) \colon \rfield^m \rightarrow \rfield^n$ is injective $\forall \, x \in U$
  \end{itemize}
\end{definition}

\begin{definition}[(b) Submanifold - Locally flat]
  $\forall p \in M, \exists$ a neighbourhood $V \subset \rfield^n, p \in V$ and a neighbourhood $W \subset \rfield^n$, $0 \in W$, and a diffeomorphism $\Phi, \Phi \colon W \rightarrow V$ such that
  $$\Phi \left ( W \cap (\rfield^m \times \{0\}^{n-m}) \right ) = V \cap M$$
\end{definition}

\begin{definition}[(c) Submanifold - Locally  a zero set]
$\forall \, p \in M, \exists$ a neighbourhood $V \subset \rfield^n, V$ open, and a smooth map $F \colon V \rightarrow \rfield^{n-m}$ such that
$$V \cap M = \{x \in V \, | \, F(x) = 0\}$$
and $F_* \colon V \rightarrow \rfield^{n-m}$ is surjective.
\end{definition}

\begin{definition}[(d) Submanifold - Locally a graph]
  $\forall \, p \in M, \exists$ a neighbourhood $V \subset \rfield^n$ and a permutation $\sigma \colon \{1, \ldots, n\} \rightarrow \{1, \ldots, n\}$ and $U \subset \rfield^m, U$ open, together with a smooth map $g \colon U \rightarrow \rfield^{n-m}$ such that
  \begin{align*}
  V \cap M = \{ &(x_{\sigma(1)} \cdots x_{\sigma(n)}) \, |\, (x_1, \ldots, x_m) \in U \\
   &\text{ and } (x_{m+1}, \ldots, x_n) = g(x_1, \ldots, x_m)\}
\end{align*}
  (g is called a \textit{graph}).
\end{definition}

\begin{theorem}
  The four definitions above are equivalent.
\end{theorem}

\begin{corollary} \label{corsubmanifold}
  Let:
  $$\phi \colon \rfield^m \supset U \rightarrow V \cap M$$
  $$\phi' \colon \rfield^m \supset U' \rightarrow V' \cap M$$
  be local parametrizations. then
  $$\phi^{-1} \circ \phi' \colon (\phi')^{-1} (V \cap V' \cap M) \rightarrow \phi^{-1}(V \cap V' \cap M)$$
  is a diffeomorphism. %write that they are open sets? intersection color...check
\end{corollary}
\begin{center}
\begin{tikzpicture}

    % Functions i
    \path[->] (-1, -2.9) edge [bend left] node[left, xshift=-2mm] {$\phi$} (0.8, 0);
    \draw[white,fill=white] (0.06,-0.57) circle (.15cm);


    % Functions j
    \path[->] (6.2, -2.8) edge [bend right] node[right, xshift=2mm] {$\phi'$} (4.2, 0);
    \draw[white, fill=white] (4.54,-0.12) circle (.15cm);

    % Manifold
    \draw[smooth cycle, tension=0.4, fill=white, pattern color=brown, pattern=north west lines, opacity=0.7] plot coordinates{(2,2) (-0.5,0) (3,-2) (5,1)} node at (3,2.3) {$M$};

    % Help lines
    %\draw[help lines] (-3,-6) grid (8,6);

    % Subsets
    \draw[smooth cycle]
        plot coordinates {(1,0) (1.5, 1.2) (2.5,1.3) (2.6, 0.4)}
        node [label={[label distance=-0.3cm, xshift=-2cm, fill=white]:$V \cap M$}] {};
    \draw[smooth cycle]
        plot coordinates {(4, 0) (3.7, 0.8) (3.0, 1.2) (2.5, 1.2) (2.2, 0.8) (2.3, 0.5) (2.6, 0.3) (3.5, 0.0)}
        node [label={[label distance=-0.8cm, xshift=.75cm, yshift=1cm, fill=white]:$V' \cap M$}] {};

    % First Axis
    \draw[thick, ->] (-3,-5) -- (0, -5) node [label=above:$U$] {};
    \draw[thick, ->] (-3,-5) -- (-3, -2) node [label=right:$\mathbb{R}^m$] {};

    % Arrow from i to j
    \draw[->] (0, -3.85) -- node[midway, above]{$\phi^{-1} \circ \phi'$} (4.5, -3.85);

    % Second Axis
    \draw[thick, ->] (5, -5) -- (8, -5) node [label=above:$U'$] {};
    \draw[thick, ->] (5, -5) -- (5, -2) node [label=right:$\mathbb{R}^m$] {};

    % Sets in R^m
    \draw[white, ] (-0.67, -3.06) -- +(180:0.8) arc (180:270:0.8);
    \fill[even odd rule, white] [smooth cycle] plot coordinates{(-2, -4.5) (-2, -3.2) (-0.8, -3.2) (-0.8, -4.5)} (-0.67, -3.06) -- +(180:0.8) arc (180:270:0.8);
    \draw[smooth cycle] plot coordinates{(-2, -4.5) (-2, -3.2) (-0.8, -3.2) (-0.8, -4.5)};
    \draw (-1.45, -3.06) arc (180:270:0.8);

    \draw[white] (5.7, -3.06) -- +(-90:0.8) arc (-90:0:0.8);
    \fill[even odd rule, white] [smooth cycle] plot coordinates{(7, -4.5) (7, -3.2) (5.8, -3.2) (5.8, -4.5)} (5.7, -3.06) -- +(-90:0.8) arc (-90:0:0.8);
    \draw[smooth cycle] plot coordinates{(7, -4.5) (7, -3.2) (5.8, -3.2) (5.8, -4.5)};
    \draw (5.69, -3.85) arc (-90:0:0.8);

\end{tikzpicture}
\end{center}
\begin{tcolorbox}\begin{example}[Submanifolds of $\rfield^n$]
  Let's consider some examples.
  \begin{enumerate}
    \item An open subset of dimension $n$ in $\rfield^n$ is a submanifold (by definition (a), we just take $\phi = id$). For instance $B^n\subset \rfield^n$ is a submanifold of $\rfield^n$.
    \item $S^{n-1} = \partial B^n$ is a submanifold of $\rfield^n$. Indeed, by definition (c), it is the zero set of the function:
    $$F(\underline{x}) = (x^1)^2 + (x^2)^2 + \ldots + (x^n)^2 - 1 = \underline{x}^2 -1, \underline{x} \in \rfield^n$$
    and $F_*$ is surjective: $F_*(\underline{x}) = 2 \underline{x}$
    \item $O(n) = \{A \, | \, AA^t = E\} \subset Mat(n \times n, \rfield) \cong \rfield^{n^2}$, with $E$ identity matrix. $O(n)$ is a submanifold of dimension $\frac{n(n-1)}{2}$. Indeed, using definition (c), it is the zero set of the function:
    \begin{align*}
      F \colon Mat(n \times n, \rfield) &\rightarrow Symm(n, \rfield) \\
      A &\mapsto AA^t - E
    \end{align*}
    What is more, $F_*$ is surjective. In order to prove that, we prove that "$\restrict{F_*}{A}(X)=0, \forall\, A \Rightarrow X = 0$" (then we know that for a linear map $L$ on a vector space $V$, $\dim(V) = \dim Ker(L) + \dim Ran(L)$, so the dimension of the range of $\restrict{F_*}{A}$ must be $\dim Mat(n\times n, \rfield)$, so the map is surjective).
    In fact:
    \begin{align*}
    \restrict{F_*}{A} &= \restrict{\frac{d}{dt}}{t=0}F(A + tX) = \restrict{\frac{d}{dt}}{t=0} \left[ AA^t + tAX^t + tXA^t + t^2XX^t \right] = \\
    & = AX^t + XA^t
  \end{align*}
    And the solution of:
    $AX^t + XA^t = S \in Symm(n, \rfield)$
    is $X = \frac{SA}{2}$ because $\frac{AA^t}{2}S + S\frac{AA^t}{2} = S$.
    Then:
    $$AX^t + XA^t = 0 \Longrightarrow X=0$$
  \end{enumerate}
\end{example}\end{tcolorbox}

Now, we want to use diffeomorphisms like those in the corollary \ref{corsubmanifold} in order to introduce the concept of manifold.

\begin{definition}[Atlas] \label{atlasdef}
  An (n-dimensional, smooth) atlas $\mathcal{A}$ on a set $M$ is a collection of maps (called charts)
  \begin{align}
    \phi_{\alpha} \colon \rfield^n &\overset{\sim}{\rightarrow} M \\
    U_{\alpha} &\mapsto W_{\alpha} \nonumber
  \end{align}
  such that:
  \begin{itemize}
    \item $\cup_{\alpha \in I} W_{\alpha} = M$
    \item $\forall\, \alpha, \beta \in I$ with $W_{\alpha} \cap W_{\beta} \not = \emptyset$,
    $$\phi^{-1}_{\beta} \circ \phi_{\alpha} \colon \phi^{-1}_{\alpha}(W_{\alpha} \cap W_{\beta}) \rightarrow \phi_{\beta}^{-1}(W_{\alpha} \cap W_{\beta})$$
    is a diffeomorphism.
  \end{itemize}
  where $\alpha \in I, I$ index set, and the $\sim$ above the arrow means that $\phi_{\alpha}$ is bijective.
\end{definition}

\begin{definition} [Equivalence relation on atlases]
  Two atlases $\mathcal{A}$ and $\mathcal{A'}$ are equivalent $\Leftrightarrow$ $\mathcal{A} \cup \mathcal{A'}$ is an atlas ($\Leftrightarrow \phi_{\beta}^{-1} \circ \phi_{\alpha}'$ is a diffeomorphism, $\forall\, \phi_{\alpha}' \in \mathcal{A}', \phi_{\beta} \in \mathcal{A})$.
\end{definition}

\begin{definition}[\textbf{Preliminary} definition of manifold] \label{manifoldprel}
  A manifold is a set $M$ with an equivalence class of atlases.
\end{definition}

\begin{remarkbox}\begin{remark}
  The definition \ref{manifoldprel} above is "\textit{preliminary}" because we have not specified anything about the topology yet (we are still working on Euclidean topology). Moreover, in every equivalence class $\exists!$ maximal atlas (i.e. such that, if combined with an other atlas, it can't get any bigger). We also notice that, if the charts are smooth enough, we can pullback "everything" (e.g. all our vector fields, differential forms, etc. defined for subset of $\rfield^n$). However, in that case, everything is defined \textit{locally}. In order to patch them all together, we need some other result (like the partition of unity, coming soon).
\end{remark}\end{remarkbox}

\begin{tcolorbox}\begin{example}[Projective space]
  $\mathbb{K} \in \{\rfield, \mathbb{C}\}$ ($\mathbb{K}$ is some field).
  $$\mathbb{K}P^n \equiv \faktor{(\mathbb{K}^{n+1} \setminus \{0\})}{\sim}$$
  where
  \begin{align*}&(x_0, \ldots, x_n) \sim (x_0', \ldots, x_n') \Longleftrightarrow \\
     \exists \, \lambda \in \mathbb{K}\setminus \{0\} &\text{ such that } (\lambda x_0, \ldots, \lambda x_n) = (x_0', \ldots, x_n')
  \end{align*}
  For instance, $\mathbb{C}P^1 = S^2$ (every point on a line passing through the origin is identified with the point on such line at distance 1 from the origin). We use the notation $[x_0, \ldots, x_n]$ for one equivalence class.
  Let's consider the atlas $\mathcal{A} = \{\phi_i \colon \mathbb{K}^n \rightarrow \mathbb{K} P^n \}$. Where
  \begin{align*}
    \phi_i \colon \mathbb{K}^n &\rightarrow \mathbb{K}P^n \\
    (x_0, \ldots, x_{i-1}, x_{i+1}, \ldots, x_n) &\mapsto [x_0, \ldots, x_{i-1}, 1, x_{i+1}, \ldots, x_n]
  \end{align*}
  And $\phi_i(\mathbb{K}^n) = \left\{[x_0, \ldots, x_n] \in \mathbb{K}P^n \, | \, \text{ i-th entry is } \not = 0 \right \}$.
  We notice that $\mathcal{A}$ is an atlas because $\phi_i$ satisfies the properties of the definition \ref{atlasdef}:
  \begin{itemize}
    \item $\phi$ is a bijection and its inverse is
    \begin{align*}
      \phi_i^{-1} \colon \phi_i(\mathbb{K}^n) & \rightarrow \mathbb{K^n} \\
      [x_0, \ldots, x_{i-1}, \underbrace{x_i}_{\not = 0}, \ldots, x_n] &\mapsto \left( \frac{x_0}{x_i}, \frac{x_1}{x_i}, \ldots, \frac{x_{i-1}}{x_i}, \frac{x_{i+1}}{x_i}, \ldots, \frac{x_n}{x_i} \right)
    \end{align*}
    \item $\phi$ is also a diffeomorphism, because $\phi_j^{-1} \circ \phi_i$, defined as
    \begin{align*}
      \phi_j^{-1} \circ \phi_i \colon \phi_i^{-1} \left(\phi_i(\mathbb{K}^n) \cap \phi_j(\mathbb{K}^n) \right) &\rightarrow \phi_j^{-1} \left(\phi_i(\mathbb{K}^n) \cap \phi_j(\mathbb{K}^n) \right) \\
      (x_1, \ldots, x_n) &\mapsto \Bigl( \frac{x_1}{x_j}, \ldots, \underbrace{\frac{1}{x_j}}_{i-th}, \ldots, \frac{x_n}{x_j} \Bigr)
    \end{align*}
    is a diffeomorphism (where $x_i \not = 0, x_j \not = 0$) because it is a differentiable map and the inverse is again differentiable.
  \end{itemize}
\end{example}\end{tcolorbox}

\begin{remarkbox}\begin{remark}[Extension of analysis to manifolds]
  The existence of an atlas allows us to define concepts from analysis to manifolds. For instance, we can define continuous and smooth functions on manifolds using the concepts of continuity and smoothness that we use in $\rfield^n$.
\end{remark}\end{remarkbox}

\begin{definition}[Continuous/smooth function on a manifold]
  Let $M$ be a manifold, a function $f \colon M \rightarrow \rfield$ is a continuous [smooth] function if and only if $f \circ \phi_{\alpha} \colon U_{\alpha} \rightarrow \rfield$ is continuous [smooth] function $\forall \, \alpha$. Such a function is well-defined, since the definition is independent on the local parametrization chosen, thanks to the definition of atlas.
\end{definition}

\begin{remarkbox}\begin{remark}[cut-off functions]
  We can construct functions on a manifold in the following way: we consider a function defined on an open set in $M$ which is homeomorphic to an open set in $\rfield^n$. Then we extend it to zero outside such open set, but in order to have a differentiable function we smoothly bring it to zero using the so-called cut-off functions.
  For instance, a smooth function from $\rfield$ to $[0, 1] \subset \rfield$ such that:
  \begin{equation*}
    h(x) = \begin{cases*}
     1, & $x \le 1$ \\
     \text{anything}, &  $1 \le x \le 2$ \\
     0, & $x \ge 2$
  \end{cases*}
  \end{equation*}
  is a cut-off function.
  For example, given
  \begin{equation*}
    a(x) = \begin{cases*}
     0, & $x \le 0$ \\
     e^{-1/x}, & $x \ge 0$
  \end{cases*}
  \end{equation*}
  we can consider
  \begin{equation*}
    h(x) = 1 - \frac{a(x)}{a(x) + a(1-x)}
  \end{equation*}
  Then we use the function
  \begin{equation*}
    h_{\epsilon}(x) \equiv h \left (\frac{x}{\epsilon} \right) =
    \begin{cases*}
      1, & $x \le \epsilon$ \\
      0, & $x \ge 2 \epsilon$
    \end{cases*}
  \end{equation*}
  Now, for $p \in M$, let
  $$\phi \colon \rfield^n \supset U \rightarrow V \subset M$$
  such that $\phi(0) = p$ (where $0 \in U$) and $\epsilon > 0$ such that $B_{3 \epsilon} \supset U $. %subset?check

  \begin{figure}[H]
          \centering
  \begin{tikzpicture}
      % Functions i
      \path[->] (-1, -2.5) edge [bend left] node[left, xshift=-2mm] {$\phi$} (1, 0.5);
      \draw[white,fill=white] (0.06,-0.57) circle (.15cm);

      % Manifold
      \draw[smooth cycle, tension=0.4, fill=white, pattern color=brown, pattern=north west lines, opacity=0.7] plot coordinates{(2,2) (-0.5,0) (3,-2) (5,1)} node at (3,2.3) {$M$};

      % Help lines
      %\draw[help lines] (-3,-6) grid (8,6);

      % Subsets
      %\draw[smooth cycle]
          \draw (3, 0.5) arc (0:359:0.8) node [label=right: $V$]{};
          \filldraw[black] (2.2, 0.5) circle (1pt) node[anchor=west] {$p$};
          %\draw[white, pattern color=blue, pattern=crosshatch dots] (5.7, -3.06) -- +(-90:0.8) arc (-90:0:0.8);
          %plot coordinates {(1,0) (1.5, 1.2) (2.5,1.3) (2.6, 0.4)}
          %node [label={[label distance=-0.3cm, xshift=-2cm, fill=white]:$V \cap M$}] {};

      % Sets in R^m
      \draw (-0.67, -3.06)  arc (0:359:0.8) node [label=right: $B_{3 \epsilon}(0)$] {};
      \draw (-1.07, -3.06)  arc (0:359:0.4) node [label=above: $U$] {};
      \filldraw[black] (-1.47, -3.06) circle (1pt) node[anchor=east] {$0$};

  \end{tikzpicture}
\end{figure}
  Then $g(x) = h_{\epsilon}(|x|)$ defines a smooth function $g \colon \rfield^n \rightarrow \rfield$ with supp$(g) = \overline{\{ x \, | \, g(x) \not =  0\}} \subset U$ and $g \circ \phi^{-1} \colon V \rightarrow \rfield$ extends (by zero) to a function $f \in \mathbb{F}(M)$ with supp$f \subset V$ and $f \equiv 1$ in a neighbourhood of $p$. In particular, this shows that $\mathbb{F}(M)$ is infinite-dimensional.

  \begin{figure}[H]
  \centering
  \begin{subfigure}{.5\textwidth}
    \centering
    \includegraphics[width=.9\linewidth]{Images/cutoff_1.pdf}
    \caption{$a(x)$}
  \end{subfigure}%
  \begin{subfigure}{.5\textwidth}
    \centering
    \includegraphics[width=.9\linewidth]{Images/cutoff_2}
    \caption{$h(x)$}
  \end{subfigure}
  \end{figure}
\end{remark}\end{remarkbox}

\begin{definition}[Vector fields on manifolds]
  Vector fields are defined as before (we just replace $\mathbb{F}(\rfield^n)$ with $\mathbb{F}(M)$):
  $$\restrict{\Der}{p} \mathbb{F}(M) = T_p M \ni v_p \colon \mathbb{F}(M) \rightarrow \rfield$$
  And
  $$\mathfrak{X}(M) = \Der \mathbb{F}(M) = TM \ni v \colon \mathbb{F}(M) \rightarrow \mathbb{F}(M)$$
  $\rfield$-linear and satisfying Leibnitz rule.
\end{definition}

\begin{remarkbox}\begin{remark}[Representation of $v$ in $U_{\alpha}$]
  Everything is done in $U_{\alpha} \subset \rfield^n$ (chart). Representation of $v$ in $U_{\alpha}$:
  $$v_{(\alpha)} = v_{(\alpha)}^i \partial_{x_i}$$
  where
  $$v_{(\alpha)}^i(x) = v^i \circ \phi_{\alpha}(x)$$
\end{remark}\end{remarkbox}

Notice that cut-off functions allow us to extend any smooth function on $V_{\alpha}$ (for some $\alpha \in I$) to all of $M$ through extension by zero outside $V_{\alpha}$.
Suppose we are given a function on $M$. How do we decide if it is continuous (or smooth)?
\begin{proposition}[Partition of unity]
Let $M$ be a compact manifold and let $\{V_{\alpha}\}$ be a covering of $M$. Then there exists a family of differentiable functions $\varphi_1, \ldots, \varphi_m$ such that:
\begin{itemize}
  \item $\sum\limits_{i=1}^m \varphi_i \equiv 1$
  \item $0 \le \varphi_i \le 1$ and supp$(\varphi_i) \subset V_{\alpha}$ for some $\alpha \in I$
\end{itemize}
(without proof)
\end{proposition}

\begin{remarkbox}\begin{remark}
  Definition of compactness for a manifold: coming soon.
\end{remark}\end{remarkbox}

\begin{definition}[Partition of unity]
  The family $\{\varphi_i\}$ defined above is said to be a partition of unity subordinate to the covering $\{V_{\alpha}\}$.
\end{definition}

Now, if $f \colon M \rightarrow \rfield$, we can consider $f_i = \varphi_i f \colon v_{\alpha} \rightarrow \rfield$ for some $\alpha \in I$.

\begin{definition}[Differential forms on M]
  \begin{align}
    \Omega^k(M) \ni \omega \colon \mathfrak{X}(M) \times \cdots \times \mathfrak{X}(M) &\rightarrow \mathbb{F}(M) \\
    (v_1, \ldots, v_k) &\mapsto \omega(v_1, \ldots, v_k) \nonumber
  \end{align}
  linear, skew-symmetric (i.e. alternating) as already defined.
\end{definition}

\begin{remarkbox}\begin{remark}[Representation of $\omega$ in $U_{\alpha}$]
  Representation of $\omega$ in $U_{\alpha}$:
  $$\omega_{(\alpha)} = \sum\limits_{i_1 < \cdots < i_k} (a_{i_1 \cdots i_k})_{(\alpha)} dx^{i_1} \wedge \ldots \wedge dx^{i_k}$$
  where $\{x^i\}$ are the coordinates on $U_{\alpha}$.
  Moreover:
  $$\omega_{(\alpha)} ((v_1)_{\alpha}, \ldots, (v_k)_{\alpha}) = \pm \sum\limits_{i_1 < \cdots < i_k} (a_{i_1\cdots i_k})_{\alpha} (v^{i_1})_{\alpha} \cdots (v^{i_k})_{\alpha}$$
  where the $\pm$ sign depends on the orientation used.
\end{remark}\end{remarkbox}

\begin{definition}[Curve on a manifold]
  A curve $\gamma \colon (a, b) \rightarrow M$ is continuous [differentiable] if $\phi_{\alpha}^{-1} \circ \gamma \colon (a, b) \rightarrow U_{\alpha}$ is continuous [differentiable]
\end{definition}
\section[Integration of differential forms on a compact manifold]{\crule[orange!50!white]{0.3cm}{0.4cm}  Integration of differential forms on a compact manifold}
$\omega \in \Omega^n(M), M \text{ compact manifold }, \dim M=n$. Pick a partition of unity subordinate to a covering $\{V_{\alpha} \}$ (i.e. pick some maps $\varphi_i, i=1, \ldots, n, 0 \le \varphi_i \le 1, \sum_{i=1}^m \varphi_i \equiv 1, \text{ supp}(\varphi_i) \subset V_{\alpha(i)}$ for some $\alpha = \alpha(i)$). Then:
\begin{align}
  \int\limits_M \omega &= \int\limits_M \sum_{i=1}^m \varphi_i \omega = \sum_i \int\limits_M (\varphi_i \omega) = \sum_i \int\limits_{V_{\alpha(i)}} (\varphi_i \omega) = \nonumber \\
  & = \sum_i \int\limits_{U_{\alpha(i)}} \phi^*_{\alpha(i)} (\varphi_i \omega)
\end{align}
where $V_{\alpha} = \phi_{\alpha}(U_{\alpha})$.
Notice that \textbf{the integral is well-defined if $M$ is orientable} (definition of orientable manifolds in the next pages).

\begin{lemma}
  $\int\limits_{M} \omega$ is independent of the choice $(V_{\alpha}, \varphi_i)$
\end{lemma}

\begin{remarkbox}\begin{remark}
  What happens with boundaries? If $p \in \partial M$, there is no neighbourhood of $p$ homeomorphic to an open set $U \subset \rfield^n$.
  Let's define the following set:
  $$H^n = \{ (x^1, \ldots, x^n) \in \rfield^n \, | \, x^1 \ge 0 \}$$
  \textbf{Note} that on \cite{Lee} $H^n$ is defined as
  $$H^n = \{ (x^1, \ldots, x^n) \in \rfield^n \, | \, x^n \ge 0 \}$$
  (everything is the same, but with $x^1$ replaced by $x^n$).
\end{remark}\end{remarkbox}

\begin{definition}[subset topology in $H^n$]
  An open set in $H^n$ is the intersection between $H^n$ and an open set in $\rfield^n$.
\end{definition}

\begin{definition}[Functions on $H^n$]
  A function $f \colon V \rightarrow \rfield, V$ open, $V \subset H^n$ is differentiable if $\exists$ an open set $U, \rfield^n \supset U \supset V$ and a differentiable function
  $\bar f \colon U \rightarrow \rfield$ such that $\restrict{\bar f}{V} = \restrict{f}{V}$.
  Furthermore, $(f_*)_p = (\bar f_*)_p, p \in V$.
\end{definition}

\begin{definition}[\textbf{Preliminary} definition of diff. manifold with boundary]
  An $n$-dimensional differentiable manifold with a regular boundary is a set $M$ with an equivalence class of atlases, as usual, but with the difference that $\rfield^n$ in the definition \ref{atlasdef} is replaced by $H^n$ everywhere. The boundary is "regular" if it is described by a regular curve (no intersection, etc...)
\end{definition}

When is a point on the boundary of a manifold?
\begin{definition} [Point on the boundary] \label{pointbdr}
  A point $p \in M$ is on the boundary of $M$ if for some parametrisation $\phi \colon U \subset H^n \rightarrow M$ around $p$, we have $\phi(0, x^2, \ldots, x^n) = p$ for some $x^2, \ldots, x^n$.
\end{definition}%insert picture here

\begin{lemma}
  The definition \ref{pointbdr} does not depend on the choice of parametrisation.
\end{lemma}

\begin{definition} [Orientable manifold]
  $M$ is orientable if there exists an atlas $\mathcal{A} = \{\phi_{\alpha}, U_{\alpha}\}$ such that for each pair $\alpha, \beta$ with $\phi_{\alpha}(U_{\alpha}) \cap \phi_{\beta}(U_{\beta}) \not = \emptyset$, the differential
  $$
    (\phi_{\beta}^{-1} \circ \phi_{\alpha})_* \colon  U_{\beta} \rightarrow U_{\alpha}
  $$
  has positive determinant. Example of a non-orientable manifold: Mbius strip.
\end{definition}

\begin{proposition}
  \begin{itemize}
    \item The boundary $\partial M$ of a $n$-dimensional differentiable manifold with boundary is a $(n-1)$-dimensional differentiable manifold. %with boundary?check
    \item The orientation on $M$ induces an orientation on $\partial M$
  \end{itemize}
\end{proposition}

\begin{theorem}[Stokes theorem on manifolds]
  $M$ orientable, then:
  \begin{equation}
    \int\limits_M d \omega = \int\limits_{\partial M} i^* \omega
  \end{equation}
\end{theorem}

\section[Abstract Manifolds and Topology]{\crule[red!50!white]{0.3cm}{0.4cm}  Abstract Manifolds and Topology}
\begin{remarkbox}\begin{remark}[Non-Example of a manifold]
The set $$M= (-\infty, 0) \cup (0, \infty) \cup \{a, b\}$$ (endowed with a particular topology that we will see later) is not a manifold. Here, $a, b$ are just two points (not necessary real numbers): just imagine $M$ as a subset of $\rfield^2$, consisting of a real line without the origin plus any two points in $\rfield^2$. Even if we have not defined a topology on $M$, we can feel that it doesn't look right: there are two maps $\varphi_a, \varphi_b$ identifying the subsets $U_a = M \setminus \{b\}$ and $U_b = M \setminus \{a\}$ with $\rfield$ (we are just calling the 0 element in other way). The transition function
$$\varphi_b \circ \varphi_a^{-1} \colon \varphi(U_a \cap U_b) = \rfield \setminus \{0\} \rightarrow \varphi_b(U_b \cap U_a) = \rfield \setminus \{0\}$$
is the identity, and is smooth. \textbf{But}  a smooth function on $M$ must have the same value when evaluated at $a$ and $b$, and this does not look right (why?).
\end{remark}\end{remarkbox}

\begin{definition}[Topology] \label{top}
A topology on a set $M$ is a subset $\mathcal{O} \subset \mathcal{P}(M)$, where $\mathcal{P}(M)$ is the power set of $M$, such that:
\begin{enumerate}
	\item $\emptyset, M \in \mathcal{O}$
	\item $\mathcal{O}$ is closed under arbitrary unions: $U_i \in \mathcal{O}, i \in I$, then $\cup_{i \in I} U_i \in \mathcal{O}$
	\item $\mathcal{O}$ is closed under finite intersections: $U_1, \ldots, U_k \in \mathcal{O}$, then $U_1 \cap \ldots \cap U_k \in \mathcal{O}$.
\end{enumerate}
Sets in $\mathcal{O}$ are called open, $A \subset M$ is closed if $M \setminus A$ is open. A subset $V \subset M$ containing $p$ is a neighbourhood of $p$ if there is an open set $U \subset V$ containing $p$.
\end{definition}

\begin{remarkbox}\begin{remark}
From the definition above we can see that $M$ and $\emptyset$ are always both open and closed sets. Notice that a set in a topological space can be: \begin{itemize}
\item both open and closed
\item neither open nor closed
\item open but not closed
\item closed but not open
\end{itemize}
(more examples later).
\end{remark}\end{remarkbox}

\begin{definition}[Closure, interior]
Let $V \subset M$, let $\mathcal{O}$ be an arbitrary topology. The closure $\bar V $ of $V$ is the smallest closed subset of $M$ containing $V$. The interior $\mathring V$ of $V$ is the largest open subset contained in $V$.
\end{definition} %TODO: existence

\begin{definition}[Continuous function, homeomorphism] \label{topcontinuity}
A map $f \colon X \rightarrow Y$ between topological spaces is continuous if $f^{-1}(U) \in \mathcal{O}_X$ for all $U \in \mathcal{O}_Y$ (i.e. if the pre-image of an open set through $f$ is still an open set). $f$ is a homeomorphism if it is bijective, continuous and $f^{-1}$ is continuous.
\end{definition}

\begin{remarkbox}\begin{remark}[Continuous maps vs. open maps]
Requiring that the pre-image of an open set is open is different from requiring that the image of an open set is open! If $f$ brings open sets to open sets, it is called open map (and it might not be continuous).
\end{remark}\end{remarkbox}

\begin{tcolorbox}\begin{example} \label{topexamples}
Given a set $M$, we can always consider two simple topologies on it:
\begin{enumerate}
	\item $\mathcal{O}_M =  \mathcal{P}(M)$ is a topology, where $\mathcal{P}(M)$ is the power set of $M$ (i.e. the family of all subsets of $M$). Everything is open in $M$, so it is easy to see that any function going from $(M, \mathcal{O}_M)$ to any space $(Y, \mathcal{O}_Y)$ must be continuous.
	\item $\mathcal{O}_M=\{\emptyset, M\}$ is a topology: the open sets are just the two ones required by the definition \ref{top}. In this case, every function going from any space $(Y, \mathcal{O}_Y)$ to $(M, \mathcal{O}_M)$ must be continuous (the pre-image of the empty set is the empty set, and the pre-image of $M$ is $Y$).
\end{enumerate}
What is more, we can consider topologies on metric spaces: if $(M, d)$ is a metric space and $B_{\varepsilon}(x)$ denotes the ball of radius $\varepsilon$ around $x$, then
$$\mathcal{O}_d = \{U \subset M \, | \, \forall\, x \in U \, \exists\, \varepsilon > 0 \colon B_{\varepsilon}(x) \subset U \}$$
defines a topology. In particular, we say that the topology is induced by a metric if the open sets are the open balls of the metric space. If $X, Y$ are metric spaces, the definition of continuity seen in the Analysis courses is: $f \colon (X, \mathcal{O}_X) \rightarrow (Y, \mathcal{O}_Y)$ is continuous if and only if $\forall \, x \in X, \forall \, \varepsilon > 0, \exists \, \delta > 0$ such that $f\left(B_{\delta}(x)\right) \subset B_{\varepsilon}\left(f(x)\right)$. One can show that in metric spaces such a definition is equivalent to definition \ref{topcontinuity}.
\end{example}\end{tcolorbox}

\begin{remarkbox}\begin{remark}
If $\mathcal{O}_1, \mathcal{O}_2$ are topologies on $M$, then the intersection of the two topologies is again a topology. Therefore, given a family of subsets of $M$ one can ask for the smallest topology on $M$ which contains a subset.
\end{remark}\end{remarkbox}

\begin{definition}[Hausdorff]
A topological space $(M, \mathcal{O})$ is Hausdorff if for all $p, q \in M$ there is a pair of open sets $p \in U_p, q \in U_q$ so that $U_p \cap U_q = \emptyset$.
\end{definition}

\begin{remarkbox}\begin{remark}
Points in Hausdorff spaces are closed sets. i.e. if $M$ is Hausdorff and $x \in X$, $\{x\}$ is a closed set. In order to see that, we just need to prove that $M \setminus \{x\}$ is open. Let's suppose $M \setminus \{x\} \not = \emptyset$ (otherwise we are done, since $M = \{x\}$ and $M$ is both closed and open). Then we can find another point $y \in M \setminus \{x\}$. Since $M$ is Hausdorff, we can find two open sets $U_x, U_y$ containing $x$ and $y$ respectively, such that $U_x \cap U_y = \emptyset$. The proof is over if we prove that $M \setminus \{x\} = \cup_{y \in M \setminus \{x\}} U_y$, since it implies that $M \setminus \{x\}$ is open (arbitrary union of open sets is open), and then $\{x\}$ is closed. It is true because:
\begin{itemize}
\item "$M \setminus \{x\} \subseteq  \cup_{y \in M \setminus \{x\}} U_y$": for each point $z$ in $M$ different from $x$, there exists an open set $U_z$ entirely contained in $M \setminus \{x\}$ by Hausdorff assumption. The point is contained in a set which is contained in $\cup_{y \in M \setminus \{x\}} U_y$ so, in particular, the point is in $\cup_{y \in M \setminus \{x\}} U_y$.
\item "$M \setminus \{x\} \supseteq \cup_{y \in M \setminus \{x\}} U_y$": if $z \in \cup_{y \in M \setminus \{x\}} U_y$, then $z \not = x$, otherwise there would be an intersection between an open set $U_{\bar x}$ for a fixed $\bar x$ and every open set containing $x$. This is impossible because by Hausdorff assumption there exists at least one open sets of those which does not intersect $U_{\bar x}$.
\end{itemize}

Moreover:
\begin{itemize}
	\item $(M, \mathcal{P}(M))$ is always Hausdorff (there are "so many" open sets that you can always find two of them containing the right points and so that they do not intersect each other).
	\item If the topology is induced by a metric, then the topological space is Hausdorff (for instance, $\rfield^n$ with the topology induced by Euclidean metric is a Hausdorff topological space).
\end{itemize}
\end{remark}\end{remarkbox}

\begin{definition}[Product topology]
Let $(M_i, \mathcal{O}_i), i \in I$ be topological spaces. Then the product topology on $\prod_{i \in I} M_i$ is the smallest topology such that all projections
$$
\prod_{i \in I} M_i \rightarrow M_j
$$
are continuous maps for all $j \in I$.
\end{definition}

\begin{definition}[Quotient topology]
Let $(M, \mathcal{O}_M)$ be a topological space and $\sim$ an equivalence relation on $M$. Then the quotient topology on $\faktor{M}{\sim}$ is the largest topology so that the function
\begin{align*}
\pi \colon M &\rightarrow \faktor{M}{\sim} \\
x &\mapsto [x]
\end{align*}
is continuous. $\pi$ is also called canonical projection.
\end{definition}

\begin{remarkbox}\begin{remark}[Quotient topology pt. 2]
We can also use another useful characterization of the quotient topology: a set $U \subset \faktor{M}{\sim}$ is open if and only if $\pi^{-1}(U) \in \mathcal{O}_M$. It is easy to prove that this definition is equivalent to the one above. Let's call $(1)$ the first definition and $(2)$ the last one.
\begin{itemize}
\item "$(1) \Rightarrow (2)$": $U$ open in the quotient topology, then $\pi^{-1}(U)$ is open in $M$ by (1), because $\pi$ is a continuous map. If $\pi^{-1}(U)$ is open in $M$, then $U$ is open in $\faktor{M}{\sim}$ because in $(1)$ we have defined the topology on $\faktor{M}{\sim}$ as the \textit{largest} topology such that $\pi$ is continuous!
\item "$(2) \Rightarrow (1)$: $\pi$ is obviously continuous. And the topology we get is the \textit{largest} one such that $\pi$ is continuous because if $\pi^{-1}(U)$ is not open in $M$, then $U$ is not open in the quotient space (read also below).
\end{itemize}
\textbf{Note that}: $f\colon X \rightarrow Y$ is continuous if the preimage of every open set of $Y$ is an open set of $X$. But if $A$ is an open set in $X$, in general it is not true that it exists an open set $U$ in $Y$ such that $A = f^{-1}(U)$. In the $"(1) \Rightarrow (2)"$ above it was true because in $Y$ we had the largest topology such that $f\colon X \rightarrow Y$ was continuous. It means that the topology on $Y$ had the same open sets of the "power set topology" (i.e. everything is open, cf. remark \ref{topexamples}) except for some sets. Which sets did we remove? We were considering the "largest" topology on $Y$ such that "$f\colon X \rightarrow Y$ is continuous". In order to have the continuity assumption hold true, we need: $U$ open in $Y \Rightarrow f^{-1}(U)$ open in $X$. Which is equivalent to: $f^{-1}(U)$ \textbf{not} open in $X \Rightarrow U$ \textbf{not} open in $Y$. So if we take the preimage of any set and we notice that such preimage is not open, than the set itself cannot be open in $Y$: it has to be removed from the topology we are considering on $Y$.
\end{remark}\end{remarkbox}

\begin{tcolorbox}\begin{example}[Cofinite topology]
Let $M$ be a set. We define the following topology: the open sets are $\emptyset$ and the sets $U \not = \emptyset, U \subset M$ such that their complement in $M$ is finite. Another way to define the same topology is using closed sets: the only closed sets in this topology are $M$ and the finite sets. This topology is called cofinite topology.
$M$ with this topology is not Hausdorff if $M$ is an infinite set.
Why? Because \footnote{the proof is easier to follow if you think of $M$ with cofinite topology as $\rfield$ endowed with cofinite topology}: given $x, y$ in $M, x \not = y$, if $U_a$ and $U_b$ are open sets that contain $a$ and $b$ respectively, the complements of $U_a$ and $U_b$ must be finite (so, $U_a$ and $U_b$ must be infinite, because the bigger space $M$ is infinite).
But then $U_a \cap U_b \not = \emptyset$. Indeed, the complement of $U_a$ in $M$ is $M \setminus U_a$ and it must be finite because $U_a$ is open.
If we assume $U_a \cap U_b = \emptyset$, then $U_b$ must be contained in the complement of $U_a$. But then $U_b$ (which is an infinite set, because it is open) would be contained in $M \setminus U_a$ (which is a finite set). Contradiction!
\end{example}\end{tcolorbox}

\begin{lemma}
A subspace of a Hausdorff space is Hausdorff.
\end{lemma}

\begin{tcolorbox}\begin{example}[Subspace topology]
Let $(M, \mathcal{O}_M)$ be a topological space and $N \subset M$ a subset. Then a natural topology for $N$ is the subspace (or subset) topology, defined in the following way. The subspace topology is the smallest topology so that the inclusion map $N \rightarrow M$ is continuous, i.e. $V \subset N$ is open if and only if there is an open set $U \subset M$ such that $U \cap N = V$.

For instance, you may know that $[0, 1)$ is neither open nor closed in $\rfield$ with Euclidean topology (i.e. the topology induced by the standard metric). Actually, it is an open set in $([0, 2], \mathcal{O})$, where $\mathcal{O}$ is the subset topology (with respect to $(\rfield, \mathcal{O}_d)$, where $\mathcal{O}_d$ is the Euclidean topology). Indeed $$[0, 1) = (-1, 1) \cap [0, 2] $$
You should always pay attention to the topology and to where it is defined, for instance $[0, 1]$ is closed (but not open) in $(\rfield, \mathcal{O}_d)$, whereas it is both open and closed in $([0, 1], \mathcal{O})$, for any topology $\mathcal{O}$.
\end{example}\end{tcolorbox}

\begin{definition}[subbasis]
Let $M$ be a set and $B \subset \mathcal{P}(M)$ some collection of subsets. Let $\mathcal{O}_B$ be the smallest topology containing $B$, then we say that $B$ is a subbasis of $\mathcal{O}_B$.
\end{definition}

\begin{definition}[Basis]
Let $(M, \mathcal{O})$ be a topological space and $\mathcal{B} \subset \mathcal{O}$. Then $\mathcal{B}$ is a basis for $\mathcal{O}$ if every open set is a union of sets in $\mathcal{B}$.
\end{definition}

\begin{definition}[Second countability]
Let $(M, \mathcal{O}_M)$ be a topological space. Then $(M, \mathcal{O}_M)$ is second countable if it admits a countable basis.
\end{definition}

\begin{tcolorbox}\begin{example}
Let $\mathcal{B} = \{B_{\varepsilon}(x) \, | \, \varepsilon \in \mathbb{Q}, x \in \mathbb{Q}^n \}$. This is a basis of the metric topology of $\rfield^n$ (and it is a countable set, because we used the density of $\mathbb{Q}$ in $\rfield$).
\end{example}\end{tcolorbox}

\begin{lemma}
If $A \subset M$ and $M$ has a countable basis, then the subspace topology on $A$ has a countable basis.
\end{lemma}

\begin{definition}[Manifold]
A manifold $M$ of dimension $n$ is a topological space which is Hausdorff, second countable and admits an (equivalence class of) atlas(es) $\varphi_i \colon U_i \rightarrow V_i$, where the 
$\varphi_i$ are homeomorphisms, $V_i$ is an open subset of $\rfield^n$ and $U_i \subset M$ is open so that the transition functions
$$\varphi_i \circ \varphi_j^{-1} \colon \varphi_j(U_i \cap U_j) \rightarrow \varphi_i (U_i \cap U_j)$$ are smooth (i.e. $C^{\infty}$). We are considering equivalence classes. we recall that the equivalence relation is the following one: two atlases are equivalent if their union is still a smooth atlas.
\end{definition}

\begin{remarkbox} \begin{remark}[2nd Countable hypothesis for a manifold]
We require the hypothesis of second countability for manifolds in order to apply several theorems or lemmas, e.g. Sard's theorem.
\end{remark} \end{remarkbox}

\begin{fact}
Manifolds are metrizable, i.e. there exists a metric $d$ such that the topology on the manifold is the one induced by $d$.
\end{fact}

\begin{definition}[Cover]
Let $J$ be a set of indices, either finite or infinite. A family of sets $\{U_i\}_{i \in J}$ is a cover for a topological space $X$ if $\cup_i U_i = X$ (the sets "cover" the entire space). It is an open cover if the $U_i$ are open sets in the topopolgy of $X$. A subcover is a subfamily of the initial cover which is still a cover.
\end{definition}

\begin{definition} [Compact space]
A topological space $X$ is compact if every open cover $\{U_i\}_{i \in J}$ admits a finite subcover $\{U_i\}_{i=1}^k$.
\end{definition}

\begin{tcolorbox}
\begin{example}
If $(X, d)$ is a metric space and is compact, then $X$ is bounded (i.e. contained in a ball). Indeed: $U_i = B_i(x) = \{y \in X\, |\, d(x, y) < i\}$ is an open set. And for a fixed $x \in X$, $\cup_i U_i$ is an open cover: $\cup_{i \in \mathbb{N}} U_i = X$. Since the space is compact $\Rightarrow U_{i_1} \cup \ldots \cup U_{i_k} = X$ for some indices $i_1, \ldots, i_k$. These indices are the radii of the balls. Take $c = 2 \max\{i_1, \ldots, i_k\}$, then we have that $d(x, y) < c \, \forall\, y, \in X$, i.e. $X$ is bounded because it is contained in the ball $B_c(x)$.
\end{example}
\end{tcolorbox}

\begin{theorem}[Heine-Borel]
A subset of $\rfield^n$ is compact if and only if it is closed and bounded.
\end{theorem}

\begin{corollary} [Weierstrass]
Let $X$ be a compact space, $f \colon X \rightarrow \rfield$ continuous, then $f$ attains its maximum (and minimum) in $X$.
\end{corollary}

\begin{lemma}[Closed in a compact] \label{closedincomp}
$X$ is a compact topological space, $A \subset X$ closed $\Rightarrow A$ is compact (in subspace topology). 
\end{lemma}
\begin{proof}
Let $\{U_i\}_{i \in I}$ be an open cover of $A$. There are open sets $V_i \subset X$ with $V_i \cap A = U_i$ by definition of open sets in the subspace topology. Moreover, $V_A \equiv X \setminus A$ is open because $A$ is closed. Since $\{U_i\}_{i \in I}$ is an open cover of $A$, we have that $\{U_i\}_{i \in I}$ and $V_A$ form an open cover of $X$. Since $X$ is compact, there is a finite number of indices $i_1, \ldots, i_k \in I$ such that $V_{i_1}, \ldots, V_{i_k}, V_A$ cover $X$. Since $V_A = X \setminus A$, it follows that the sets $V_{i_1}, \ldots V_{i_k}$ cover $A$. Since $U_i = V_i \cap A$, also $U_{i_1}, \ldots, U_{i_k}$ cover $A$. Then $A$ is compact because we proved that every open cover admits a finite subcover.
\end{proof}

\begin{lemma} [Cont. functions map compact sets to compact sets] \label{contcomp}
If $A$ is compact and $f \colon A \rightarrow Y$ is continuous, then $f(A)$ is compact.
\end{lemma}
\begin{proof}
Let $\{U_i\}_{i \in I}$ be an open cover of $f(A)$. Then $\exists\,  \{V_i\}_{i \in I}, V_i \subset Y$ open, so that $U_i = V_i \cap f(A)$ (by definition of open sets in subspace topology). Since $f$ is continuous, $f^{-1}(V) \subset A$ is open, and $f^{-1}(V)$ covers $A$. Since $A$ is compact, there exist $i_1, \ldots, i_k$ such that $f^{-1}(V_{i_1}), \ldots, f^{-1}(V_{i_k})$ still cover $A$. Thus, $V_{i_1},\ldots, V_{i_k}$ is a collection of open sets in $Y$ covering $f(A)$. In the subspace toplogy, $U_{i_1} = V_{i_1} \cap f(A), \ldots, U_{i_k} = V_{i_k} \cap f(A)$ cover $f(A)$.
\end{proof}

\begin{lemma}[Compact set in a Hausdorff] \label{compinhauss}
Let $X$ be Hausdorff. Let $A \subset X$. Assume $A$ is compact. Then $A$ is closed.
\end{lemma}
\begin{proof}
Let $x \in X \setminus A$. For every $a \in A$, there are $U_a \subset X, V_x(a) \subset X$ open sets such that $U_a \cap V_x(a) = \emptyset$ (because of the Hausdorff hypothesis). Here, w used the notation $V_x(a)$ to stress out the dependence of $V$ on $x$ and $a$. The sets $\{U_a \cap A\}_{a \in A}$ are form an open cover of $A$. Since $A$ is compact, there are finitely many $a_1, \ldots a_k$ so that $U_{a_1} \cap A, \ldots, U_{a_k} \cap A$ cover $A$. Now, $V_x = V_x(a_1) \cap \ldots \cap V_x(a_k)$ contains $x$ and it is open. Moreover, $V_x \,\cap A = \emptyset$ because $V_x$ is disjoint from each $U_{a_i} \Rightarrow X \setminus A$ is open, becuase for each point we can find an open set that contains the point and that is contained in $X \setminus A$.
\end{proof}

\begin{theorem} [How to get a homeomorphism]
Let $f \colon X \rightarrow Y$ be a continuous function, $X$ compact, $Y$ Hausdorff and $f$ bijective. Then $f$ is a homeomorphism.
\end{theorem}
\begin{proof}
The only thing which is left to prove is that $f^{-1}$ is a continuous function. First, we notice that an equivalent definition of continuous function is: "$f \colon X \rightarrow Y$ is continuous $\Leftrightarrow f^{-1}(C)$ is closed $\forall \, $ closed set $C \subset Y$". In order to prove that $f^{-1}$ is continuous, we need to show that $f(C) \subset Y$ is closed for all $C \subset X$ closed. This is true because: $C$ closed in $X \Rightarrow C$ compact because of lemma \ref{closedincomp} $\Rightarrow f(C)$ is compact in $Y$ because of lemma \ref{contcomp} $\Rightarrow f(C)$ is closed, using the lemma \ref{compinhauss}.
\end{proof}

\begin{remarkbox}
\begin{remark}
We can consider the concept of sequential compactness: $X$ is sequentially compact if every sequence in $X$ contians a convergent subsequence in $X$. In general, $X$ compact $\not \Rightarrow X$ sequentially compact. But, $X$ compact $+$ first countable $\Rightarrow X$ sequentially compact. Here, $X$ is first countable if $\forall\, x \in X\, \exists \,  N_1, N_2, \ldots, N_i$ (where $i \in I, I $) countable collection of neighbourhoods of $x$ such that every neighbourhood contains one of the $N_i$. Notice that manifolds are first countable! (CHECK)
\end{remark}
\end{remarkbox}

\begin{definition}[Compact exhaustion]
Let $X$ be a topological space. A compact exhaustion $\{K_i\}_{i \in \mathbb{N}}$ of $X$ is a countable collection of compact sets $K$ so that $\bar K_i \subset \mathring K_{i+1}$
\end{definition}

\begin{theorem}
A manifold admits a compact exhaustion.
\end{theorem}
\begin{proof}
The proof uses the second countability. We will skip it.
\end{proof}

\begin{definition}[Paracompactness] \label{paracompactness}
A topological space is paracompact if  every open cover admits a locally finite refinement. i.e. if $\{U_i\}_{i \in I}$ is an open cover of $X$, there is another open cover $\{V_j\}_{j \in J}$ of $X$ such that $V_j \subset U_i(j)$ (refinement) and every point $x$ has a neighbourhood $V_x$ such that $V_x$ intersects only a finite number of $V_j$ (locally finite).
\end{definition}

\begin{theorem} \label{paracompactnessthm}
Manifolds are paracompact.
\end{theorem}
\begin{proof}
The proof uses the second countability property. We will skip it here.
\end{proof}

\begin{definition}[Partition of unity]
A partition of unity subordinate to an open cover $\{U_i\}_{i \in I}$ of $X$ is a collection of continuous functions $f_j \colon X \rightarrow [0, 1]$ such that $\forall \, x \exists \, V_x \ni x, V_x$ neighbourhood of $x$ so that the support $\supp(f_j) = \overline{f_j^{-1}((0, 1])}$ is contained in one of the $U_i$, and $V_x$ intersects only finitely many $\supp(f_j)$, and
$$\sum\limits_{j \in J}f_j(x) \equiv 1$$
\end{definition}

\begin{theorem}
Every open cover of a manifold admits a subordinate partition of unity which is smooth.
\end{theorem}

\begin{definition}[Connected space]\label{connspace}
A topological space $X$ is connected if it cannot be written as union of two non-empty, disjoint open sets.
\end{definition}

\begin{remarkbox}
\begin{remark}[Equivalent definitions of connected space]
Sometimes it is useful to consider equivalent definitions of connection: a topological space $X$ is connected if:
\begin{itemize}
\item It cannot be written as union of two non-empty, disjoint closed sets,
\item $X = A \cup B$ with $A, B$ disjoint and open sets implies that $A= \emptyset$ or $B = \emptyset$,
\item The only sets in $X$ that can be both open and closed are $X$ and $\emptyset$.
\end{itemize}
\end{remark}
\end{remarkbox}


\begin{definition}[Smooth function from a manifold]
Let $M$ be a smooth manifold, then $f \colon M \rightarrow \rfield $ is smooth at $p$ if for a chart $\varphi \colon U \rightarrow \rfield^n$, with $U \ni p$, we have that
$$f \circ \varphi^{-1} \colon \varphi(U) \rightarrow \rfield$$
is smooth near $\varphi(p)$
\end{definition}

\begin{remarkbox}
\begin{remark}
The definition does not depend on the chart chosen. In fact, let's choose another chart $(V, \psi)$, then:
$$f \circ \psi^{-1} = \underbrace{f \circ \varphi^{-1}}_{\text{smooth}} \circ \underbrace{\varphi \circ \psi^{-1}}_{\text{smooth}}$$
is smooth around $p$ because the composition of smooth functions is smooth.
\end{remark}
\end{remarkbox}

\begin{definition}[Smooth function between manifolds]
Let $M, N$ be smooth manifolds. Then $f \colon M \rightarrow N$ is smooth around $p$ if $f$ is continuous and for charts $(U, \varphi)$ around $p$ and $(V, \psi)$ around $f(p)$ we have that $\psi \circ f \circ \varphi^{-1}$ is smooth on a neighbourhood of $\varphi(p)$
\end{definition}

Now that we know when a map is smooth, we want to differentiate. For this, we want the concept of tangent vector. We will use three equivalent definitions.

\begin{definition}[Equivalent curves] \label{equivcurves}
Two curves $\gamma_0, \gamma_1 \colon (-\varepsilon, \varepsilon) \rightarrow M$ with $\gamma_0(0)=\gamma_1(0)=p$ are equivalent if for a chart $(U, \varphi)$ around $p$:
\begin{equation*}
\restrict{\diondi{t}}{t=0} \varphi \circ \gamma_0(t) = \restrict{\diondi{t}}{t=0} \varphi \circ \gamma_1(t)
\end{equation*}
And we notice that such definition does not depend on the chart chosen.
\end{definition}

\begin{definition}[Geom. tangent space] \label{geomtangent}
We define the geometric tangent space at $p$ as:
\begin{equation}
\tangentgeom{p} M = \faktor{ 
\left \{ \text{ smooth curves } \gamma \colon (-\varepsilon, \varepsilon) \rightarrow M \, | \, \gamma(0) = p, \varepsilon > 0 \right \}}
{\sim}
\end{equation}
where $\sim$ is the equivalence relation described in def. \ref{equivcurves}. The elements of $\tangentgeom{p} M$ are called tangent vectors.  
\end{definition}

\begin{remarkbox}\begin{remark}
Let's analyze def. \ref{geomtangent}: it is an intuitive definition of the tangent space but from it it's not obvious that $\tangentgeom{p} M$ is a vector space. The following definition of tangent space is more abstract, but on the other hand it will be evident that the tangent space is a vector space.
\end{remark}\end{remarkbox}

\begin{definition}[Derivation - pt.2]
A derivation $v$ on smooth functions $C^{\infty}(M)$ at $p$ is a $\rfield-$linear map $v \colon C^{\infty}(M) \rightarrow \rfield$ which satisfies the Leibnitz rule $v(fg) = \restrict{v(f)}{p} \restrict{g}{p} + \restrict{f}{p}\restrict{v(g)}{p}$
\end{definition}

\begin{definition}[Alg. tangent space]
We define the algebraic tangent space at $p$ as:
\begin{equation*}
\tangentalg M = \{\text{derivations of } p \}
\end{equation*}
\end{definition}

\begin{remarkbox}\begin{remark}
$ $
\begin{itemize}
\item Instead of using $C^{\infty}(M)$ (which is a ring) we could have used the space $\mathcal{E}^{\infty}_p (M) = \faktor{C^{\infty}}{\sim}$ of germs of functions, where the equivalence relation is the following one:
$$f \sim g \Leftrightarrow f \equiv g \text{ on a neighbourhood of } p$$
In such cases, $v(f) = v(g)$ for $v \in \tangentalg{p} M$. In fact, let's consider a smooth function $h$ with support $\overline{h^{-1}(\rfield \setminus {0})}$ (so, $h(0) = 0$) and $h \equiv 1$ near $p$. Then:
$$0 \overset{(1)} {=} v\left(h(f-g)\right) = v(h)\underbrace{\left(f(p) - g(p)\right)}_{=0} + \underbrace{h(p)}_{=1}\left (v(f) - v(g) \right)$$
where we used: (1) $h(f-g) = h(0) = 0$ near $p$ and the derivative of a constant function is 0 by the Leibnitz rule: $v(1) = v(1\cdot 1) = v(1)+v(1) \Rightarrow v(1)=0$. (2) Leibnitz rule again.
We also say that the derivation $v$ is local.
\item As we already pointed out, $\tangentalg{p} M$ is a vector space. The only non-trivial thing to prove is that $\dim \tangentalg {p}M = n$ if the dimension of $M$ is $n$. 
\end{itemize}
\end{remark}\end{remarkbox}

\begin{lemma} \label{usefullemma}
Let $f \colon \rfield^N \supset B_{\varepsilon}(0) \rightarrow \rfield$ be smooth, then there are smooth functions $f_i$ on $B_{\varepsilon}(0)$ so that:
\begin{equation}
f(x) = f(0) + \sum\limits_i x^i f_i(x^1, \ldots, x^n)
\end{equation}
\end{lemma}
\begin{proof}
\begin{equation*}
f(x)-f(0) = \int\limits_0^1 \diondi{t} f(tx^1, \ldots, x^n)\, dt = \int\limits_0^1 \sum_i x^i \deonde{x^i} f(tx^1, \ldots, tx^n) \, dt
\end{equation*}
So, the $f_i$ we are looking for are:
\begin{equation*}
f_i = \int\limits_0^1 \defonde{f}{x^i}(tx^1, \ldots, tx^n)\, dt
\end{equation*}
\end{proof}

\begin{remarkbox}\begin{remark}
Now, we can prove that the $\dim \tangentalg{p} M = n$. Let $v$ be a derivation at $p$ and $(U, \varphi)$ a local chart around $p$. $x^1, \ldots, x^n$ are the local coordinates, such that $p$ coincides with the origin ($x^i(p) = 0$). From lemma \ref{usefullemma}:
$$f(x) = f(0) + \sum\limits_i x^i f_i(x)$$
and
$$v(f) = \underbrace{v(f(0))}_{=0} + \sum\limits_i \left( v(x^i) \underbrace{f_i(x^i)}_{=f_i(0)} + \underbrace{x^i}_{=0} v(f_i) \right)$$
Then, $v$ is defined by the way it reacts to coordinate functions around $p$. Thus, $\dim \tangentalg{p} M \le n$. Since $v_i = \deonde{x^i}$ is a derivation, we have $\dim \tangentalg{p} M = n$.
\end{remark}\end{remarkbox}

\begin{definition}[Physicists's tangent space]
Let $M$ be a manifold of dimension $n$, then a tangent vector is a map
\begin{equation}
v \colon \{ \text{ charts } (U, \varphi) \text{ around } p \} \rightarrow \rfield^n
\end{equation}
where $v$ is defined such that if we change charts we have:
\begin{equation} \label{phystransf}
v(V, \psi) = D_p (\psi \circ \varphi^{-1})\, v((U, \varphi))
\end{equation}
The space of such maps is called tangent space (according to the physicists's definition) and it is denoted by $\tangentphys{p} M$.
\end{definition}

\begin{remarkbox}\begin{remark} \label{equivremark}
The three previous definitions of tangent space are all equivalent, indeed we can find isomorphisms between such tangent spaces.
\begin{enumerate}
\item
\begin{align}
\tangentgeom{p}M &\rightarrow \tangentalg{p}M  \\
[\gamma] &\mapsto v_{\gamma}\colon C^{\infty}(M) \rightarrow \rfield \\
& \qquad \qquad \qquad f \mapsto \restrict{\diondi{t}}{t=0} (f\circ \gamma)(t)
\end{align}
\item 
\begin{align}
\tangentalg{p}M &\rightarrow \tangentphys{p}M  \\
v &\mapsto \left \{ (U, \varphi) \mapsto\{v(x^i)\}_{i=1, \ldots, n} \right \}
\end{align}
where $x^i$ are the coordinates around $p$ from $\varphi$.
\item 
\begin{align}
\tangentphys{p}M &\rightarrow \tangentgeom{p}M  \\
v &\mapsto \left\{ \gamma_i \colon t \mapsto \varphi(\varphi^{-1}(p) + tv((U, \varphi)))\right\}
\end{align}
with $|t| < \varepsilon$, where we picked up a chart $(U, \varphi)$ around $p$ with coordinate $x^i$.
\end{enumerate}
Accordingly, we also have three different definitions for differentiating, i.e. three definitions for the map
\begin{equation}
D_p f \colon T_pM \rightarrow T_{F(p)} N
\end{equation}
where $p \in M, F \colon M \rightarrow N$ smooth:
\begin{enumerate}
\item $D_p^{\text{geom}} F ([\gamma]) = [F \circ \gamma]$, where the right-hand side is an equivalence class of equivalence curves on $N$,
\item $D_p^{\text{alg}} F (v) = \left \{C^{\infty}(N) \ni f \mapsto v(f \circ F) \right \}$,
\item $D_p^{\text{phys}} F(v) = \{ (V, \psi) \rightarrow D_p^{\text{phys}} (\psi \circ F \circ \varphi^{-1})\, v(U, \varphi)\}$, where $(V, \psi)$ is any chart of $N$ near $F(p)$.
\end{enumerate}
These definitions are compatible with the maps above, e.g. the following diagram
\begin{center}
\begin{tikzcd}
T^{\text{geom}} M \arrow[r, "D^{\text{geom}}"] \arrow[d] & T^{\text{geom}} N \arrow[d] \\
T^{\text{alg}}M \arrow[r, "D^{\text{alg}}"]              & T^{\text{alg}}N            
\end{tikzcd}
\end{center}
commutes.
\end{remark}\end{remarkbox}

\begin{lemma}[Chain rule]
Let $M_0, M_1, M_2$ be smooth manifolds, and $f_0 \colon M_0 \rightarrow M_1, f_1 \colon M_1 \rightarrow M_2$ smooth. Then:
$$D(f_1 \circ f_0) = Df_1 \circ Df_0$$
\end{lemma}
\begin{proof}
Using the geometric definition:
\begin{equation*}
[D(f_1 \circ f_0)][\gamma] = [(f_1 \circ f_0) \circ \gamma] = [f_1 \circ (f_0 \circ \gamma)] = (Df_1 \circ Df_0)[\gamma]
\end{equation*}
\end{proof}

\begin{definition}
For a manifold $M$ of dimension $n$, let 
\begin{align}
TM = \bigcup_{p \in M} \tangentphys{p} M & \overset{\text{pr}}{\longrightarrow} M \\
v &\longmapsto p \nonumber
\end{align}
This map is the bundle projection of the tangent bundle $TM$.
\end{definition}
\begin{remarkbox}\begin{remark}
$TM$ is a manifold, if equipped with the following structure. Pick an atlas $(U_i, \varphi_i)_{i \in I}$ of $M$ (remember: $\varphi \colon M \supset U_i \rightarrow \rfield^n, U_i$ open). Thanks to the second-countability property, we can take $I$ as a countable set without loss of generality. Then, one defines local coordinates (i.e. a local parametrization) on $\pr^{-1}(U_i)$ via
\begin{align} \label{TMmap}
\hat{\varphi_i} \colon TM \supset \pr^{-1}(U_i) &\longrightarrow \varphi_i(U_i) \times \rfield^n \\
v &\longmapsto \left ( \varphi(\pr(v)),\, v((U_i, \varphi_i)) \right ) \nonumber
\end{align}
This is a bijective map, and we define the topology on $TM$ as the topology so that $\hat{\varphi_i}$ is a homeomorphism for all $i$. This topology is Hausdorff, indeed: let $v_1, v_2 \in TM$. If $p_1 = \pr v_1 \not = \pr v_2 = p_2$, then there exist open sets $V_j \ni p_j, j =1, 2$ which are disjoint and open (because $M$ is Hausdorff). Let $U_{i(p_1)}, U_{i(p_2)}$ be charts around $p_1, p_2$. Then:
$$\pr^{-1} \left( U_{i(p_1)} \cap V_1 \right) \cap \pr^{-1} \left( U_{i(p_2)} \cap V_2 \right) = \emptyset$$
On the other hand, if $\pr v_1 = \pr v_2$, let $\left( U_{i(v_1)}, \varphi_{i(v_1)} \right)$ be a chart containing $\pr v_1$. Then $\hat \varphi_i(v_1), \hat \varphi_i (v_2)$ differ in the second entry in $\rfield^n$: pick disjoint open sets $W_1, W_2$ of $\rfield^n$ separating those entries and consider the open sets $\hat{\varphi_i}^{-1} \left( \varphi_i(U_i) \times W_j \right), j =1, 2$.

Moreover, the topology is second countable. Finally, we need to check the smoothness of the transition functions $\hat \varphi_j \circ \hat \varphi_i^{-1}$. Note that $\hat \varphi_i \left( \pr^{-1}(U_i) \cap \pr^{-1}(U_j) \right) = \varphi_i (U_i \cap U_j) \times \rfield^n$. By the transformation rule \eqref{phystransf}, and using \eqref{TMmap}:
$$\hat \varphi_j \circ \hat \varphi_i^{-1} \left (x, w = (w^1, \ldots, w^n) \right) = (\varphi_j \circ \varphi_i^{-1}(x), D_x(\varphi_j \circ \varphi_i^{-1}) (w))$$
This map is smooth. Note that $D_x(\cdots)$ is a linear transformation for fixed base point $x$. 
Thus $ T_pM = \pr^{-1}(p)$ is a vector space
\end{remark}\end{remarkbox}

\begin{definition}[Vector field]
A vector field $X$ on a manifold $M$ is a smooth map
\begin{center}
$X \colon$
\begin{tikzcd}
 M\rar{} & TM \arrow[l, black, bend left]{r}[black, swap]{\pr}
\end{tikzcd}
\end{center}
so that $\pr \circ X = \id_M$.
\end{definition}

\begin{remarkbox}\begin{remark} [Commutator of vector fields]
Let $X, Y$ be two vector fields on $M$. Then the assignment 
\begin{equation*}
C^{\infty}(M) \ni f \longmapsto [X, Y](f) = X \left(Y(f)\right) - Y \left(X(f)\right)
\end{equation*}
is independent of the local coordinates  and satisfies the Leibniz rule (it can be easily verified): $[X, Y](fg) = g\, [X, Y](f) + f \, [X, Y](g)$. Moreover, the commutator satisfies the properties already seen in lemma \ref{commlemma}. And in local coordinates: 
$$X= \sum\limits_i a^i \partial_i, \qquad Y= \sum\limits_j b^j \partial_j$$
we have:
$$[X, Y] = \sum\limits_{i, j}\left[ \left (a^i \deonde{x^i} b^j \right) \deonde{x^j} - \left(b^j \deonde{x^j}a^i \right) \deonde{x^i} \right]$$
\end{remark}\end{remarkbox}

% TODO:MOVE THIS REMARK
\begin{remarkbox}\begin{remark}[Tangent space to a $\rfield$-vector space]
Let $V$ be a an $\rfield$-(vector space) (i.e. a vector space on the field $\rfield$). $V$ is a manifold of dimension $\dim V$. Then, the tangent space to $V$ is $V$ itself (or, better, the tangent space to $V$ is isomorphic to $V$). Indeed, given $v \in V$, there exists an isomorphism:
\begin{align*}
V &\cong T_v V \\
w &\mapsto \left[ t \mapsto (v + tw)\right]
\end{align*}
where the map $t \mapsto (v + tw)$ goes from $(-\varepsilon, \varepsilon) \subset \rfield$ to $V$.
\end{remark}\end{remarkbox}

\begin{definition}
In a manner similar to $TM$ becoming a manifold, one can define the manifold given by the cotangent bundle
\begin{equation}
T^*M = \bigcup_{p \in M} T_p^*M \overset{\pr}{\longrightarrow} M
\end{equation}
\end{definition}

\begin{definition}[1-form]
A 1-form is a smooth map 
\begin{center}
$\alpha \colon$
\begin{tikzcd}
 M\rar{} & T^*M \arrow[l, black, bend left]{r}[black, swap]{\pr}
\end{tikzcd}
\end{center}
so that $\pr \circ \alpha = \id_M$ (it is a section of the bundle). $k$-forms are defined in an analogous fashion, and so are $\Omega^k(M), d \colon \Omega^k(M) \rightarrow \Omega^{k+1}(M), f^*$ and $f_*$.
\end{definition}

\section[Lie Groups]{\crule[yellow!50!white]{0.3cm}{0.4cm}  Lie Groups}
\begin{definition}
A Lie group $G$ is a group and a smooth manifold, such that the operation of the group and the inverse of the group (seen as maps) are smooth. i.e. the maps
\begin{align}
\mu \colon G \times G &\rightarrow G \\
(g, h) & \mapsto g \cdot h \nonumber \\
\text{inv} \colon G &\rightarrow G \\
g & \mapsto g^{-1} \nonumber
\end{align}
are smooth.
\end{definition}

\begin{remarkbox}\begin{remark} [$\text{Aut}(T_e G)$ is a Lie group] \label{autremark}
Let $G$ be a  Lie group and $g \in G$. Let's consider the map 
\begin{align} \label{cgmap}
c_g \colon G &\rightarrow G \\
h &\mapsto g h g^{-1} \nonumber
\end{align}
$c_g$ is  a smooth group homomorphism (i.e. $c_g(h)c_g(k)=c_g(hk)$) and has the property 
\begin{equation} \label{cgprop}
c_g \circ c_{g'} = c_{g \circ g'}
\end{equation}
 It also preserves the identity element: $c_g(e) = e$. If we differentiate at $e$:
\begin{equation}
\text{Ad}_g = D_e c_g \colon T_e G \rightarrow T_e G
\end{equation}
is an isomorphism with inverse $\text{Ad}_{g^{-1}}$. Using the property \eqref{cgprop} we have:
\begin{equation}
\text{Ad}_g \circ \text{Ad}_{g'} = \text{Ad}_{gg'}
\end{equation}
Therefore, we obtain a smooth group homomorphism:
\begin{align} \label{grouphom}
\text{Ad} \colon G &\rightarrow \text{Aut}(T_e G)  \\
g &\mapsto \text{Ad}_g \nonumber
\end{align}
with $\text{Ad}_e = \id_{T_e G}$. Here, $\text{Aut}(T_e G)$ is the set of automorphisms of $T_e G$ (see also remark \ref{endoremark}).
Differentiating again, we get:
\begin{align} \label{grouphom2}
\text{ad} \colon T_e G &\rightarrow T_{\id} (\text{Aut}(T_e G)) \cong \text{End}(T_e G) \\
X &\mapsto \{ Y \mapsto \text{ad}(X)(Y)\} \nonumber
\end{align}
where $\text{End}(T_e G)$ is the space of endomorphisms of $T_e G$ (see also the remark \eqref{endoremark}).
So, since the map given by $\ref{grouphom}$ is bijective, we have that $G \cong \text{Aut}(T_e G)$, i.e. $\text{Aut}(T_e G)$ is a Lie group.  For the same reason, since the map given by \eqref{grouphom2} is bijective, we have that $\text{End}(T_e G)$ is a Lie algebra.
% TODO: T_id and Lie algebra?
\end{remark}\end{remarkbox}

\begin{definition}[Left-invariant vector field]
A vector field $X$ on a Lie group is left-invariant if $l_{g*}X = X$, where $l_g$ is the homeomorphism given by
\begin{align}
G &\rightarrow G \\
h &\mapsto gh \nonumber
\end{align}
The space of left-invariant vector fields is denoted by $\lie{g}$. 
\end{definition}

\begin{remarkbox} \begin{remark}[$\lie{g}$ is a Lie algebra]
Let's consider:
\begin{align}
\lie{g} &\rightarrow T_e G \\
X &\mapsto X(e) = X_e \nonumber
\end{align}
It is an isomorphism since it is linear, i.e. $(X + Y)_e = X_e + Y_e$, and we can get the vector field back from the vector field itself evaluated at $e$. Indeed, the inverse is:
$$X_h = l_{h*} X_e= (Dl_h)(X_e) \quad \oast$$
by left-invariance. In fact, we can prove $\oast$ in the following way: the left-invariance condition $D l_g X = l_{g*}X = X$ means that: 
$$D(l_g)_g'\, (X_g') = X_{g g'}, \, \forall\, g, g' \in G$$
where we wrote explicitly the element $g'$ about which we compute the derivation and the vector field. In the right-hand side the product $gg'$ is well defined by the operation of the group $G$. Then, by left-invariance, we have that:
$$D_e(l_h)\, (X_e) = X_{e h} = X_h$$
which is what we wanted to prove.
So, we have $\lie{g} \cong T_e G$. In order to prove that $\lie{g}$ is a Lie algebra, we need Lie brackets that satisfy Lie algebra properties. In order to solve the problem, we notice that $l_g$ is not only a homomorphism, but also a diffeomorphism. By lemma \ref{commlemma}, point 5, and by recalling that the commutator of vector fields is still a vector field:
$$Dl_g [V, W] = [Dl_g V, Dl_g W] \quad \text{for vector fields } V, W$$
which means that the commutator of left-invariant vector fields is still left-invariant.
All of this makes $\lie{g}$ a Lie algebra, i.e. a $\rfield$-(vector space) with a pairing $[\cdot, \cdot]$ which is antisymmetric and satisfy Jacobi identity.
\end{remark} \end{remarkbox}

\begin{remarkbox} \begin{remark} \label{alphacurve}
Left-invariance provides even more properties. Given a left-invariant vector field $X$, let's consider the curve
\begin{align}
\alpha^X \colon \rfield &\rightarrow G \\
t &\mapsto \alpha^X(t) \nonumber
\end{align}
such that $\restrict{\diondi{t}}{t=t_0} \alpha^X(t) = X(\alpha^X(t_0))$ and $\alpha^X(0)=e$. Such a curve locally exists by the theorems about ordinary differential equations. The global existence is assured by the left-invariance property (it lets us extend the solution on larger sets).

Then, we can completely describe the flow of $X$ using the curve $\alpha^X$. In fact, we can write the flow $\varphi$ of $X$ as:
\begin{align} 
\varphi \colon \rfield \times G & \rightarrow G \\
(t, h) &\mapsto h \alpha^X(t) = l_h (\alpha^X(t)) \nonumber
\end{align}
It satisfies $\diondi{t} \varphi(t, h) = X(h \alpha^X (t))$ by the chain rule and by left-invariance. Moreover, $\alpha^X(s+t) = \alpha^X(s) \alpha^X(t)$ holds.
\end{remark} \end{remarkbox}

\begin{tcolorbox}
\begin{example}
Assume that  $G \subset GL(n, \rfield)$. Then $\lie{g} \cong T_e G \subset \text{End}(n, \rfield)$ and $X \in G$ is given by a matrix $X$. The curve $\alpha^X$ in this case is given by:
\begin{equation} \label{expmatrix}
\alpha^X(t) = \sum\limits_{j=0}^{\infty} \frac{t^j X^j}{j!}
\end{equation}
What if we want to compute the commutator of two left-invariant vector fields? We have that $[X, Y] = \text{ad}(X)(Y)$, where "$\text{ad}$" is the map defined in \eqref{grouphom2}. This is true in general, not only for $G \subset GL(n, \rfield)$. In fact, let's recall the maps defined in the remark \ref{autremark}:
\begin{enumerate}
\item \begin{align}
c_g \colon G &\rightarrow G \\
h &\mapsto ghg^{-1} \nonumber
\end{align}
\item \begin{align}
G &\rightarrow \text{Aut}G \\
g &\mapsto c_g \nonumber
\end{align}
\item \begin{align}
\text{Ad} \colon G &\rightarrow \text{End}(T_e G) \\
g &\mapsto D_e c_g = D(c_g)(e) \nonumber
\end{align}
$D_e c_g \in \text{End}(T_e G)$ because the differential of the map $c_g$ is itself a map from $T_e G$ to $T_e G$
\item \begin{equation}
\text{ad} \colon T_e G \rightarrow T_e \text{End}(T_e G) \cong \text{End}(T_e G)
\end{equation}



\end{enumerate}
Then, if we consider:
%$$\text{Ad}(g)(Y) = (D_e c_g)(Y) \overset{(1)}{=} \left [c_g(\alpha^Y(t)) \right ] = \restrict{\diondi{t}}{t=0} c_g (\alpha^Y(t))$$
$$\text{Ad}(g)(Y) = (D_e c_g)(Y) \overset{(1)}{=}  \restrict{\diondi{t}}{t=0} c_g (\alpha^Y(t))$$
where we used: (1) the geometric definition of differential, see remark \ref{equivremark}.
Thus:
\begin{align*}
\text{ad}(X)(Y) &= \restrict{\diondi{s}}{s=0} \left(  \restrict{\diondi{t}}{t=0} c_{\alpha^X(s)} \alpha^Y (t) \right ) \overset{(2)}{=} \restrict{\diondi{s}}{s=0} \left(  \restrict{\diondi{t}}{t=0} \alpha^X(s) \alpha^Y(t) \alpha^X(-s) \right)  \overset{(3)}{=}  \\
&= \restrict{\diondi{s}}{s=0} \left(  \restrict{\diondi{t}}{t=0} \exp(sX) \exp(tY) \exp(-sX) \right) = \ldots = XY -YX
\end{align*}
where we used: (2) definition of $c_g$ and the fact the $\varphi^{-1}(t) = \varphi(-t)$ if $\varphi$ is the flow of a vector field, (3) definition \eqref{expmatrix} for the case $G \subset GL(n, \rfield)$.
It is common to use the following notation for Lie groups:
\begin{align}
\exp \colon \lie{g} &\rightarrow G \\
X &\mapsto \alpha^X(1) \nonumber
\end{align}
where, in general, $\exp(X+Y) \not = \exp(X)\exp(Y)$. The map "$\exp$" is smooth and 
\begin{equation} \label{deexp}
D_e \exp \equiv \id_{\lie{g}}
\end{equation}
(recall that $T_e \lie{g} \equiv \lie{g}$ because $\lie{g}$ is a vector field and $T_e G \cong \lie{g}$).
\end{example}
\end{tcolorbox}

\begin{lemma}
Let $f \colon G \rightarrow H$ be a smooth group homomorphism. then
\begin{equation}
\exp^H \left(Df(X) \right) = f \left( \exp^G(X) \right)
\end{equation}
where $Df \colon \lie{g} \rightarrow \lie{h}$ with $\lie{g} \cong T_e G, \lie{h} \cong T_e H$, as usual.
\end{lemma}
\begin{proof}
Given $Df(X)$, $\alpha^{Df(X)} \colon \rfield \rightarrow H$ is a 1-parameter subgroup (i.e. it satisfies the same properties of $\alpha^X$, see also remark \ref{alphacurve}). $\alpha^{Df(x)}$ coincides with $f \circ \alpha^X$. i.e. the following diagram commutes:
\begin{center}
\begin{tikzcd}
\mathfrak{g} \arrow[r, "Df"] \arrow[d, "\exp^G"'] & \mathfrak{h} \arrow[d, "\exp^H"] \\
G \arrow[r, "f"]                                  & H                               
\end{tikzcd}
\end{center}
\end{proof}

\begin{lemma} \label{connliegroup}
Let $G$ be a connected Lie group. Then, if $f \colon G \rightarrow H$ is a smooth homomorphism, it is determined by its differential at the identity element $D_e f$.
\end{lemma}
\begin{proof}
Let's take a lok at the diagram of the previous proof. It would be nice if we could go back from $G$ to $\lie{g}$ like this: 
\begin{center}
\begin{tikzcd}
\mathfrak{g} \arrow[d, "\exp^G"'] \\
G \arrow[u, bend right]          
\end{tikzcd}
\end{center}
If it was true, then the thesis would be satisfied because the diagram of the previous proof commutes. Unfortunately, this is not always true. It is \textit{locally} true because $D_e \exp^G \equiv \id_{\lie{g}}$ (think about the Taylor expansion, see \eqref{deexp}). Since it is locally true, there exists a neighbourhood $\mathfrak{U} \subset \lie{g}$ of 0 and a neighbourhood $\mathcal{U} \subset G$ of $e$ such that 
$$\restrict{\exp^G}{\mathfrak{U}} \colon \mathfrak{U} \rightarrow \mathcal{U}$$
is a diffeomorphism. Thus, on $\mathcal{U}$ the $f$ is defined by:
$$f = \exp^H \circ Df \circ \left(\restrict{\exp^G}{\mathfrak{U}}\right)^{-1}$$
Since the Lie group is connected, this is enough to prove the thesis. Indeed every neighbourhood $\mathcal{U}$ of $e$ in $G$ generates $G$ itself if it is connected. This fact can be proved in the following way, let's consider:
$$V \equiv \mathcal{U} \cap \mathcal{U}^{-1}$$
It is an open neighbourhood, because intersection of open sets, with 
$\mathcal{U}^{-1} = \left \{u^{-1} \, | \, u \in \mathcal{U} \right \}$. Moreover,
$$G^V \equiv \bigcup_{j=0}^{\infty} V^j, \qquad V^j \equiv \left \{v_1 \cdot \ldots \cdot v_j \, | \, v_i \in V \right \}$$
$G^V$ is a subgroup of $G$, and it is open because every element of the union is open: $V$ is open, $V^2 = \bigcup_{v \in V} v V$ is open, etc... We will now show that also the complement is open:
$$G = \bigcup_{[g] \in \faktor{G}{G^V}} g G^V$$ 
because: 
\begin{itemize}
\item "$\supseteq$" is trivial since we are considering union of elements of the group,
\item "$\subseteq$" follows from the fact that "$[g] \in \faktor{G}{G^V}$" means that if $g \in G^V$, then "$gG^V$" yields the entire subgroup $G^V$. If $g \not \in G^V$, then we are considering the elements in $G \setminus G^V$. Since "$g G^V$" means that we are multiplying $g$ by all possible element of $G^V$, and since $G^V$ contains the identity element (because it is a subgroup), we can get all the possible elements of $G \setminus G^V$ from the right-hand side. Then $G \subseteq \bigcup_{[g] \in \faktor{G}{G^V}} g G^V$.
\end{itemize}
Thus, we can obtain $G \setminus G^V$ if $g \not \in G^V$ in the union:
$$G \setminus G^V = \bigcup_{[g] \not = [e]} g G^V \text{ is open}$$
Now, given that $G$ is connected (see definition \ref{connspace}), either $G^V$ or $G \setminus G^V$ must be empty. $G^V$ contains at least the identity element if $G$ is non-empty, so $G \setminus G^V$ must be the empty one. So, $G=G^V$.
\end{proof}

\begin{remarkbox}
\begin{remark}
The hypothesis of connection is necessary for theorem \ref{connliegroup}.
\end{remark}
\end{remarkbox}

\section[Compactly Supported Cohomology and Poincar Lemma]{\crule[orange!50!white]{0.3cm}{0.4cm}  Compactly Supported Cohomology and Poincar Lemma}

Let $M$ be a manifold with or without boundary. We have already seen (remark \ref{drremark}) that the $k$-th de Rham cohomology of $M$ is
\begin{equation}
H^k(M) \equiv H_{dR}^k(M) = \frac{\ker ( d \colon \Omega^k(M) \rightarrow \Omega^{k+1}(M))  )}{\im ( d \colon \Omega^{k-1}(M) \rightarrow \Omega^k(M))  ) }
\end{equation}
And we have already considered the Poincar lemma \ref{poinclemma}:
\begin{equation} \label{poinclemmashort}
H^k(\rfield^n) \cong 
\begin{cases*}
0, & $k \not = 0$ \\
\rfield, & $k=0$
\end{cases*}
\end{equation}
Indeed, in \eqref{poinclemmashort} we used that $\rfield^n$ is contractible (case $k \not = 0$) and that the only exact 0-forms (i.e. functions) are the constant ones (case $k=0$).
Now, we want to discuss about $k$-forms with compact support (i.e. when the function $a$ of the definition \eqref{kformdef} has compact support). First, we notice that the exterior derivative $d$ maps forms with compact support to forms with compact support. Then, it is natural to define the following.
\begin{definition}[Compactly supported de Rham cohomology]
We define the compactly supported de Rham cohomology of a manifold $M$ as:
\begin{equation}
H^k_c(M)= \frac{\ker \left ( \restrict{d}{\text{compact supp forms}} \right )}{\im \left ( \restrict{d}{\text{compact supp forms}} \right )}
\end{equation}
\end{definition}

\begin{remarkbox}
\begin{remark}
The compactly supported de Rham cohomology is somehow different from the standard de Rham cohomology. Indeed, for the latter, given a map $f \colon U \rightarrow V$, we could consider the pullback:
\begin{align*}
f^* \colon H^k(V) &\rightarrow H^k(U) \\
[\omega] &\mapsto [f^* \omega]
\end{align*}
For the compactly supported case, we can't do that anymore, because if $\omega$ is compactly supported, in general $f^* \omega$ might not be compactly supported (preimages of compact sets are not compact in general).
\end{remark}
\end{remarkbox}

Now, we want to prove a compactly supported version of the Poincar lemma. In order to do that, we need some general result on manifolds.

\begin{remarkbox}
\begin{remark}[Integration over the fiber] \label{intfiberrem}
Let $M$ be a smooth manifold, oriented. Let's consider the projection map
\begin{equation}
\pi \colon M \times \rfield \rightarrow M
\end{equation}
Given $\omega \in \Omega^k(M), \pi^*\omega$ is not compactly supported in general. However, we can define a map 
\begin{equation}
\pi_* \colon \Omega_c^k(M \times \rfield) \rightarrow \Omega_c^{k-1}(M)
\end{equation}
defined in such a way that $\pi_* \omega$ still has compact support. Such a map is called \textit{integration along the fiber}. We used the notation of the pushforward because it is indeed another way to write the pushforward of a $k$-form. 
We also notice that every $k$-form on $M \times \rfield$ can be written as sum (more than two summands in general) of two types of $k$-forms:
\begin{description}
\item[$\bullet $ Type A)] $\omega = f(x, t) \, \pi^* \eta, \quad \eta \in \Omega^k(M)$,
\item[$\bullet$ Type B)] $\omega = f(x, t)\, \pi^*\eta \wedge dt, \quad \eta \in \Omega^{k-1}(M)$.
\end{description}
where $f(x, t)$ is a compactly supported function, $dt \in \Omega^1(\rfield)$.

Now, we define $\pi_*$ in the way it acts on (type A) and (type B) forms:
\begin{align}
&\text{(type A)} \qquad  f(x, t) \, \pi^* \eta \overset{\pi_*}{\longmapsto}  0  \label{typea} \\
&\text{(type B)}\qquad  f(x,t) \, \pi^*\eta \wedge dt \overset{\pi_*}{\longmapsto} \eta \, \,  \underbrace{\int\limits_{-\infty}^{\infty} f(x, s) ds }_{\equiv F(x)}  \label{typeb} 
\end{align}
Formally speaking, $f$ is the pullback of $F$.
\end{remark}
\end{remarkbox}

\begin{lemma} \label{pichain}
$\pi_*$ is a chain map, i.e.
\begin{equation}
\pi_* \circ d  = d \circ \pi_*
\end{equation}
where the differential in the left-hand side acts on $M \times \rfield$, whereas the one in the right-hand side acts on $M$.
\end{lemma}
\begin{proof}
We prove the lemma for (type A) and (type B) forms, thus it is proved for any form on $M \times \rfield$ (see remark \ref{intfiberrem}).
\begin{itemize}
\item(Type A):
\begin{align*}
\pi_* \circ d \left ( f(x, t) \,\pi^* \eta \right ) &= \pi_* \left ( \underbrace{\defonde{f}{x}(t, x) dx \wedge \pi^* \eta}_{\text{type A}} + \underbrace{\defonde{f}{t}(x, t)\, dt \wedge \pi^* \eta}_{\text{type B}} + \underbrace{f(x, t) \, d\pi^* \eta}_{\substack{\text{type A because } \\ d\pi^*=\pi^*d}} \right ) = \\
&=\int_{-\infty}^{\infty} \defonde{f}{s}(x, s) ds \, \pi^* \eta = 0
\end{align*}
where we used the definition of $\pi^*$ for (type B) forms \eqref{typeb} and  the fact that $\pi_*(\text{type A}) = 0$. Finally, we used the fundamental theorem of integral calculus with the fact that $f$ has compact support.
Moreover, $d \circ \pi_*( f(x, t) \, \pi^*\eta) = 0$.
\item(Type B):
\begin{align*}
\pi_* \circ d (f(x, t) \, \pi^* \eta \wedge dt) &= \pi_* \left ( \defonde{f}{x} (x, t) dx \wedge \pi^*\eta \wedge dt + f(x, t) \pi^* d\eta \wedge dt \right)  = \\  
&=\defonde{F}{x}(x, t)dx \wedge \pi^* \eta + F(x) \, \pi^* d\eta 
\end{align*}
where we used the $F$ defined in \eqref{typeb}.
Moreover,
\begin{align*}
d \circ \pi_* (f(x, t) \, \pi^* \eta \wedge dt) = d(F(x)\, \pi^*\eta) = \defonde{F}{x}(x, t)dx \wedge \pi^* \eta + F(x) \, \pi^* d\eta 
\end{align*}
\end{itemize}
\end{proof}

\begin{remarkbox}
\begin{remark}
Consequence of lemma \ref{pichain}: $\pi_*$ defines a map
$$\pi_* \colon H_c^k(M \times \rfield) \rightarrow H^{k-1}_c(M)$$
We want to prove that such a map is an isomorphism. Let's construct the inverse in the following way: let $e \colon \rfield \rightarrow \rfield$ be a smooth function with compact support  such that $\int_{-\infty}^{\infty} e(s) ds = 1$. Then, we define $e_*$ as:
\begin{align} \label{einverse}
e_* \colon \Omega^{k-1}_c (M) &\rightarrow \Omega^k(M \times \rfield) \\
\eta &\mapsto e(t) \, \pi^*\eta \wedge dt \nonumber
\end{align}
We have $\pi_* \circ e_* = \id_{\Omega_c^{k-1}(M)}$ \textbf{but} $e_* \circ \pi_* \not = \id_{\Omega_c^k(M \times \rfield)}$. Nevertheless, we just need $e_*$ to be the inverse of $\pi_*$ on $H_c^k(M \times \rfield)$, not on all $\Omega^k_c(M \times \rfield)$. This is true thanks to the two following results.
\end{remark}
\end{remarkbox}

\begin{lemma}
The map $e_*$ defined in \eqref{einverse} is a chain map, i.e.
\begin{equation}
d \circ e_* = e_* \circ d
\end{equation}
where, again, the differential in the left-hand side acts on $M \times \rfield$ and the one in the right-hand side acts on $M$.
\end{lemma}
\begin{proof} 
Let $\eta \in \Omega^{k-1}_c(M)$.
\begin{equation*}
d \circ e_*(\eta) = d \left ( e(t)  \pi^*\eta \wedge dt \right ) = e(t) d^{M \times \rfield} \pi^*\eta \wedge dt = e(t) \pi^*(d^M \eta) \wedge dt = e_* d \eta
\end{equation*}
\end{proof}

\begin{proposition} \label{kprop}
$e_*, \pi_*$ are mutually inverse isomorphisms on $H_c$, since there is an operator $K \colon \Omega^k_c(M \times \rfield) \rightarrow \Omega_c^{k-1}(M)$ such that:
\begin{equation}
\mathbb{1} - e_* \circ \pi_* = (-1)^{k-1} (d \circ K - K \circ d)
\end{equation}
where:
\begin{itemize}
\item $K$ is defined as:
\begin{align}
&K(f(x, t)\, \pi^*\eta) = 0 \quad \text{on (type A) forms} \\
&K(f(x, t)\, \pi^* \eta \wedge dt) = \int_{-\infty}^t f(x, s) ds\, \pi^* \eta - F(x) \left ( \int_{-\infty}^t e(s)ds \right) \pi^* \eta \, \, \text{on (type B) forms}
\end{align}
where $\eta$ is a $k$-form in the first case, and it is a ($k-1$)-form in the second case.
\item $K$ is not a chain map (otherwise the right-hand side would be 0)
\end{itemize}
\end{proposition}
\begin{proof}
We will prove it for (type B) forms. Then, for (type A) forms it is analogous. Then, recalling the definition of the integration along the fiber $F$ contained in \eqref{typeb}:
\begin{enumerate}
\item
\begin{align*}
(\mathbb{1} - e_* \circ \pi_*)(f(x, t) \, \pi^* \eta \wedge dt ) &=f\, \pi^*\eta \wedge dt - e_*(F(x) \eta) = \\
&= \underbrace{f\, \pi^* \eta \wedge dt}_{(a)} - \underbrace{F(x) e(t) \pi^* \eta \wedge dt}_{(b)}
\end{align*}
\item 
\begin{align*}
(d \circ K)(f(x, t) \, \pi^* \eta \wedge dt) &= \underbrace{f\, \pi^* \eta \wedge dt}_{(a)}(-1)^{k-1} + \underbrace{\left( \int_{-\infty}^t \defonde{f}{x}(x, s) ds \right) dx \wedge \pi^* \eta}_{(c)} + \\
&+ \underbrace{\int_{-\infty}^t f(x, s) ds \, \pi^* d\eta}_{(d)} - \underbrace{\left( \defonde{F}{x}\int_{-\infty}^t e(s) ds \right) dx \wedge \pi^* \eta}_{(e)} + \\
&- \underbrace{e(t)F(x) \, \pi^*\eta \wedge dt}_{(b)} (-1)^{k-1} - \underbrace{F(x)\left(\int_{-\infty}^t e(s) ds \right)\, \pi^* d\eta}_{(f)}
\end{align*}
\item 
\begin{align*}
(K \circ d) (f(x, t) \, \pi^* \eta \wedge dt) &= K \left( \defonde{f}{x}(x, t) dx \wedge \pi^*\eta \wedge dt + f(x, t) \pi^* d\eta \wedge dt\right) \overset{(1)}{=} \\
&= \underbrace{\left(\int_{-\infty}^t \defonde{f}{x}(x, s) ds \right) dx \wedge \pi^*\eta}_{(c)} - \underbrace{\defonde{F}{x} \int_{-\infty}^t e(s)ds \,dx \wedge \pi^* \eta}_{(e)} + \\
&+ \underbrace{\int_{-\infty}^t f(x, s) ds \, \pi^* d\eta}_{(d)} - \underbrace{F(x) \int_{-\infty}^t e(s) ds \, \pi^* d\eta}_{(f)}
\end{align*}
Using that: (1) we can exchange integral and derivative, and remember that: $F(x) = \int_{-\infty}^{\infty} f(x, s) ds$.
\end{enumerate}
By comparing the three results, we have the thesis.
\end{proof}

Now, we are finally ready to prove the compactly supported Poincar lemma.

\begin{lemma}[Compactly supported Poincar lemma] \label{compsupppoinclemma}
\begin{equation}
H_c^k(\rfield^n) \cong 
\begin{cases*}
0, & $k \not = n$ \\
\rfield, & $k = n$
\end{cases*}
\end{equation}
\end{lemma}
\begin{proof}
The proof is by induction. The map used to prove the isomorphism for the case $k = n$ is:
\begin{equation} \label{mapn}
[\omega] \mapsto \int_{\rfield^n} \omega
\end{equation}
We notice that the integral is well defined because $\omega$ is compactly supported, and the integral does not depend on the element of the equivalence class chosen. In fact, if we consider another $n$-form $\omega'$ such that $[\omega'] = [\omega]$, then it means that $\omega'$ and $\omega$ differ by an exact form: $\omega' = \omega + d \alpha, \alpha \in \Omega_c^{n-1}(\rfield^n)$. Let's suppose, without loss of generality, that $\supp \alpha, \supp \omega, \supp \omega' \subset B_{2R}(0)$. Then:
$$\int_{\rfield^n} \omega' = \int_{B_{2R}(0)} \omega' = \int_{B_{2R}(0)}(\omega + d \alpha) = \int_{B_{2R}(0)}\omega + \int_{B_{2R}(0)} d \alpha \overset{\text{Stokes}} = \int_{\rfield^n} \omega + \underbrace{\int_{\partial B_{2R}(0)} \alpha}_{=0}$$
For $\fbox{n=1}$, the case $k \not = 1$ coincides with the case $k=0$: we are considering compactly supported closed 0-forms, i.e. functions which are 0 everywhere. Thus, the only compactly supported closed 0-form which is not exact is the trivial one:  $H^0_c(\rfield) = 0$. As regards the case $k=1$, by the standard Poincar lemma \eqref{poinclemma} we know that every closed 1-form on $\rfield^n$ is exact, i.e. $\omega = d\alpha$, but $\alpha$ might not be compactly supported! Indeed let's write $\omega$ as $\omega=f(x) dx$, $f$ compactly supported function. If $\int_{\rfield} \omega = \int_{\rfield} f(x)dx = 0$, then the primitive $F(x)=\int_{-\infty}^x f(t)dt$ has compact support (for $x$ small we are outside of the support of $f$, whereas for $x$ big we integrate over all the support, i.e. it's like considering $\int_{\rfield} f(x) dx = 0$). Then $\omega$ is exact: $\omega = dF$ and $F$ has compact support $\Rightarrow [\omega]=0$. Indeed, this explains why we chose the map \eqref{mapn}: if the integral of the $n$-form is 0, the form is exact and it is the differential of a compactly supported $(n-1)$-form. In a similar way, we can choose another closed $n$-form $\eta=g(x)dx,\, g$ with compact support such that $\int_{\rfield}\eta = \int_{\rfield} g(x)dx=c \not=0$. In this case, the primitive is $G(x) = \int_{-\infty}^x g(t)dt$ and $\eta = dG$ (we already knew that such a $G$ existed, thanks to the standard Poincar lemma), but $G$ is not compactly supported anymore (we have problems for $x$ big). Moreover, for each value of $c$ we get a different primitive. Since we can choose $g(x)$ such that we get any value of $c \in \rfield$, we have that $H_c^1(\rfield) \cong \rfield$. For $\fbox{n > 1}$: from proposition \ref{kprop}, we know that there is an isomorphism 
\begin{equation} \label{hisomorph}
H_c^k(M \times \rfield) \,  \substack{\overset{\pi_*}{\longleftarrow} \\ \underset{e_*}{\longrightarrow}} \, H_c^{k-1}(M)
\end{equation}
Let's suppose the thesis is true for $n-1$, then we can consider $M=\rfield^{n-1}$ (notice that $\rfield^n$ is an oriented manifold) and thanks to the isomorphism \eqref{hisomorph} the thesis is true also for $n$.
\end{proof}

\begin{remarkbox}
\begin{remark}
The Poincar lemma \ref{compsupppoinclemma} is valid for $\rfield^n$, but it also holds for open subsets of manifolds which are diffeomorphic to $\rfield^n$. For instance, we can consider the convex subsets $U$ of $\rfield^n$ contained in a manifold. The form $\eta \in \Omega_c^{n-1}(U)$ with $d\eta=\omega$ produced in the proof of the Poincar lemma for a compactly supported form $\omega$ can be extended by zero to a smooth form on the entire manifold. We will not make any distinction between the extended form and the form on $U$.
\end{remark}
\end{remarkbox}

We will now analyze some consequences of the compactly supported Poincar lemma.

\begin{definition}[Bump form] \label{bumpform}
A bump form on $M^n$ (manifold of dimension $n$) is a compactly supported $n$-form $\omega = \rho \, dx^1 \wedge \ldots \wedge dx^n$ in a coordinate domain $(U, \varphi), \varphi \colon U \rightarrow \rfield^n \supseteq \varphi(U)$ so that $\left | \int_U \omega \right | = 1$.
\end{definition}

\begin{remarkbox}\begin{remark} \label{bumpremark}
The Poincar lemma tells us that: given $\omega, \omega'$ bump forms in $U$, then:
\begin{equation}
\omega = \omega' + d \eta,\, \eta \in \Omega_c^{n-1}(U) \Leftrightarrow \int_U \omega = \int_U \omega'
\end{equation}
Indeed, the "$\Rightarrow$" is always true thanks to Stokes's theorem, whereas the "$\Leftarrow$" follows from the proof of the Poincar lemma \ref{compsupppoinclemma}, because bump forms are $n$-forms in a $n$-dimensional manifold and the equality between the integrals implies that $[\omega] = [\omega']$ (because the isomorphism used in the proof is given by the integral map \eqref{mapn}). Thus, the two forms differ by an exact form. If $[\omega] = [\omega']$, the two forms are called \textit{cohomologous}, and we will also write $\omega \sim \omega'$.
\end{remark}\end{remarkbox}

\begin{proposition} \label{prop2bumpforms}
$M$ connected manifold, then any two bump forms are cohomologous (on $M$) up to a sign.
\end{proposition}
\begin{proof}
Let's consider two bump forms $\omega, \omega'$, compactly supported on $(U, \varphi), (U', \varphi')$, respectively. If $(U, \varphi) = (U', \varphi')$, then we have that $\int_U \omega = \int_U \omega'$ up to a sign (by definition \ref{bumpform}). Then, by the previous remark \ref{bumpremark} we have that $\omega$ and $\omega'$ are cohomologous up to a sign. On the other hand, if $(U, \varphi) \not = (U', \varphi')$, we do in the following way: since $M$ is a connected manifold, it is also path-connected (see problem set 12, ex. 1.(e)). Thus, we can find a path between $U$ and $U'$, and we can cover it with open sets $U_1, U_2, \ldots$.
\begin{figure}[h] \label{Fig:bumpforms1}
     \centering
     \includegraphics[width=0.8\textwidth]{Images/bumpforms1.pdf} 
     \caption{We can cover the path between $U$ and $U'$}      
\end{figure}
Then, we can consider a bump form $\omega_1$ in $U \cap U_1$, and we can extend it by zero in $U$. For the same reason seen in the case $U=U'$, we have that $\omega$ is cohomologous to $\omega_1$ up to a sign. Again, we can consider a bump form $\omega_2$ in $U_1 \cap U_2$ and extend it by zero in $U_1$. As before, $\omega_2$ is cohomologous to $\omega_1$ up to a sign, and so on. We continue in this way up to $U'$, and using the transitive property of the equivalence relation we find that $\omega$ is cohomologous to $\omega'$ up to a sign.
\end{proof}

\begin{lemma} \label{lemmalocfinite}
Every $n$-form on $M$ is cohomologous to a locally finite sum of bump forms.
\end{lemma}
\begin{proof}
Pick a cover $(U_i, \varphi_i), \varphi_i \colon U_i \rightarrow \varphi_i(U_i) \subseteq \rfield^n$, and a locally finite refinement $V_j$ (recall the definition contained in def. \ref{paracompactness}) with a subordinate partition of unity $\lambda_j$ (i.e. $\sum_j \lambda_j = 1$). Then:
$$\omega = \sum\limits_j (\lambda_j \omega) \sim \sum\limits_{\substack{j \text{ s.t. } \\ \int \lambda_j \omega \not = 0}} \underbrace{\left( \frac{\lambda_j \omega}{\int \lambda_j \omega} \right)}_{\text{bump form}} \cdot \underbrace{\left ( \int \lambda_j \omega \right )}_{\text{coefficients}}$$
And the sum if locally finite because we chose a locally finite refinement, which is always possible since manifolds are paracompact (thm \ref{paracompactnessthm}).
\end{proof}

\begin{theorem} \label{surjintegral}
If $M$ is a connected, smooth manifold of dimension $n$, such that $\partial M = \emptyset$ and $M$ is oriented, then the map:
\begin{align} \label{surjmap}
H^n(M) &\rightarrow \rfield \\
[\omega] &\mapsto \int_M \omega \nonumber
\end{align}
is well defined and surjective.
\end{theorem}
\begin{proof}
Since $\partial M = \emptyset$, we have $\int_M (\omega + d \eta) = \int_M \omega$ by Stokes's theorem. So, the definition does not depend on the element of the equivalence class chosen. Moreover, let $(U_i, \varphi_i)$  be a chart from the oriented atlas, and let $\omega=f(x_1, \ldots, x_n) dx^1 \wedge \ldots dx^n$ in $U_i$, with $f$ smooth function, $f > 0$. Then, $\int_M \omega > 0$. Since $\omega$ and $c \, \omega$ are cohomologous $\forall \, c \in \rfield$, we have that for every real number $\lambda$ we can consider a $n$-form $c\,  \omega$ such that $\int_M c\, \omega = \lambda$. So, the map is surjective.
\end{proof}

\begin{theorem}
If $M$ is a connected, smooth manifold of dimension $n$, $\dim \left( H^n(M) \right) \le 1$.
\end{theorem}
\begin{proof}
Using the previous theorem \ref{surjintegral}, we just need to verify how many values $\int_U \omega$ can have, where $U$ open subset of $M$ diffeomorphic to $\rfield^n$ (Notice: $\partial U = \emptyset$ because $U$ is open). Now, every element of $H^n(M)$ is cohomologous to a (real) multiple of  a bump form in a fixed chart $(U, \varphi)$. Bump forms have integral $ =\pm 1$. Therefore, the value of the integral of a closed-but-not-exact $n$-form in $H^n(M)$ can be any real number. Since the map \eqref{surjmap} is surjective, we have the thesis.
\end{proof}

\begin{definition}
A manifold $M$ is closed if it is compact and $\partial M = \emptyset$.
\end{definition}

\begin{theorem}
Let $M$ be a connected, compact, orientable manifold with $\partial M \not = \emptyset$ and of dimension $n$. Then $H^n(M) = 0$.
\end{theorem}
\begin{proof}
It suffices to find a bump form which is cohomologous to zero in $M$. Once we find it, we have the thesis because $M$ is connected, so any other bump form in the manifold is zero by proposition \ref{prop2bumpforms}. Also, by lemma \ref{lemmalocfinite}, every $n$-form is cohomologous to a locally finite sum of zero-forms, i.e. it is a zero form and we would have the thesis.
\begin{figure}[h] \label{Fig:bumpforms2}
     \centering
     \includegraphics[width=0.8\textwidth]{Images/bumpforms2.pdf} 
     \caption{We can find a bump form cohomologous to a zero form}      
\end{figure}
Let's find such a bump form. First, let's consider a boundary chart $(U, \varphi), \varphi \colon U \rightarrow \rfield^n \cap {x_1 < 0}$ (such that $\varphi(U)$ is a half-ball) and a bump form $\omega$ compactly supported in the interior of the half-ball. We will also consider the other half-ball. We will call $B_l$ the lower half-ball ($B_l = \varphi(U)$) and we will call $B_u$ the other half. We consider another bump form $\omega_u$ contained in $B_u$ such that:
$$\int_{B_u} \omega_u = \int_{B_l} - \omega$$
It is always possible, because bump forms can only have integral $=\pm 1$.
Now, we consider the form $\omega_u + \omega$ and we extend it by zero. It is compactly supported in the open ball $B \equiv B_u \cup B_l$. We notice that $\int_{B} (\omega_u + \omega) = \int_B d\eta, \, \eta \in \Omega_c(B)$ by Stokes's theorem and using that $\partial B =\emptyset$. Then, by remark \ref{bumpremark} we have that $d\eta = \omega_u + \omega$. Thus:
$$\restrict{d\eta}{B_l} = \restrict{\omega_u}{B_l} + \restrict{\omega}{B_l} = \restrict{\omega}{B_l}$$
Then, if $\varphi^* \eta$ is a primitive of the bump form $\varphi^* \omega$. So, $\varphi^* \omega$ is the zero form we were looking for.
\end{proof}

What if we remove orientation? Recall that we can't compute integrals without orientation.
\begin{theorem}
Let $M$ be a connected but not orientable manifold of dimension $n$. Then: $H^n(M) = 0$.
\end{theorem}
\begin{proof}
Let's cover $M$ by an atlas $(U_i, \varphi_i), \varphi_i \colon U_i \rightarrow \varphi_i(U_i) = \rfield^n$ (we choose the atlas such that $\varphi(U_i) = \rfield^n$). Let's consider an open set $U_0$.
\begin{figure}[h] \label{Fig:bumpforms3}
     \centering
     \includegraphics[width=0.8\textwidth]{Images/bumpforms3.pdf} 
     \caption{We can consider two paths connecting $U_0$ and $U_i$}      
\end{figure} Let's consider another set $U_i$ from the cover, $U_0 \not = U_i$. Since $M$ is a connected manifold, it is also path-connected, so there exists a path $\gamma \colon [0, 1] \rightarrow M$ that connects $U_0$ to $U_i$. By the same reasoning of the proof of proposition \ref{prop2bumpforms}, we get that a bump form $\omega_0$ contained  in $U_0$ is cohomologous to a bump form $\omega_i(\gamma)$ contained in $U_i$. But, since $M$ is not orientable, there also exists a second path $\gamma'$ connecting $U_0$ and $U_i$, and a second chain of charts covering $\gamma'$, such that, repeating the same procedure, $\omega_0$ is cohomologous to $\omega_{i'}(\gamma')$ with the opposite sign of before (i.e. the integrals have opposite signs). Notice that even if we cannot integrate on $M$, we can integrate on the charts. It means that:
$$\omega_i(\gamma) \sim \omega_0 \sim \omega_{i'}(\gamma') \sim -\omega_0$$
So, $\omega \sim - \omega \Leftrightarrow 2 \omega \sim 0$.
\end{proof}

\begin{remarkbox}\begin{remark}
The proofs of the following facts are similar to the previous ones: given a connected manifold $M$:
\begin{itemize}
\item $M$ not compact, $\partial M = \emptyset \Rightarrow H^n(M)=0$
\item $M$ compact $\Rightarrow H_c^*(M) = H^*(M)$
\item $M$ not compact, oriented, $\partial M= \emptyset \Rightarrow H^n_c(M)=\rfield$ 
\end{itemize}
\end{remark}\end{remarkbox}

%Add picture vector field pag. 207 Lee, and pag. 221. CHECK from page 14

\clearpage
\fancyhf{}
\begin{thebibliography}{AA}
\addcontentsline{toc}{section}{Bibliography}
\bibitem{Lee}
{J.} M. Lee, Introduction to Smooth Manifolds, Springer
\bibitem{Notes}
Lecture notes (Differentiable Manifolds Sachs, Vogel WS 19)
\end{thebibliography}


\end{document}
